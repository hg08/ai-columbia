## 4.1 对抗搜索

 　　我们已讨论过搜索智能体．其目标是寻找一个问题的解． 这个解常常表示为这样的形式：一系列**动作构成的一个序列**.　 例如从A点,到B点,从宜宾到泸州，再从泸州到重庆等等.


　　本节中,我们将讨论另一类搜索．它在人工智能中非常重要.　这就是**对抗搜索**.
 也称游戏(games).　这种搜索是应对这样的竞争环境的：有一个对手与我们对抗，我们无法控制它.

​	为了阐明游戏概念和游戏中的搜索，你寻找最优解．这个最优解不再是动作的序列,而是一个能帮助我们赢得游戏的**策略**.　
 	若对手做A操作,那么我们就做B操作.

 	若对手做C操作,那么我们就做D操作. 

因此，我们可将最优解表达为规则的一个集合．但是，如果实现规则，你会发现它非常冗长！因此，我们需要一种方法来设计一个智能系统，用来在游戏中赢得对手．

​	

​	好消息：为了设计这个游戏,　我们仅需要两类要素!　它们都是我们学过的知识----**搜索算法**和**启发式估值函数**．我们利用这些知识来建AI模型，并解决游戏．


​	值得注意：游戏被认为是AI中一个很难的话题!  它很难，以致于它的进步能推动AI领域的前沿研究.
从AI视角来看，它也极其有趣，因为它太难以解决！　例如，无论对于人还是机器而言，国际象棋都堪称一个复杂问题, 它有多达35的分支因子, 这意味着需要探索的搜索空间中的态多达$35^100$, 或者说$10^154$结点.
这个空间非常的大！


​	所以，想找到最优解(最优决策) 是不可行的.　这就是说，我们实际上 需要的是：在AI中，解决游戏问题的关键要素是----找到合适的近似.　 我们要近似地解决游戏问题！


​	总结：通过搜索整个搜索树来解决问题，在此根本行不通！原因：搜索树的结点太大了！


​	举例：对于checkers, 在1994年, 智能体"Chinook",打败了人类(M. Tinsley).　从此, checkers 被视为一个完全解决了的问题．它用了许多不同的策略来搜索搜索空间，如由上至下，由下至上等等． 但它也用了一个 end game数据库，该数据库已存储a perfect play for包括棋盘上八个或更少的棋子的所有位置 , 共4.43千亿个位置.　 



### 国际象棋

​	在AI领域，最引入瞩目的游戏，几乎就是国际象棋.　它是一个复杂的游戏. 香农在其文章 *Programming a Computer for Playing* *Chess* (1949)中建议, 将国际象棋作为一个AI问题来研究.　

​	不仅如此，他同时还制定了解决游戏的基础方法．这些方法在其他应用和游戏中也非常有用途.
 香农发明了minimax算法，建议用**启发式估值函数**来减小搜索空间，而不要到上到树的顶端去.


​	国际象棋史上另一个重要时刻：1997年, 深蓝 (IBM智能机) 击败了人类冠军卡斯帕罗夫. 斯帕罗夫后来说, 从深蓝下棋的走法，他已经感觉到深层智慧，甚至是创造性蕴含其中.

​	在AI和国际象棋历史上，最后一个重要的事件:　21世纪初, 毫无争议的世界冠军Vladimir Kramnik 在与Deep Fritz (IBM公司) 对战时输了．

 

### 围棋

​	另一个值得提到的复杂游戏，就是围棋．围棋直到2016年都还被认为是AI无法应对的问题，因为它的搜索空间实在是太大太大了！的确，围棋的分支因子超过了XXX.  然而,　好消息是：2016年，Google Deep Mind项目, AlphaGo, 已经击败了樊麾, (欧洲围棋冠军), 和世界冠军李世石 Lee Sedol.

 	另外，其他游戏，如Othello. 它虽不如国际象棋，checkers,围棋等游戏那么复杂． 所以，Othello也是一个已经解决了的游戏.　计算机下得比人类好.




### 完全信息游戏和非完全信息游戏

​	在AI领域，我们将游戏沿着不同的维度分类. 例如,　如果对于这个游戏我们有完全的信息,　或者说游戏双方都能看清整个游戏局面, 我们称之为**完全信息游戏**（也称决定性游戏）这类游戏包括国际象棋，checkers, 围棋， backgammon and Monopoly.
 	如果我们没有完全信息，也就是说我们对游戏进展不能有一个完整的图像, 例如我们的对手的牌我看不见．我们称这类游戏为**非完全信息游戏**（也称非决定性游戏）.这类游戏包括Battleship, blind, tic-tac-toe, bridge, 扑克, Scrabble, nuclear war, etc.

 	非决定性游戏，又称为随机游戏．随机的意思就是，概率因素引入到了游戏之中.　可能性因素由洗牌，丢骰子等过程引入.

 	本节中，我们关注的是决定性游戏：环境完全可测，或说信息是完全的. (足够难了！)
 我们也称其为"零和游戏"：　两个游戏者交替玩此游戏．　

然后，我们将要涉及一点概率，让你对于概率对决定性游戏的改变有一点体会.

 	零和游戏是典型的对抗游戏: 纯对抗, 其中的对手有不同的得分值，其中一个选手试图使某个值最大化，而另一个选手则尽量使其最小化.  游戏中被某个选手所做的一次移动称为一个回合(ply). 我们在此有一个目标函数, 我们想要一个选手使其最大,而另一个选手使其最小. 这就是零和游戏.

零和游戏包含所谓的embedded thinking, 或称反向推理．在此过程中，一个游戏参与者试着想出该做什么, 如何做决定.　他思考他自己的动作会导致的一些后果.　他也需要思考其对手.　同样， 对手也在思考该做什么，等等等等.
	 It will imagine what would be the response from the opponent to his or her actions.　这需要嵌入式思考(embedded thinking).

 

### 将问题形式化

​	我们需要嵌入式思考来解决游戏问题．我们可将该问题按如下方式形式化.


​	我们从某个初始态(initial state)出发.　例如，称其为0.　我们从此开始: 
$$
sum = 0     \nonumber
$$

​	 然后，状态s 定义哪一个参与者将于态s操作(action), 通常是交替地操作. 对 s的动作可以返回：在s态时合法移动的集合.
 	我们将定义一个转变函数，他goes from s cross a, the set of s takes
 crossed 可能的多个动作 to s, which 定义了该移动的结果.


​	我们要定义终态测试(terminal test). 当游戏结束时，这个函数返回真(True)， 否则，该函数返回假(False).
 游戏结束的态，我们称为终态(terminal states).
 	我们将定义另一个函数: 
$$
{\text{utility}}_{s,p}
$$
我们称它为效用函数，或目标函数．　其含义是对于游戏者p在终态s结束的游戏的一个函数. 举例如下：

1. 在国际象棋中, 结果只有三种: 赢，输，平. 三种情况的效用函数值分别为1, 0, 或1/2.
2. 对于一个tic-tac-toe游戏，效用函数的值可以是1, -1, 或0 (分别对应于赢，输，平).



### 仅有一个参与者的游戏

​	为了简单起见，我们先来思考只有一个游戏参与者的情形.  我们假设一个 tic-tac-toe游戏，只有一个参与者参加. 这种情况，我们一般称其为Max. Max一个人玩此游戏.　

​	例如，我们叫Max玩三个回合(移动三次). Max开始玩了，他放了一些X 在棋盘上，例如，他分别放了三个X在(0,0), (1,1)(2,2)位置．因此Max赢得了游戏. 这就是解决这个问题的一种可能性．注意，这里没有对手与其较量.

​	让我们看看搜索空间是怎样的.Max即将开始游戏时，初始态是0 (棋盘上是空的，一个空的３X3网格子．)
此时，Max可以将X放在棋盘上的任意一个格子中，这就是说Max有9中可能的动作可做．
 因此，这是一个分支因子:
$$
b = 9.    \nonumber
$$
Max可以将X放置在初始态(以搜索树的观点，抽象地从理论上看，这就是初始结点) 转变为下面任意一个态：（图片）．从搜索树的观点看，也就是说可以从根结点到达９个子结点中的任意一个.

 	假设Max将X放在(0, 0) 位置.　第二步，Max放置第二个X. 此时，可以放在剩下的８个位置中的任意一个,　假设Max将其放在(1,1)位置． 接下来，Max可以放入第三个X在(2,2)位置并赢得游戏；或者他可以将X放在(0,2)位置，而不能赢得游戏.

 如果Max放置 三个位置(可以用树的结点来表示)为：(0,0), (1,1), (2,2), 那么该结点的效用函数为1. 

(0,0), (1,1), (0,2)结点的效用函数为0,....

(只要Max未能将三个X排成一线，那么此状态下的效用就是0.)
 

因此，在只有一个游戏参与者的情形,没有对手阻止Max赢得游戏. 只要Max选择正确的路径，使他到达效用为1的结点.



 ### 真实情况的游戏(对弈)

​	例如，这里是一个真游戏.　有两个玩家， tic-tac-toe游戏.　两个玩家交替出手.　

​	 Max 先操作(move).　 Max将结果最大化, 而 Min试着将结果最小化.
 有一个目标函数(utility).　Max将其最大化, Min将其最小化.
 他们将对每一个结点做计算． minimax 值是面对一个完美对手时能达到的最好的效用函数值.
 Minimax值是面对最聪明的对手时可达到的最好的回报.
 假设两个玩家都极其聪明,二者的tic-tac-toe游戏技术都是炉火纯青，且每个人都竭尽全力阻止对方赢得游戏．
 因此，现在，搜索空间就变得有些不同了.　我们不仅仅只有Max一个玩家．我们还有阻止Max赢得游戏的另一个玩家: Min. 因此我们有：一层是Max所面对的状态, 一层是Min所面对的状态.
 每一层，我们称之为一个回合（ply）.
我们将试着(沿着搜索树)往下走直到Max赢得游戏.

 	从Max开始. Max将在棋盘上放X.　接下来Min将有不同的操作可做.　他可能放置一个圆圈来阻止Max赢得游戏.　假设Min放圆圈在这里,　然后，又轮到Max玩游戏，他将放一个X在这里(坐标)以阻止Min赢得游戏,等等.

 So we have now a whole such play that are actually different plies.
 也有一些叶子结点，表示平局．谁也赢不了谁．　这就是Max与Min交替tic-tac-toe对弈的可能状态的整个搜索空间.

​	所以minimax算法版的对抗搜索按如下方式工作:
	minimax算法以找到让Max赢得游戏的策略为目标．所用的方法是深度优先搜索(depth-first search).
我们有一个游戏树，例如tic-tac-toe游戏的搜索树.
 我们将要增加深度优先搜索．深度游戏搜索中，一个最优叶子结点可能出现在树的任意深度上的任意位置.
 minimax原理计算处于一个态的效用函数,直到游戏结束．前提是假设两个游戏参与者都极其聪明,也就是说：两个游戏参与者都是不犯错误的．

 	Minimax算法的基本思想就是：一旦叶子结点被发现，就将minimax的值传播至树的根部.　

　　Minimax算法的基本思想可以总结为如下"if论断".　如果一个状态是终态(叶子结点) ,　那么我们只需将该态的目标函数值提取出来.

 	该状态的效用函数值：

1. 我们将给出的一个数； 
2. 依赖于该态是否有利于Max.

 如果此态是一个Max结点,　那么其值就是所有子结点的效用函数值的最大值.
 再次调用minimax算法, 它是一个递归函数．它将一直往下走以得到沿途路径的最大值.
 如果此态是一个Min结点, 那么你将要到取的值就是所有子结点的效用函数的最小值.
 总之, Max总试图让目标函数取尽量大的值,　而Min则总是让目标函数的值最小.

 	换句话说,状态s的minimax值由s的utility值给出(或者说由目标函数在 s的值给出), 如果对状态s做终态测试，结果为真(True), 则我们达到游戏终点态.
 	

​	如果轮到Max玩游戏, 那么我们将最大化所有的子结点.
我们将递归地调用 minimax of the results of applying the action
 a on s.　We're going to do that over all the actions
 and take the maximum over the path of all the actions for s.
 	

​	如果轮到Min玩游戏, 那么我们将递归地调用minimax with the result of applying
 the action a for all the actions on s.
 因此，我们将取最小值of the resulting from the minimax of all
 of those actions for Min.


 	这是一个简单的递归算法：对Min,它将试着找出整个路径的最小值,
 且对Max,试着找出整个路径的最大值.




