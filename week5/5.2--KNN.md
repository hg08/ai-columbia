Let me first point out that not every machine learning methods builds a model,
and today I choose to talk about k nearest neighbors.
Maybe this is one of the oldest machine learning methods out there.
So typically K nearest neighbors don't build the model, but
they still do some, they're able to do classification and regression.
So our first MM method is KNN.
The main idea of KNN is to use the notion of similarity between examples.
The assumption is that two similar examples should have the same labels, and
this is something very intuitive so suppose you have a set of fruits and
you want to if you look at it if you take a look at two fruits and
look at the similarity between them.
You could say whether they are likely to be the same category not so the two
similar, two bananas have some similar features such as the length and the width
roughly in the same range of width and length, so we are going to assume that all
our examples are points in a d-dimension space this is the space of features.
So we have data points or examples that this data points are described by a set of
features and they lay in some.
Dimensional space just like the case of bananas and
apples, we could think that we have the width and
the length of the fruits and we have our data points lying somewhere.
So when we have something like all the bananas lying,
on one side and all the oranges maybe they
simply on the other side based on their future values of width and length.
So K nearest neighbors uses us what we call the standard the clearly a distance.
This is the distance between the data points,
which means that if I go back to my previous example.
The distance between this point and
this point would be measured by their values at the X.
And their values at the Y axis.
K nearest neighbors uses the Euclidean distance to define.
The nearest neighbors.
So the principle is very straightforward, so you take two examples x_i and
x_j two data points in some d-space and you calculate the distance between.
Two data points by taking their feature values going from one to d so
you have d dimensions.
Just take the difference between their values of each feature,
square this and put everything, the sum and they're square root, so
this tells you how far are each point.
The point x_i from the point x_j.
Now of course there are other distance metrics, but
the most commonly used is the Euclidian distance.
The training algorithm is very simple.
It simply consists in adding all the examples, X and
Y where X is the feature value and Y is the label.
To the data set D.
This simply means.
Store your data that's all you need to do to make your training.
There's no training and no model built at all, okay.
So then, the classification stage or
classification algorithm consists in the following.
So suppose you have your data set of bananas and apples, you store this data
set in D, right in the dataset D you want to use it to predict a new instance.
So given another example let's call it XQ for
X square you want to query what would be the label of the new fruit.
Suppose you, you know you don't you can't see the fruit so
you can't tell whether it's a been an orange, but
however you know the length and the width of the fruit.
In this case I'm going to just drop this fruit in the middle of all your fruits and
see where it falls, if the neighborhood is mostly bananas,
then you would classify this as banana.
If the neighborhood is mostly oranges, you would classify the fruit as an orange, so
this is a very simple principle.
So, the idea of the classification,
so if you have our fruit example suppose you have the length of the fruit and
the width of the fruit and you have all the bananas lay somewhere on this side
of the of the plot, whereas the oranges would lay somewhere maybe here, right.
So the idea is given this query for
which we don't know actually what is the label.
Let's put it somewhere here, right.
So for this point here we want to see what is it's neighborhood.
So this table example is x_q, we want to be able to find, what's y_q and
we put the hat because we're going to estimate it based on the data.
So this y_q is calculated as what?
So let's suppose that N_K(x_q) means that
look in the k neighbors of the X query of the new example to classify.
And then take the label of all this points in the neighborhood, right.
And do majority voting.
So to predict that this fruit here is, is likely to be an orange.
So we're going to define that y_q, would be equal in this case two minus one which is
the the sign or the label of the, fruit that are in the around it.
So here is another example, suppose that you have
this configuration which you have two kinds of fruits, the orange and the blue,
all right, and the query is the point across here in the middle.
How can you classify this so we look in the neighborhood right.
So in the case of three nearest neighbors are going to look into the three nearest
neighbors to the point of the query.
In this case we have two blue and one orange, and
you would classify this is pretty as a blue circle, right.
So as I said, k nearest neighbors don't provide a model, they just, we just store
the data and we just put the point in the middle and see where it falls, right.
So if we ask whether we could draw an approximate decision
boundary to see what actually k nearest neighbor is doing for K equals three.
So for example what we could do is to look in the neighborhood draw this kind of,
What we can do is to imagine like effective points around, right.
And for each of the data points let's check whether we could predict the,
the kind of the class of the point.
So if I look at this point here it looks like the nearest neighbor of this point
would be something, like this point here is the nearest neighbor this point here
is the nearest neighbor and this point here is the nearest neighbor, right?
So this point would be classified as blue.
If I take a point here the nearest neighbors will be this one, this one and
this one and I would predict that this part is orange.
So I do could that for a kind of dividing this picture into a set of a grid of
points, for each point I would look into the neighborhood of three neighbors and
decide whether it is actually a blue or and circle.
So if I did this I would come up with a decision boundary that
actually just effective we don't really do it in k nearest neighbors, but
look at how non-linear is decision boundary.
So explicitly k nearest neighbors is doing some none linear job, in identifying,
the class or predicting the class of examples.
k nearest neighbor seems very simple and very straightforward.
What are the process and cons of k nearest neighbors.
First of all the advantages of k nearest neighbors is that
as you have as you have seen this is very simple to implement store data point.
Fits in the data, right.
It works very well in practice and as a matter fact it has been used for
several decades, on different domains.
It doesn't require to build the model and makes no assumptions about
the distribution of the points, and there is no need to trim the parameters.
Except maybe care if you want to learn the best optimal key for
the nearest neighbors.
It can be also extend easily with new examples so
whenever you have a new example that you collect,
you just add them to the data set and there is no need to train any model.
However they also have these advantages and
this include that it acquire a large space to store the entire training set.
So suppose with time you're going to collect three or
four millions of fruits you still have to store this somewhere on your disk.
And it's also very slow.
So you can imagine when you have a new query you need to figure out
what are the can years neighbors to this point, which means that you're going to
calculate the distance between this point and the rest of the data that you have,
given any examples and the features to be general, the methods takes about
in the order of N times this, N is the number of examples times d.
That's a lot, for each data point you need to do a lot of calculations to figure out
what are the points in the neighborhood.
And also that suffers from what we call the curse of dimensionality
which is that in very high dimensional spaces so,
so in the case of fruits, we have two or three dimensions.
But suppose you collect a lot of features described in the examples and
in this case, the distance becomes meaningless when you move to a very, very
high dimensional space, more on it would be in your course on Machine Learning.
Applications of KNN as I said are numerous.
This include information retrieval when you try to search a text
on in a search engine.
This text is the system would be looking for similarity between your text and
the existing web pages.
Hundreds in character classification using K nearest neighbors have been also out there
for a long time, or
recommender systems, example users like human like the same movies.
So you need to find similarity between you and
the other users which implies that you're going to look for the K nearest neighbors or
the K closest people that have similar features like yourself.
Breast cancer diagnosis,
medical data mining a lot like finding patients with similar symptoms.
And in general pattern recognition in general has used a lot in
the best K nearest neighbors and
it's still being used today for its simplicity and and effectiveness.
Let's go back to our training and testing process.
So we said that we have a training said we deploy some mission and
methods we have seen can end as an example.
We built a model right.
And given this model we can make a prediction you have some input and
you can get some outputs.
Now how can we be confident?
That this model F, is good enough.
We calculate some kinds of errors to be able to assess whether the model
is good or not.
So for this purpose are going to calculate what we call,
the insensible error or the training error, or also empirical error or risk.
The training error is a measure that tells you how much,
how many errors is your model F doing on the training data itself, right?
So we call it train of F which means that
given the model that you learn how many errors you're making.
A way to do it is to calculate how much loss of information you're getting.
So suppose that, remember that we're in the case of supervised learning,
we are trying to find a function f that goes from the feature values to
the label Y, right.
So this function F means that when I apply this function F on
X on a new feature value, I can make a prediction.
This is the prediction.
But for training data,
it happens that I also have the true label available and this is Y, right?
So if I think this two quantities for each training example, I compare what
the true label is and I compare that to what the model is telling me.
So I compare Y, to the model F, applied on the future values X.
So I would define this difference to be actually, what we call the loss.
How much information are lost based on the labels,
that is the true label versus the predicted label by F and
I could sum this up over all the training example I have.
So take the prediction made on your training examples in your model F and
make calculate some loss between.
The true label and the predicted label.
There're different kinds of loss functions.
In a classification setting for example, we could calculate the loss between
the true label and the predicted label as one,
if the sign of the true label and the predicted label are different.
Which means that it was a banana and your model F predicted it as an orange or
vice versa.
So we are going to add one to the loss whenever we make a mistake.
So this means simply that, there was a mistake made on the model
by the model F on predicting the class of the element x_i.
If there is no mistake in this case we're just going to sum up zero, to add zero to
the loss, another kind of loss a function that does actually also suited for
ratio is the last that simply makes a difference,
between the truly ball and the predicted label f(x_i).
so it's created just not to worry about the sign of the difference.
So if you have to add to it if you have an example for which you have a label, and
you have a prediction.
You can simply do the difference between the label and the predicted label.

