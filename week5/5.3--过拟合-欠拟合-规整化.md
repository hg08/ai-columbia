# 过拟合与欠拟合

So now, how can we be sure that our minimizing the training error
will lead to a better model when applied to the test set?
So this is an illustrative example from the machine learning literature that
actually aims to illustrate the concept of overfitting and underfitting.
So suppose we have a dataset of trees and
all the botanical objects, all the flowers etc.
And you want to be able to learn the concept of what’s a tree.
So we could use the natural language in a statement.
we could come up with just sentences that tell what's a tree from what's
not a tree.
An example of that, we could come up with a sentence that tells,
a tree has a brown bottom part, right, and a green top.
So this describes a tree.
however, let's say we have also bushes and other things,
some other things can be mistaken as trees while they are not.
So this is actually a very general concept.
This is a general description of a tree.
So this is our first function, f1, that describes a tree.
A second possible function describing what's a tree could be
let's give as much details as we can about the tree.
So it could be that this tree is five feet long.
And actually this tree has three roots, and
has four major branches, major branches.
And we could go on and on in describing, very specifically, what's a tree,
maybe the shade of brown, the shade of green, and let's say, 50 leaves, etc.
So we're going to provide a very,
very specific description that tells what's a tree.
The trick here is that this description will match this tree exactly.
So it will be, actually, fitting this tree perfectly,
which means that if we have other trees in the dataset with other variations,
with other features, maybe they have 40 leaves, or they have 4 roots,
these trees won't be classified as trees using this second function, f2.
So this is too general and would let many non-trees be classified as trees.
And this is a very specific description that will let, actually,
no trees be classified as a tree, because it's so
specific to this specific tree here, all right?
So the idea of machine learning is to explore everything in between, what
would be the sweet spot, the best hypothesis, or the best function f, right?
I will call it o for optimal, so the best optimal hypothesis or
description that tells me what's a tree.
So maybe you could do something in between, in which we specify what's
a tree, but without really going too general or too specific.
So it could be if the bottom is a trunk, the bottom is woody.
All right, is woody, with between five and seven feet.
We could say there is lots of leaves on the top, lots of leaves.
It has branches, it has between three and ten roots.
This is something more general that would let pass many trees,
but would also not let past bushes, because bushes are probably shorter,
in terms of the height of their trunk, and so on and so forth.
So between these two hypotheses, we would prefer then picking up this one because
it has more chances to make less error in predicting 一个对象是否是一棵树.
At the core of machine learning is what we call structural 风险最小化,
which actually aims to find the best possible models, given some problems.
So on the x-axis, what we have here is the complexity of the models.
So we have a set of models,
going from the very low-complexity models to very high-complexity models.
例如, the model that tells that the tree has a brown bottom and
a green top is a local model, 它极简单.
The other one that has many specifics about the number of leaves,
the number of branches, and so on is 一个高度复杂的模型.
On the y-axis, we have the prediction error, and
here we consider the error on both the training set and test set.
So we're going to plot two curves, one for the training set, in green, and
the test set, in red.
So we're going to see the behavior of the error
when we vary the complexity of the model.
So you could expect that with 简单模型, such as our
simple sentence about trees,　我们将会把很多不是树的对象预测为树.
所以，我们将犯很多的错误, 也就是说语言误差会非常高! 即使对训练集也是如此!

如果它在训练集上表现不好,

很有可能在测试集上也表现很差!
For the high-complexity models, actually, in the example of trees,
our model would be able to match the training set exactly.
So we are going to really find a model that overfits, or learns by heart,
our training points.
So the training error will be quite low
because we really learned exactly what was in our training data.
However, when we plug this function f into a new dataset of trees and
bushes, etc, we're going to do poorly because the complexity of the model was so
high that it wouldn't let any other tree make it through the test error.
So basically, we would have a 很高的测试误差和一个较小的训练误差.
The art of machine learning is to find something in between.
Good models would be somewhere in the area in the middle, in which you don't, what
we call, underfit the data by making poor models, in terms of they're too simple.
And you don't overfit the data, because otherwise,
your model will be learning it by heart.
So you have to find a good model,
something in between is at the core of machine learning methodologies.
Okay, so let's illustrate this concept of training, and testing, and
structural risk minimization.
We'll go back to this example on regression.
Suppose we want to predict the incoming function of age, right?
So remember, we have part of the data as points on a 2-D space.
And the idea is to find different functions or
different prediction models that give us a prediction of a future instance.
So for example, this line here is a linear model that goes through the data points,
or this second curve in blue, or also, this complicated curve here.
In the first example, we talk about underfitting or high bias,
because this line does not really fit well the data.
In the last example, we talk about high variance of overfitting,
because we came up with a curve that actually goes through each and
every single data point in the data.
We really learn the data by heart, and it's not a good model, either.
So the two models are going to be on the extreme sides of
the structural risk minimization curve.
The third one is more promising, it's just right.

# 

We find something that goes through most of the data points, but
doesn't fit them very well, but it doesn't fit them poorly, either.
Okay, so how do we avoid underfitting and overfitting, in general?
To avoid underfitting,
we need to find models that at least to do well on the training set, right?
So this is easier than avoiding overfitting.
To avoid overfitting,
what we can do is, in general, try to avoid complicated models.
If we remember, we find this complicated model that describes trees,
this model will make poorly on any other object, except those seen for training.
So in general, use a simpler model.
And the way to use a simpler model is, for example, to reduce the number of features.
For example, the number of features for trees would be the number of leaves,
number of branches, height of the trunk, and so on and so forth.
So the more we're going to put in the training set in the feature values,
the more the model will try to include all of them.
So what we can do is to remove some features to avoid,
really, finding a model that would use all of them.
We could also do some model selection.
And this is something that will be left for the machine learning course.
We could use what we call regularization, which is, okay, so
we have a lot of features described in the data.
Let it be, let's use all of them.
However, let's reduce the importance or
the impact of this, the small parameter values.
So if you have a lot of features that contribute a little bit to the model,
let's try to regularize that.
So we're going to make a balance between learning the data, and also, having all
the features having an impact, and reduce their importance in the final model.
The last possible thing we can do is to do what we call a cross-validation,
which is to estimate the test errors.
我们将看看这是什么意思．


##　规整化
So first of all, the intuition behind regularization is as follows.
So remember, we want to minimize some error, some loss.
So, we have a classification term in general that we want to minimize.
And in this case, in the case we have seen before, it's minimizing the loss.
We don't want to make a lot of mistakes on the training error,
but you're going to add to that besides minimizing the error.
Adding C time some regularization term,
which is actually a function of the model F.
This means that, okay, we want to minimize this quantity of loss.
At the same time,
you want to keep the contribution of the features reasonably controlled.
This is just an idea.
We're going to definitely see more of that in your mission on the course coming up.
In the example of regression, what we can see here is that, for example,
for the first model, the function of X would be just the function of a line.
Will be lambda 0 and lambda 1 being parameters.
Lambda 0 plus lambda 1X is the equation of this line here.
So, the function for the second one would be a polynomial of degree two and
the third one would be a higher degree polynomial.
So if you wanted the regularization what we can do is to avoid high degree polynomial
by setting up there actually their weights lambdas to be very small
by regularization.
So, another way to control the way we are performing on the training and
the set is to use what we call a train validation tests scenario.
For example, you have your dataset,
what you can do is to split the dataset into three pieces.
The training set, the validation set and the test sets.
So in this setting, for example, we could decide to split 60% for
learning the model, 20% for validation of the model and 20% for testing the model.
So, the test set is just like a dataset that we don't touch at all.
You put it in the drawer.
You don't look at it.
You build a model, try to find your best parameters.
Once you are final with your model, you just apply it on the test set.
So, first step is training set is the set of examples used for learning the model.
Example a classification model.
So, we use the example that you're having a dataset to build the function F.
The validation set is a set of examples that cannot be used for learning,
but that can help you choose the parameters.
For example, in the case of K nearest neighbors, you need to know what's K.
K would depend a lot on the domain that you are working with.
So if you wanna learn K, you could use some validation to try different case and
see which one would have the best performance.
Finally, the test that is used to assess the performance of the model and
provide an estimation of the test error.
This is the out of sample error.
Note that you should never use the set test that in any way to further
tune the parameters or revise the model.
This means that given the training and
validation sets do whatever you can to build the best possible models.
Tune the parameter to derive this the best possible model.
Once you get this model, call it F applied on the test set, but
never revise your model based on a test set.
You of sample error should be a true out of set of error, [delete]
you could revise.
Otherwise, you're going to again, try to overfit the test set etc., etc.
So in this scenario,
just give it a set separate always separate from your learning.
Another method for estimating test are using only the training data is called K
fault cross-validation.
In case of cross-validation, we're going to use a training and
validation data to squeeze as much as possible out of your data.
So given the learning algorithm A and given a dataset D,
you're going to first start by splitting your data into K partition.
So, split your data into K equal partition.
So typically, what you are doing is you take your training validation set and
you split it into K pieces.
So the first round you would do the training on the first pieces,
first partitions and then do the testing on the reminder partition, so
that would be the test.
The next stage you will create, you will actually used four other
partitions which could be this partition here and here and here and here.
And afterwards, do the tests on the remainder partition and
you would do this 5 times if you have K equal to 5.
So the idea is that you're going to look at the data and squeeze it in different
ways and the average, you calculate then the average error which is over all
the faults which is the error done on each of the partitions.
So, this gives you a better assessment by looking at the data and
squeezing as much information as possible out of it.
So suppose you went through all that process of finding the best model, we
tried to be careful that the model is not overfitting or underfiiting the data.
So, how can we assess quantitatively how our model or function F is doing?
Typically, what we want to do is to calculate the number of errors that
the classifier is doing.
In which case, we're going to draw what you call the confusion matrix.
A confusion matrix is a matrix of numbers
that actually quantify how we are doing.
So, in a confusion matrix what we're going to contrast is the actual label
versus the predicted level.
So the actual label is the label that comes in your dataset that tells
you whether the example is positive or negative, banana or apple to put it
some label is the label that you obtain by actually applying the model F.
For doing F of what? of XI.
So you have an instance XI or an example XI, you apply the model F and
you get some predicted label, white hat.
Okay, great.
So in this table, what you're going to assess is how many errors you make?
And as you may guess,
the errors actually are in the in this section here and in this section here.
So if your example was positive and you credit it as positive, great.
You make a good prediction and
you add this number to the number of true positive that you are making.
If your label is negative and
your model F is predicting it as negative, very good job too.
You are creating a true negative here,
which means you made a good prediction on a negative example.
However, if your example was positive and you predicted as negative, you made
a mistake and you're gonna increase the number of false negative in your table.
If your label is negative and you predicted as positive,
it's also got the kind of mistakes.
Kinds of errors that's we can make are errors that are either false negative or
false positive.
Based on the confusion matrix,
we can calculate different measures to tell us how our classify doing.
So, there are different measures you could use.
You could calculate something called accuracy,
which is the percentage of predictions are correct.
The percent, which is the percentage of positive prediction that are correct.
Also what you call recall or sensitivity, which is the percentage of positive cases
that were predicted as positive or specifically which is the percentage
of negative cases that were predicted as negative.
Depending on the application, you want to see all kinds of errors and
estimate with how many errors we are making on the positive,
on the negative and how this compare to each other.
Okay, great.
So an example of that is that in medicine, for example, we have to talk about
sensitivity and specificity in some cases where we want to see, for example,
whether we predicted the disease correctly or predicted the disease uncorrectly.
So after going through that, let's finally review the terminology seen today and
the concepts that we have learnt.
So first of all, we call an instance or example is data point in the space.
It's a line or a row in the fruit table, for example.
We talk about a feature, which is a variable or description of the feature.
It could be the weight, the length of each example or each instance.
We talked also about label, which is the tag that we provide to each example.
The example is either a banana or an orange.
We talk about supervised learning in which we leverage both the feature values and
the label, and unsupervised learning in which we don't expect to have labels.
We spoke about classification, which is the train whether it's a banana or an orange
or regression about what would be the weight in function of the length and
we spoke about clustering which is how to put the data together by finding
clusters.
We spoke about prediction.
Whenever you build a classification model,
it's possible to use it to make a prediction on an unseen example.
We spoke about training.
Said, this is what you use in general to derive your model.
We spoke about validation sets which is a set not used for training,
but used to find the best parameters.
We spoke about this set which is the “no, no touch” dataset.
This set is something that you put on the side,
you don't look at all during the learning time.
You just put it whenever you want to assess how your function is doing or
your model is doing on an unseen dataset.
So, this is helps to estimate your out of sample error.
We spoke about K fold validation which is a different way to slice and dice your
dataset, so as you get the best performing model with the best performing parameters.
We spoke about classification error,
which is how much error you're making in predicting positive and negative example?
We also spoke about loss function,
which is a way to asses the classification error.
Two important concepts we have also seen is overfitting and underfitting.
When you find the model, you try to find the best fit model in
which we actually don't and underfoot the data by heart in the very, very,
very simplistic model that we make a lot of mistake or finding a very, very,
very complicated mothers will actually also make a lot of mistakes and
that's would be our overfitting and underfitting.
And finally, we spoke about regularization which is just I gave you just an intuition
of that which actually aims to use all the features to find your model,
but then you could just temper a little bit and
control how the features contribute to the final model.
So I think this was our initial, our basic concepts on what's machine learning.
Basically, you will definitely see a way more information in the next couple
lectures, but also you're going to take a class on machine learning in this serie.
Finally, I would like to mention machine learning resources and books.
So for example, if you wanna have access to introductory textbooks,
I would recommend the book by Tom Michel.
Mitchell and About Mustafa who actually provide the very understandable
introduction and several methodologies, of machine learning.
If you want to dive more, I would recommend this last three books.
The Element of Statistical Learning, it has the statistical side.
An Introduction to Machine Learning part and also I would
recommend Pattern Recognition and Machine Learning from that Christopher Bishop and
Pattern Classification which is a classic book in pattern classification in
which you'll find things like K-nearest neighbors, and all its variants,
and so on, and so forth.
There are also a wealth of resources online and
this includes all major journals, and conferences such as ICML, nips,….
These are the top one can pretty much the top one conferences in
machine learning and AI.
You could have access to a large number of video lectures in this website.
Also, if you are more interested in other aspects of machine learning such as the theory aspect of machine
learning.
I would recommend checking this website here.
There are other groups and newsletter that you could also tap into that has a wealth
again of information about machine learning.
So, thank you again for your attention in this lecture.
Next time, we are going to talk more about linear models and also nonlinear models.
So, stay tuned.
