# 机器学习 

Today we develop a basic understanding of the principle and methods of machine
learning, one of the most prominent approaches in artificial intelligence.
Let’s first clarify the terminology.
So you all have probably heard many buzzwords including machine learning,
data science, data mining, data analysis.

Statistical learning knowledge discovery in databases, pattern discovery, and so
on. So they all fall under the same umbrella which is learning from data.
How to do science with data. 

The term data mining tend to disappear today, it was coined in the mid 90s.
Today we talk more about **data science**.
All this terminology has been unified under the data science terminology,
which is how to make science with data.
What motivated the field of data science is that data today is everywhere.
(For example, Google processes 24 petabytes of data per day.
Facebook processes ten millions of photo every hour.
YouTube, we have about one hour of video uploaded every second.
Twitter, about 400 million tweets per day.
And in astronomy, for example, satellites data is in hundreds of petabytes.
It is estimated that by 2020, the digital universe will reach 44 zettabytes of data.
That is 44 trillion gigabytes.
That's huge.)
So data comes in different types and flavors.
Data could be text, it could be numbers, click streams,
graphs, tables, images, transactions, videos, and sometimes all of the above.
For example, some websites would have a mix of all of these kinds of data.
Okay, the fact is that today we are “datafied”.
Wherever we go, we leave a trail of data.
Smartphones for example, are tracking our locations.
We leave it a data trail in our web browsing.
We also interact a lot today with social networks, leaving behind us photos,
comments, and so on, and so forth.
Which actually makes privacy an important issue in data science.
However, data science aims to make a good use of this data for
the good sake of humanity.
The data science process consists of five main steps.
The first one is data collection.
As I said, data comes in different flavors, so
you could do data collections of data that is static, dynamic.
It could be structured in databases.
We also need to collect information about domain expertise.
This is crucial to be able to understand the domain
in order to be able to leverage this data and turn it into knowledge.
The second step is data preparation.
It's really rare that the data comes clean and ready to be used so
that the data preparation consists in doing some data cleaning and
moving spurious for example information, and creating some features or
variables to do some feature of variable engineering.
So the outcome of this step is some curated data that is
almost readily available to be fed to machine learning and
statistics methods.
The third step is exploratory data analysis.
This step has been there actually in statistics for a long time.
It's about exploring your data with different charts and curves and
understand what we have received.
It also allows us to ask the right question, given the data we have received.
First of all do we have enough?
Do we need to collect more?
And second, what are the questions we want to have answered with this data?
So after these three steps we should have a clean data set that's comes out of it.
Along with focused and interesting questions you want to address.
So then comes the step of machine learning.
Machine learning is the core step in data science in which we deploy machine
learning methods and statistics methods to get knowledge and
to learn models from the data.
So these models could be either classification models, clustering models,
regression, density estimation, and so on and so forth.
So we have our data.
How is it that we extract models, and we hope that these models
are good to make predictions, to help us make better decisions.
So the results of the machine learning are in general visualized
with curves to show the performance of the models.
It also could be mapped on a map if it's a spatio-temporal data and
also deployed in real life.
So the hope is that we acquire knowledge from this data and sometimes
we need to refine the process a little bit so we go back into the main step.
Possibly including more pieces of data and refining the process until we
are satisfied with the knowledge that we have learnt from the whole process.
We have seen that machine learning is one of the core steps in the data science
process.
So now, what are the applications of machine learning?
We all use machine learning on a daily basis, sometimes without even knowing it.
For example, we all use emails, right?
So in our emails we sometimes get spams coming into our inbox.
But as the times goes on, spams are being filtered by
your email system because there is a smart machine learning piece behind it
that learned how to classify good emails from spam emails.
And this all hidden to you.
It's in the email, embedded in your email system.
Machine learning has been applied also for detection.
In recognizing images on checks and zip codes.
It's also used in detecting faces and images.
And analyzing images, MRI images.
Machine learning is also the core of a recommender systems, search engine such as
Google's, and hand recognition along with many, many, many other applications.
Another interesting fact is that machine learning is an interdisciplinary field.
There are many different theories that contribute to machine learning.
These include biology, engineering,
signal processing, databases, economics, visualization techniques.
In particular, the connection between machine learning and
statistics is very strong.
Let's look closer into this connection and
clarify how machine learning relates to statistics and data analysis.
So this is actually a decade-long question about the connection between the two
fields and whether machine learning and statistics should be separate fields or
should merge intimately as one single field.
The fact is that they are two complimentary fields with different
visions and terminology, but do have a lot in common.
And what they have a lot in common is data.
Both fields care about data.
They both provide a set of analytical tools and methods, with some overlap.
For example, probability-based methods exist both in machine learning and
statistics.
We talk about, in statistics, about **fitting models**,
whereas in machine learning we talk about **learning models**.
So statistics care about topics like hypothesis testing, experimental design.
Methods like **ANOVA, linear regression, logistic regression,**
**GLM generalized linear models, principal component analysis**.
Machine learning cares about computational modeling and high dimensional spaces.
It includes advanced non-linear techniques such as **support vector machines, neural networks,**
**deep learning**, that use actually special geometry rather than probability methods.
Other techniques include **decision trees**.
Decision trees were both invented in the statistics community and
also in the machine learning community.
Other techniques include **rule inductions, clustering methods, association rules,**
**future selection, visualization graphical models** that are based on probability,
genetic algorithms etc.
So for an interesting contrast between machine learning and
statistics, or determining statistics,
I refer the viewers to these interesting article about about that.
So finally, can we find a good definition of machine learning.
So according to one of the founders of machine learning,
Professor Tom Mitchell, machine learning is about
how do we create computer programs that improve with experience.
So more specifically, a computer program is set to learn
from experience E with respect to some class of tasks, T.
And performance measure P.
If it's performance at task T as measured by P, improves with experience.
So here, professor Mitchell made it clear about what is learning.
In the sense that we talk about experience E with some respect of class T.
So we want to solve some problem, we have some task to achieve.
So we have for that, we have some experience E, and
this experience comes in terms of data.
So we say that a computer program is capable of learning if its performance,
right, measured by measure P doing task T using E,
right, improves with the experience.
Which means that the more we have data, the more
our computer program will be able to learn and do a better job at doing some tasks.
So I think this is a very compelling definition of machine learning.



ML: **from a collection of input-output pairs, learn a function that predicts the output for new inputs.**




# 监督学习和非监督学习

Training data comes in the form of what we call training examples
in which each example is described by a set of features and possibly also a label.
So here, we have a set of examples in the lines.
And the rows represent a set of features.
So we're talking about having data in the form of feature vectors x and a label y.
So the label y would tell us whether there is a label attached to each example.
An example of that, if you consider the set of fruits, fruit one to fruit N,
you could have for example 10,000 fruits.
And for each fruit, we have a measurement of the length of the fruit,
the width of the fruit, and the weight of the fruit.
So these represent the feature values for the fruits.
And we have here the 10,000 fruits.
And each of the fruit will be labeled by the kind of fruit that we're
talking about.
So for example, the fruit number one of length 165 millimeters
with 38 millimeters and weighting 172 grams is a banana.
So given this configuration of data,
we can do typically two kinds or deploy two kinds of methodologies.
So broadly speaking, machine learning methods can be classified into two
categories, supervised and unsupervised methods.
In the unsupervised learning, we learn models based on unlabeled
data which means that we don't need to know the label altogether.
But we just need to deploy the methodology used on the left-hand side of this table,
which is the feature vectors describing the examples.
However in supervised learning,
we talk about learning models based on labeled data, which means that we need to
use this information attached to each example to learn supervised models.
In the unsupervised setting, we use training data as a set of example.
So we are going to only use the x's, right?
And the goal is to find some clustering or
segmentation of the data which is to build a model that we call F.
And this model will be mapping the input vectors, the x's, to some set of clusters.
So we want to be able to say these examples belong to some cluster.
Example of this includes find clusters in the population of data,
for example or classify fruit or species of fish.
In our previous example, we could plot the data or
the examples on a 2D space in which we could put the length of the fruit
in the x-axis, and the weight of the fruit in the y-axis.
So we would see naturally the bananas coming up together and
the oranges coming up together.
So this would be our group of banana would be here because they are longer and
their heads are thinner.
And our group of oranges should lay out somewhere here.
So we can deploy clustering methods in order to uncover these groups
in the data.
In this case, who could discover these two ellipsoid set for
each of the categories of fruits.
In the methods that do unsupervised learning include and clustering include
K means, Gaussian mixtures, hierarchical clustering, spectral clustering, etc.
In a supervised setting,
we can use examples with a feature of vectors along with their labels.
The goal is: given a y label that is discrete.
We simplify to y is equal to minus 1 plus 1,
which means that we have access to positive or negative examples.
So in our previous example of fruits,
we could call all the bananas positive examples.
And we could call all our oranges negative examples.
So given examples along with their labels, the goal of supervised learning
is to find a classification, or a supervised model,
that maps the future values that are in our D, given that we have D dimensions,
into a class label that tells whether the fruit is a banana or an apple.
Examples of supervised learning include approving credit yes or no for
a bank or your email classifying a new email as a spam or
ham, or also in our toy example, classifying bananas from oranges.
So in general, we like to plot the data just to visualize a little bit,
here on two features, suppose we have again let's say the length and
the width are other kinds of features.
So we're going to plot all the bananas and all the oranges on the 2D space.
So we're going to put a plus on all the fruits that are bananas because we know
the label.
And we're going to put a minus on all the fruits that are oranges because again we
know the label.
The goal is to cover what we call a decision boundary that separates
the bananas from the apples or positive from negative examples.
So in this case, this is a possible decision boundary that
tells what is the area of the bananas from the area of the oranges.
Example of methods include support vector machines, neural networks,
decision trees, K-nearest neighbors; Naive Bayes, and many more.
So you're going to see a subset of things.
And you're going to see even more methods
in your second course dedicated to machine learning.
So classification can be linear.
So just like in this case where the data is really well-behaved.
We have one side completely dedicated to positive examples,
the other side only our negative examples.
It's good that the decision boundary could be something curvy like this.
It could be also some more complicated shapes like this case here or here.
And if we have two or more classes, the decision boundary could separate each
class separately, suppose you have oranges, bananas, and apples in this case,
you have three classes.
And you want a decision boundary that separates
these three categories of fruits.
Classification could be a little bit more complicated such as this example here.
Suppose you have all your positive examples in the middle
surrounded by a lot of negative examples.
It's not feasible to find a decision that is linear to separate the data.
The reason is that no matter how you would pick your line, the line would do
a better job at classifying positive from a negative example.
Wherever I pick the lines gonna be, it's gonna do poorly on the data.
So the idea here is that we're going to,
as you can see this data is in a two-dimensional space.
We have x1 and x2 as features.
It's possible through machine learning methodologies to put this data
in another space,
which is in this case a three dimension by transforming the features x1 and x2.
And in this new space we're going to be able to find a linear decision boundary
whereas in the initial space the decision boundary was actually
this ellipse around the positive examples.
So in this case,
we're going to kind of pull this positive example out to make them into 3D and
then find the linear separation between positive and negative example.
This is something that is possible with machine learning new methodologies.
So there's another category of supervised learning which is regression.
In this case, we still have our feature values and labels.
But the labels happen to be actually in R,
which means that instead of having a label plus or minus,
we would have a real value that tells for example with the amount of credit.
In this case, what we want to cover is what we want find is rather
a function or model that goes still from our D, from the D dimensional space.
But that would map this to a real value which is the label that we have.
We call f in this case a regressor rather than a classifier.
And examples of that include, for example for a bank to address to find
the amount of credit or if you have supposed the length of the fruit,
can you try to build the model to predict or to find the weight of the fruit.
As an example, suppose you have data in which you have one feature,
it could be for example the length of the fruit.
And you want to be able to predict the weight of the fruit.
So it's possible to plot the data.
In this case, we don't have pluses and minuses.
But we just have data points for which we want to predict,
to mark the weight of the fruit based on the features that we are considering.
Other examples include doing the regression to predict the income in
function of age or, as I said, the weight of the fruit in function of its length.
So in this case, we're going to try to fit some curve through the data points.
This is the first possibility.
So we could try to fit in the linear function through the data points.
You could also try to find something curvy that would map the shape of the points, or
finally we really try to find a more complicated kind of
function that goes through all the data points to really match the data exactly.
So in a typical setting, what we do whether it's in a supervised or
unsupervised is to do a training and
testing scenario in which we start with training data so
could be feature values with label, yes or no for the label depending on the method.
We deploy some machine learning algorithm.
So we cited some of them, support vector machines, K means,
etc., to build what we call a model or function F.
Then, given this model,
this model has learnt from previous experience to offer a present data.
We're going to try to use it to make a prediction.
So suppose we have in the case of a bank.
In the bank example, suppose we have the income, the gender, the age,
the family status, and the zip code of the customer.
We want to be able to use the model to predict, in the case of classification,
whether the [edit]customer should get a credit, yes or no, and in the case of regression,
what would be the credit amount that the bank should give to the customer.
Other examples, we could plug in the length, the width of the fruit and
predict the kind of fruit, whether it's a banana, or an orange, or whatever, okay?
So this is the typical setting in which we want to do a training, build a model.
And this model is readily available to be tested on new instances
that were not used for training.

