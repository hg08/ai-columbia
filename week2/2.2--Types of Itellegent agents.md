## Types of Itellegent agents

Let's illustrate this concept of environment types

on different environments and different agents.



So first of all, for the 8-puzzle,

it's a fully observable because you could

see the game all the time.

You use only one player, so it's a single agent.

It's deterministic and also static.

There's nothing going on around you,

so there's no changes going on.

And there's no chance random that things will change

the course of your actions.

And finally, it's discrete, because there

is a finite number of things you can do.

So the descriptions are similar for chess,

except that you have a multi-agent, since you

have more than one player.

And you also have the environment

can be either semi-static, if you have a clock, in which case

the score of the player will be actually decreasing

as the clock ticks, or fully static if there is no clock.

Concerning poker and backgammon, they

are both actually stochastic, because there

is a chance to have randomness happening by rolling dice.

So basically, they are both stochastic, not deterministic.

They are multi-agent since you have more than one player.

They are both static-- the environment is not changing--

and they are both discrete.

However, poker is partially observable

because it can see the cards on your hands, open it.

And backgammon is fully observable,

because you see the whole board and everything is really clear.

Finally, if we take the example of self-driving car,

or a taxi cab, and also a Roomba,

they're both actually continuous,

because they both evolve in a continuous space and time.

They are both dynamic, because what's going around

is actually quite changing.



So there are other cars going on, there

are other things going on in the room-- kids playing, 等等.

They are both stochastic.

1. For the vacuum agent, or the Roomba, actually,
2. dirt can happen at any time.
3. So the environment, it can be deterministic.
4. But sometimes, you could say that it is deterministic,
5. such as in our small example of the simple vacuum word.
6. For a car, it's multi-agent.
7. For Roomba, it's single agent, unless you
8. decide to have several Roomba cleaning your house.
9. And finally, they are both partially observable,
10. because you can see that next block, or behind this big truck
11. ahead of you.
12. And for the Roomba, you can see end of the sofa,
13. or what's going on in the next room.
14. Actually, these environment descriptions
15. are really not set in stone, and depend a lot
16. on how the task environment is defined.
17. So this is just given as a reference.
18. But depending on the kind of Roomba
19. you have, the kind of car you have, and so on and so forth,
20. these things can change a little bit.
21. There are also four basic types of agents
22. in order of increasing generality-- simple reflex
23. agents, mobile-based reflex agents, goal-based agents,
24. and utility-based agents-- all of which
25. can actually be generalized to learning agents
26. that can improve their performance
27. and generate better actions.
28. So first of all, what are simple reflex agents?
29. These are agents that will select an action based
30. on the current state, ignoring all
31. of the history-- of percept history.
32. It's simple but, of course, limited, because it can only
33. work if the environment is fully observable-- that is,
34. the correct action is based on the current percept only.
35. Simple reflex agents are illustrated with this diagram,
36. here, that looks like the general diagram
37. which we saw earlier.
38. So we have, still, environment.
39. We have the agent.
40. But in this case, we are going to use a condition action
41. rule that will actually map each percept to the action it should
42. do.
43. To illustrate the concept of simple reflex agent,
44. let's take, again, our example of vacuum, the simple world
45. in which we have two rooms, A and B, and the vacuum
46. needs to figure out what to do.
47. So the percepts are the location and the content
48. that are detected by the location sensor,
49. and the dirt sensor.
50. And the actions are going left, right, suck or do nothing.
51. So we have seen that, actually, we
52. could write a table in which we have
53. the percepts and the actions.
54. For example, if A, clean, go to the right.
55. If A is dirty, suck it, et cetera--
56. suck the dirt, et cetera.
57. So let's start, first, by writing
58. the function that actually does the vacuum cleaning.
59. Let's call a vacuum agent.
60. And this function will require to send two parameters, which
61. are the location and the state-- the state of whether it's
62. dirty or not.
63. And this function will actually return the action
64. to do based on the table you have here.
65. All right?
66. So it's a simple if statement in which we have,
67. if the status of the room is dirty,
68. then you're going to just suck.
69. Right?
70. But in this case, we are going to return.
71. Since we are returning an action,
72. we are going to return the action suck.
73. Right?
74. Else means that the status is clean.
75. If we are here, and the room is clean,
76. we need to go in this next room.
77. If we are here and the room is clean, we go to the left.
78. So if the status is dirty, then we could ask,
79. if the location is A, then I'm going to return what?
80. Return the actions of going right.
81. Else, I'm going to return the actions of going left.
82. This is a very simple reflex agent that just, actually, you
83. look up in this table.
84. And check, actually, which one of the actions
85. is the most appropriate, based on the percepts
86. that it is proceeding-- only one percept that it's
87. perceiving now.
88. One question is if the vacuum agent is actually deprived
89. from its location sensor.
90. So what's going to happen?
91. What's going to happen is that we don't have,
92. actually, these values anymore.
93. We just know that it's clean or dirty.
94. So if it's clean, we don't know what
95. to do-- whether we should go right or left, because we don't
96. know in which room we are.
97. And this is the problem with the reflex agent
98. is that they are limited if they are partially observable.
99. So in this case, the fact is that what's
100. going to happen is, if we are already in the room A,
101. and we don't know, actually, what to do,
102. because we don't know in which room we are, and we ask to go,
103. actually, left, then we're going to have loops happening here,
104. because we're going to be not finishing anymore.
105. So if we are A, and it's clear, and we go left,
106. left, left, left, and same thing if we are in B,
107. and we go right, right, right, right,
108. so that's going to be a problem.
109. So we're going to not avoid having this infinite loop
110. if we have, actually, one of the sensors actually
111. not working anymore.
112. So a trick that we could do here is
113. to turn, actually, the simple deterministic reflex agent
114. into a randomized one, which means
115. that, when the room is clean, and we don't know in which room
116. we are, the agent could simply flip a coin and decide,
117. based on the coin, what to do.
118. So we are going to turn it into a randomized simple agent.
119. And we decide between left and right by just flipping a coin.
120. It's actually going to be better than a simple reflex agent that
121. actually gets stuck in a state, and can't really move forward.
122. Model-based reflex agents actually
123. handle partial observability by keeping
124. track of the part of the word it can see now.
125. It's actually an maintained that we
126. call an internal state that depends
127. on the percept history, which is actually, usually,
128. a best guess of what's going on, rather than an exact percept
129. history.
130. It models the world based on what?
131. Based on two things-- how the world evolves, independently
132. of the agent, and how the agent actions affect the world.
133. So in this case, we're going to have a similar scenario
134. in which we have sensors, actuators,
135. what the world is like now.
136. But we're trying to model the world as a state.
137. So how the world evolves, and what my actions will do,
138. would help define the model of the world.
139. And then, we still have the condition action rules,
140. because we have a reflex agent.
141. But in this case, we're maintaining
142. a model of the world based on history of the percepts-- which
143. makes a big difference in terms of deciding what to do next.
144. Goal-based agents are needed when
145. knowing the current state of the world is not enough.
146. The agents need something extra, and this something
147. is some goal information.
148. The agent program, in this case, combines the goal information
149. with the environment model to choose the actions that
150. achieve that goal.
151. So it actually considers the future with some questions
152. like, what would happen if I do A?
153. So there is some thinking and deliberation,
154. instead of just checking a list of actions
155. and reacting with a reflex.
156. So in this case, we are going to think
157. through different possibilities, and then
158. think about what would be the best choice to achieve
159. the goal of the agent.
160. It's flexible as knowledge supporting the decision
161. is explicitly represented, and can be modified.
162. So in this case, we are going to have goals.
163. And we are not going to have a simple, actually,
164. table-- look-up table.
165. Because such a look-up table can be really huge.
166. Still, sometimes, achieving the goal is not enough.
167. We may look for a quicker, safer, cheaper trip
168. to each destination.
169. Agent happiness, in this case, must be factored in,
170. and taken into consideration.
171. We call it utility if we use the economics and computer science
172. jargon.
173. A utility function is the agent's performance measure.
174. But because of the uncertainty of the world,
175. a utility agent chooses the action
176. that maximizes not simply utility, but the expected
177. or the average utility.
178. So in this case, in the case of the agent,
179. we are going to have the utility function that actually is here,
180. and assess how happy is the agent in such a state.
181. And based on this utility, it with
182. decide what actions to do for the actuators.
183. So I've several types of agents, going from simple reflex agent
184. into utility agent.
185. However, one quick think, that programming such agents
186. can be very, very tedious.
187. Because they need, for example, for a reflex agent
188. to enumerate all possible percept actions
189. I need to define the model of the world, which
190. is not an easy task to do.
191. So as Alan Turing said in 1950, in his famous paper,
192. some more expeditious methods seem reasonable.
193. We need to find something else.
194. And this something else comes from a learning agents.
195. This generalized, actually, all the previous agents
196. we have seen.
197. So a learning agent has four conceptual components.
198. And these are, first of all, a learning element.
199. And learning element is responsible for making
200. improvements.
201. This is what actually makes the agent learn from the past.
202. A performance element is actually
203. what is responsible for selecting external actions.
204. It is what we have seen so far as agent.
205. A critique is how well is the agent doing with respect
206. to a fixed performance standard.
207. And finally, a problem generator that actually
208. allows the agents to explore different possibilities,
209. and learn on the go.
210. Agent internal states can be represented in different ways.
211. So we have the atomic representation,
212. in which each state of the world is a black box that
213. has no internal structure.
214. For example, finding a route-- driving route-- where each city
215. is considered actually a state.
216. So suppose you are in city B, and you
217. want to go to city C. Right?
218. All you want to find is the route between the two,
219. and we don't care about the internal structure of city B
220. and of city C.
221. So we are going, basically, to just consider
222. this as black boxes.
223. And all we want is to go from a state to another one.
224. AI algorithms that leverage this kind of representations
225. are search, games, Markov decision
226. processes, hidden Markov models, et cetera.
227. A second way of representing the information
228. is called factored representation,
229. in which each state has some attributes value properties.
230. So in this case, going from state B
231. to state C, we have some information such as,
232. for example, our GPS location, the amount of gas
233. we have in the tank, and so on, and so forth.
234. And we have two states, B and C, that could actually share
235. different characteristics.
236. For example, these two have a flag on this second attribute.
237. So it's an attribute value information
238. that is added to the state.
239. So this is a state.
240. actually, the state, we know it as, not only a black box,
241. but we have information that are actually feature values,
242. or attribute value information.
243. These are properties of the states.
244. Artificial intelligence algorithm
245. that leverages organization includes
246. constraint satisfaction problems,
247. and Bayesian networks, to cite a few.
248. In the third representation, called
249. structured representation, relationships
250. between the object of state can be explicitly expressed.
251. So we have a state A and a state B.
252. And actually, each of the states not
253. only have attribute values that describe the state,
254. it has also a relationship between the objects.
255. For example, pedestrian crossing the street.
256. So there is a relationship between pedestrian and street,
257. et cetera.
258. So the algorithm leveraging this kind of structure
259. includes first order logic-- in which
260. have relationship between objects--
261. knowledge-based learning, natural language process
262. understanding, et cetera.
263. So we had we went through several concepts today.
264. We went through the concept of rational agent,
265. the concept of different types of environment,
266. different types of agents, and also the different types
267. of representations of states and agents.
268. So I would like to conclude by saying
269. that the concept of intelligent agent
270. is actually central in artificial intelligence.
271. AI aims to design intelligent agents that
272. are useful, reactive, autonomous,
273. and even social and proactive.
274. An agent perceives its environment through percepts,
275. and acts through actuators.
276. We also saw that there is a performance
277. measure that evaluates the behavior of the agent.
278. And an agent that acts to maximize
279. its expected performance measure is called a rational agent.
280. We also so that we can define different criteria
281. for rational agent environment.
282. And this is called PEAS-- a task environment specification that
283. includes performance measure, environment, actuators,
284. and sensors.
285. We also saw an important aspect that, actually, an agent
286. is a set of architecture, or hardware,
287. plus a program or system.
288. And both have to be compatible to work together.
289. We also saw four types of agents.
290. Reflex agents, that are the simplest ones.
291. Mobile-based agent that try to model the world by creating
292. the model internally.
293. A goal-based agent that wants to model the world, but still
294. have a goal to achieve.
295. And finally, utility-based agents
296. that try to increase their happiness.
297. Agents can improve their performance
298. through learning, which is a central piece today in AI that
299. actually aims to make the agent aware of different situations,
300. and learn from its experience.
301. So this was a high-level representation
302. of the different agents programs.
303. We have seen, lastly, that state representation can
304. be either atomic, factored, or structured, which
305. goes by increasing order of expressiveness.
306. Finally, I would like to finish with this beautiful diagram
307. that actually shows the road map for the next lectures in AI.
308. Our next lecture is in AI-- that shows the road map
309. for our next lectures in AI.
310. And this includes, actually, on the axis,
311. here, we have the level of intelligence,
312. which is also the expressiveness power of the AI agents.
313. So we have, first, the reflex agents.
314. And we saw that reflex agents are very simple.
315. They have a look up table that has, actually, precepts
316. and has actions.
317. And all that the reflex agent is doing is, for a given percept,
318. pick the action of corresponding to that precept.
319. So this is considered as a lower level of intelligence,
320. because there is no much thinking or deliberation
321. about what to do.
322. Just look up this table-- it can be, actually, very big--
323. and decide what to do.
324. 
325. We have seen that state representation could be atomic,
326. and could be considered as a state, in which it's
327. a black box.
328. And this includes search problems,
329. such as finding a route, or Markov decision processes,
330. or games.
331. We also have seen the factored state representation,
332. in which we have variables describing the state.
333. And this includes problems in constraint satisfaction
334. and Bayesian network.
335. And finally, we have seen the state where
336. we have an increased level of expressiveness that are structured
337. in logic, in which we have, actually,
338. more information being encoded in the state that actually
339. includes the relationship between the different objects
340. describing the state.
341. Finally, machine learning provides a set
342. of techniques and methodologies that
343. can produce agents that go across all
344. of this level of intelligence and expressive powers.
345. So you could have, for example, of spam filter
346. that could predict whether an email is a spam or not.
347. That would be a reflex agent.
348. And the complexity and expensiveness
349. of the knowledge discovered by machine learning algorithms
350. could go as high as logic expressions, and first order
351. logic, and also natural language processing.
352. So this was it for intelligent agents today.
353. Next time, we are going to start the topic of search and games,