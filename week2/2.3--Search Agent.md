## 2.3 Search Agent

Welcome to your first topic in artificial intelligence.

This topic will deal with **search agent** or planning agents.

So far we have been talking about what

to call **reflex agents**.

reflex代理are agent that use a mapping from states to actions.

也就是说, reflex agent has a look up table,

and they know what to do according to the states.

Goal-based agents operate differently.

But instead of having this predefined list of states

to actions, the goal-based agents

would have to find a solution for the problem.

Just to put things back in context,

we are going to start, first, by drawing

the level of intelligence, so that's

level, or intelligence level.



On this side, we are going to put agents

1. with high level of intelligence.

2. On this side, we are going to put agents

3. with lower level of intelligence, right?

4. So we could put our reflex agents at this level.

   So reflex agents are just agents that

   don't have any intelligence.

   They just have a look up table.

   They look what to do according to kind of dictionary

   or look up table, all right?

   So reflex agents are considered as one

   of the lowest level of intelligence.

   We are going today to introduce what

   we call **state-based agents**.

   And it turned out that, actually,

   search algorithms, or search agents,

   are one of the categories of state-based agents, OK?

   So we're going to, as we go in this course,

   fill this access with more and more intelligence agents,

   as we see more and more topics in artificial intelligence.

   So what are goal-based agents?

   These agents that work toward a goal, just like the name

   indicates.

   This are agents that consider the impact of actions

   on future states, which means that their job is

   to identify the actions or series of actions

   that lead to the goal.

   In AI, this is formalized as a search

5. through possible set of solutions.

6. So in general, the search space of the possible solutions

7. is large enough to prevent from putting

8. this information, or the solutions,

9. into a look-up table.

10. And hence, reflex agents won't be effective in this case.

11. We need more of a goal-based agent that

12. knows how to search through the possibilities,

13. and get the best solutions to the goal.

14. To illustrate what goal-based agents do,

15. let's take a couple toy examples.

16. The first one is a maze.

17. We all played with mazes.

18. So for example, in this maze, we're

19. trying to figure out a way to go from a starting point, s,

20. here, to a goal, g, in the middle of the maze, all right?

21. Different ways we could do it.

22. One possible way is to go, let's say, this way.

23. To the right, and then another one to the right.

24. And then to the left, and then go to the left.

25. And then keep going.

26. Search for a possibility to reach this point

27. in the middle of the maze.

28. And then, I'm actually getting stuck here.

29. So you could try other alternatives.

30. So maybe, let me try to go from the left.

31. And then going this way.

32. And then here again, I'm stuck.

33. There's no way I could get to the point,

34. to the goal in the middle.

35. So let me try, last possibility is let's try the first path,

36. but may be change a little bit direction once we reach,

37. we get closer to the goal.

38. So maybe instead of going to the left here,

39. let me go to the right.

40. Nope, that didn't work.

41. Last one, hopefully this will be a nice tentative,

42. a successful one.

43. So here I'm going to the left.

44. And maybe move it here.

45. And then here.

46. And then here, got it.

47. OK, so the blue path was the most-- the blue is the success.

48. The blue is a success path that does actually lead to the goal.

49. All right, so what did I do?

50. I just explored my search space to find one possibility, one

51. path, that led me to the goal.

52. So what search agent do in general, is something similar.

53. They would actually explore the search space

54. to find the path between the start

55. and the goal of the search problem.

56. Another toy example is called the eighth queen problem.

57. Probably you played with that.

58. So you have a chess board, on which

59. we to want to place eight queens,

60. so that no queen is attacking another one.

61. So how do queens attack another one?

62. If they are placed on the same line,

63. either horizontally, vertically, or diagonally.

64. So this is an example, successful example

65. in which you put the eight queens such as no one

66. is kidding the other one, or attacking the other one.

67. So if I put one queen here, then this one will attack it

68. and it won't work.

69. So this is a successful state, that this

70. is a goal state, right?

71. A goal, what you call the goal state, in which, actually, we

72. reached a point where we were able to put

73. eight queen on the chessboard.

74. Let's think first about how we can

75. count the number of possibilities,

76. or the size of the search space.

77. So we have here 8 times 8 cells or positions

78. on the chess board, which makes 64 possibilities

79. to put the first queen.

80. So the first queen could be put any of the 64 positions.

81. The second queen would be put into 64 minus 12,

82. since we already placed one queen, which

83. would be leading us to 63 possible solutions,

84. possibly to put the second queen.

85. And so on and so forth, are going

86. to go down and into 57, because we only have eight queens.

87. This represent a search space of 1.8 times 10

88. to the 14 possible solutions to put the queens

89. on the chess board.

90. That's huge.

91. This is all possible sequences in which

92. we put the eight queens.

93. Some of them are successful.

94. Some of them are not.

95. And the idea of a goal-based agent

96. is to find one successful possibility to put

97. the queens on the chess board.

98. Problem solving as a search problem can

99. be defined through one, defining what's the goal to achieve,

100. and two, formulating the problem.

101. These are two important aspects to tackle

102. if you want to design a goal-based,

103. or search-based agents.

104. So I would like to emphasize a subtlety here,

105. which is that solving the problem

106. is actually a two stage process.

107. First, we need to search mentally, or offline,

108. by exploring the set of several possibilities.

109. And then we execute the plan or the solution found.

110. So if you remember in the maze example,

111. I did write the different paths that would lead from the start

112. to the goal.

113. But that could also do that mentally or offline,

114. and then once I found this a successful solution,

115. I would execute it.

116. So similarly a goal-based agent won't really

117. try and go ahead and try different possibilities.

118. And go back to the start and start over.

119. It will do the calculations mentally, and once it's found,

120. suppose it's a robot, once it's found the successful solutions

121. to go from point A to B, then it will execute it

122. by moving effectively to that position.

123. So we need to execute the solution found

124. once we have explored and found the solution that

125. leads to the goal, OK?

126. So the problem formulation is, in general, done as follows.

127. First of all, we need to identify

128. what is our initial state, the state

129. in which the agent starts.

130. And I was the agents when I did the maze.

131. I started when I was in the lower circle of the maze.

132. What are the states?

133. These are all states reachable from the initial state

134. by any sequence of actions.

135. It's also called the state space.

136. So there are different states as you

137. are planning on how to move in the maze, for example,

138. or where to put your queen.

139. You are creating new states that actually

140. change from the initial state to other kinds of possibilities.

141. So this is called the state space.

142. The actions are the possible actions

143. available to the agents.

144. For example, I could go ahead go, right, left on the maze.

145. I could also put the queen on the top left corner,

146. or the bottom right corner, and so on and so forth.

147. So I'm doing actions that will actually change a state

148. from one case to another one.

149. So actions are possible actions available to the agents.

150. At the state s, actions of s will

151. represent all possible acts that could

152. be executed in this state.

153. So this is called the action space.

154. So here I'm going to talk about action space and state

155. space to define the different state of the problem,

156. and the different actions of the problem given each state.

157. We also define what to call a transition model.

158. This helps us, actually, describe

159. of what each action does, which means

160. that's what are the resulted actions from being in the state

161. s, and applying some actions a.

162. We also define what to call a goal test.

163. This actually determines if a given state is a goal states

164. or not.

165. So we're going to have this kind of test that

166. tell us whether we wish to, what we wanted to achieve or not.

167. Finally, we define the last element

168. is called the path cost.

169. It's a function that actually assigns a numeric cost

170. to a path, with respect to some performance measure.

171. It doesn't always exist, but whenever

172. we have a performance measure, we want to reach the goal.

173. We are going to apply this function

174. to know how much this is costing us.

175. It could be a solutions, but there may be other solutions.

176. And you want to see how much each of the solutions costs.