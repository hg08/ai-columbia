# 3.4 局部搜索

 前面的课程中, 我们研究了不同的搜索方法. 今天我们研究局部搜索.
 到此为止，搜索算法都用来设计成系统地探索搜索空间．
 这些搜索算法可用在在这些问题中： 环境用可观测量表示,且是决定性的,已知的，解是多个动作的一个序列.
 但要解决的问题不总是如此．
 真实世界中的问题更加复杂．
 当一个目标被找到, 到达路径的目标就构成了该问题的一个解．
但有时候，路径可能并不重要．
 [If the path does not matter or the systematic search is not
 a possibility, then consider another class
 of problems of such problems called local search.]

 局部搜索算法are iterative improvement
 algorithms．它不仅对于那些我们不能系统地搜索的问题很有用,
它对纯优化问题也很有用.　纯优化问题的目标是根据优化函数找出最佳态.

 例如集成电路设计, 通讯网络优化,等等.
 We also have seen the part of the 8-puzzle or the 8 queen.
 目前，我们将其视为一个搜索问题.
 且我们已经看到可能的态的数目相当大，因为实际上我们考虑此搜索为一个由多个动作构成的序列
 来填充这个棋盘(board)
 (the 8 queen or the 8-puzzle).
 我们并不一定要这么做！

 在这两个问题中，我们关注的实际上是拼图的最终构型，而非到达最终构型所经历的中间的步骤.
 局部搜索的基本思想是：关注(keep)单个当前态，并且试着提高它．这是说我们将只是移动当前态对应的结点的邻居． 其优势就是我们不需要掌握和维护整个搜索树或搜索图,因此此方法将用更少的内存.
 So we use little memory just to know
 that we need to actually keep track of, which can actually
 often find good enough solutions in continuous or large-state
 spaces, where a systematic solution is not possible.
 So it is doing a pretty decent job with less memory.
 Local search algorithms include algorithms
 like hill climbing, steepest ascent or descent,
 which is actually the most basic local search
 algorithms, simulated annealing, that's
 inspired by statistical physics, local beam search,
 and finally, genetic algorithms inspired
 by evolutionary biology.
 Let's go to the state space landscape
 for local search, on which we have on the x-axis the states,
 and on the y-axis the objective function.
 So if the function on the y-axis is the cost,
 then the goal of the search, a local search,
 is to find the lowest valley or the global minimum
 for that function.
 This could be this global minimum or this local minimum.
 But if the y-function is an objective function,
 then the goal of the search is to find the global maximum.
 That's here, which is the highest peak in the landscape.
 Other elements in the topology include flat local maximum,
 from which no uphill is possible,
 or a flat local maximum, from which some uphill is possible,
 what we call a shoulder.
 In both of these cases, local search can get stuck.
 In the field, we're going to explore
 two methods of local search, hill climbing
 and genetic algorithms.
 Hill climbing may constitute the most basic methods
 for local search and is also called a greedy local search
 because it looks only to the immediate good neighbors
 and not beyond.
 So imagine we got in this node here in the current state.
 We're going to look only to the neighbors of this node
 and try to go uphill to reach, preferably, a global maximum,
 but if not, a local maximum.
 So the search moves uphill, which
 move the direction of the increasing element of value
 to find the top of the mountain.
 It terminates when it reaches the peak,
 but also can terminate with a local maximum, ideally
 a global maximum, or can get stuck
 and no progress is possible if, for example, it's
 in a flat local maximum.
 And node in this case is made of two components, a state
 and a value.
 The value would be the value of the objective function
 at the current state here.
 The hill climbing algorithm is pretty simple.
 So it acts as input the initial state.
 That actually is some position on the landscape.
 So suppose I start with this landscape, something
 that looks like this.
 And we have the initial state here.
 This would be our initial state.
 So the idea is to go up the hill until we
 reach that maximum, whether it's a global or minimum.
 That would be the maximum.

 So the idea is given this initial state,
 return the local maximum.
 That could be a global maximum if we're lucky.
 All right, so we start with initialising the current state
 as the initial state.
 And we're going uphill starting from this element.
 So the idea is to look in the neighbors
 and pick the highest valued successor of the current.
 So we're going to pick the neighbors here and pick
 this one.
 And at each point, the current node
 is replaced by the neighbor with the best
 or with the highest value.
 So we're going to replace this element here,
 the current node, by this one, and then
 by this one, et cetera, until at some point
 we reach the point where the next neighbor or the highest
 element has a lower value than the current neighbor, which
 means in this case that we reached that maximum.
 And if we go any further, we're going
 to go down instead of going up.
 So if we find this highest point,
 then we're going to return the current.state.
 Remember, each node has a state and a value.
 And so we are going to use the value to go up the hill.
 But then once we find the best value,
 we stop and we return that state.
 Hill climbing algorithms have several variants.
 And this includes sideways moves in which
 you want to escape a plateau such as this local maxima
 or this shoulder.
 And the idea is, if we move sideways,
 there is a hope that we're going to escape from this flat area
 and reach a shoulder.
 We can put a limit on the number of sideways moves
 as we avoid infinite loops.
 Another variant is called a random start
 in which we do hill climbing several times
 to overcome local maxima.
 This means keep trying until you get a better solution.
 So it depends on the problems.
 If the problem requires to find the goal,
 then we could do that several times until we find the goal.
 And if the problem is to optimize
 some objective function, we could do the local search
 several times until we reach the maximum value
 for this function.
 And finally, stochastic hill climbing
 chosen at random among the uphill
 moves in order to find a better solution than just looking just
 around the current state.
 Hill climbing algorithms are effectively general,
 but depends a lot on the landscape shape.
 So for example, if we have one or more local maxima,
 if we had many shoulders, this search
 can be more or less effective.
 It has been successfully applied in many real world problems
 after reasonable number of restarts.
 One of the variance of hill climbing
 is called local beam search in which
 we maintain k states instead of maintaining
 only one single state.
 But the other difference is that the states
 are communicating useful information to make
 a more effective search.
 In order to help alleviate the problem of the states
 agglomerating, which may happen when states communicate,
 we use what we call **stochastic beam search** in which we
 choose k successors at random.
 Now how about genetic algorithms?
 Genetic algorithms is a variant of stochastic beam search
 algorithms in which successor states are generated
 by combining two parents rather than
 by modifying a single state.
 The process is heavily inspired by natural selection,
 in which we start with k randomly generated
 states, called population.
 该population中的每一个元素称为一个 individual.
 An individual is通常表示为０和１构成的字符串, 或表示为数字构成的字符串--
 由元素构成的有限集合.
 目标函数称为fitness function,
 in which various states have a better fitness or better
 objective function, or higher values as
 compared to the other states.
 例如，在8皇后程序中， an individual
 can be presented by a string of digits between one and eight
 that represent the position of the 8 queen
 in the eighth column.
 So for example for this first column,
 the queen is looked as at position three.
 In the second one, it's position two.
 In this one, position seven, then
 position four, position eight, position seven, six, five,
 and five, and then at the end, position two.
 So this string here would represent actually
 the encoding of this state in the search space.
 对于8皇后问题，一个可能的fitness函数可以是不相冲突的皇后的对数.
 And for solutions we represent 28, which is 7 plus 6
 plus 5 plus all the way down to
 So this would represent a number of nonattacking pairs
 of queens in a successful board of the 8 queen.
 In genetic algorithms, pairs of individuals
 are selected at random for reproduction,
 with respect to some probabilities.
 A crossover point is chosen randomly in the string.
 And at this crossover point, we are
 going to cross the parents to create an offspring.
 Each element in this string is also
 subject to some mutation with some small probability.
 In the case of the 8 queen, it's possible to cross over
 the boards.
 For example, if we have these two unsuccessful boards.
 So one of them is parent one, and the second parent,
 it's possible to cross over these two parents
 to build a new board.
 The crossover point would be this point here.
 And we get, from the first board, we get this first part.
 And we put that in the child or in the offspring.
 And from the second board we get the second part.
 And thus we did the crossover of these two parents
 to build a new board.
 So This was a representation we have seen before
 to represent each board.
 So remember, it's a sequence of digits.
 So suppose we have four boards, b1, b2, b3, and b
 And we want to generate from these four individuals new
 boards.
 So we could first do some selection.
 So we would select board one, two, and three.
 We don't select board four.
 Note here that the fitness function for this board
 is given by this red value here.
 24, 23, 20, and 11 corresponds to the number
 of pairs of queens that are not attacking each other.
 So we are not reaching 28, which is the best solution.
 All right, so based on this, I'm going
 to select some pairs of elements that will
 play as the role of parents.
 And we do some crossover.
 So select these two parents.
 We select some crossover point here, for these two,
 and crossover point here for this second pair.
 So we're going to build four new individuals.
 That's actually our mix of the parents.
 So for example, for this one is a mix of this individual here
 getting the first three digits from that,
 and the last digits coming from the second individual.
 So from this we do a crossover.
 We get these four new boards.
 After that, we could apply the mutation
 with some small probability, such as replacing the five here
 by one, replacing the element seven here by two,
 and replacing the element one here by seven.
 The following algorithm reflects the process
 that we just described.
 So the algorithm receives as input the population,
 and the fitness function that we want to maximize.
 And we return an individual.
 And this individual is the best one
 in the population with respect to that fitness function.
 So the process is to start a new population.
 Initially the population is empty.
 There are no individuals.
 And we're going to go over our population,
 do the process of doing the selection, selection
 of the parents to cross over, cross over.
 

And finally do some mutation with
 some small random probability.
 So this will create a new child.
 And we need to add this child to the new population.
 And we start the process over until actually either we
 have some good individual in the population that are fit enough,
 or that time has elapsed.
 So the process is just starting from this population,
 create a new population by crossing
 over the old population, mutation,
 and then keep doing it until the population has some more
 variability, and with a good chance
 to have some individuals that are fit enough
 to be presented as a solution.
 

Genetic algorithms have long been
 used in optimization problems, and perform quite well
 in applications such as circuit layout design.
 We will see later in this course an application of genetic algorithms to association.

