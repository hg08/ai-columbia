## AI Course Outline

### **Welcome to Artificial Intelligence!** 

In this course, you will learn the fundamental concepts of Artificial Intelligence (AI) and apply them to the design and implementation of intelligent agents that solve real-world AI problems, including problems in search, games, machine learning, logic, and constraint satisfaction.

We will provide a broad understanding of the basic techniques for building intelligent computer systems.  Topics include the history of AI, intelligent agents, state-space problem representations, uninformed and heuristic search, game playing and adversarial search, logical agents, constraint satisfaction problems, along with techniques in machine learning and other applications of AI, such as natural language processing (NLP).

#### **Course Level**

Please note this is a **Master’s/graduate level course**. Expect to spend at least several hours to complete the programming assignments, although the exact amount of time will depend on your background and proficiency with coding. If you are taking this course for fun, and are not working towards a passing grade for credit, you can, of course, watch the lectures and answer the quizzes.

#### **Prerequisites**

Students are required to have the following prerequisites: 

- Linear algebra (vectors, matrices, derivatives)
  - Calculus
  - Basic probability theory
  - Python programming

The course offers an excellent opportunity for students to dive into Python while solving AI problems and learning its applications. Programming assignments will be in Python. 

#### **Class Schedule**

This is a 12-week course and the weekly materials will be released on a Monday. Each week includes **lectures**, supplemental materials, as well as **assignments**. Please note that edX courses use Coordinated Universal Time (UTC) for all due dates and release times.  

Week 1: Introduction to AI, history of AI, course logistics, and roadmap

Week 2:  Intelligent agents, uninformed search

Week 3: Heuristic search, greedy search, A* algorithm, stochastic search

Week 4: Adversarial search, game playing

Week 5: Machine Learning 1: basic concepts, linear models, K nearest neighbors, overfitting

Week 6: Machine Learning 2: perceptrons, neural networks, naive Bayes

Week 7: Machine Learning 3: Decision trees, ensemble, logistic regression, and unsupervised learning

Week 8: Constraint satisfaction problems

Week 9: Markov decision processes, reinforcement learning. 

Week 10: Logical agents, propositional logic and first order logic

Week 11: AI applications to natural language processing (NLP)

Week 12: AI applications and course review 

#### **Assignments**

There will be two kinds of assignments:

**Quizzes (conceptual):** These test your understanding of the lectures. You may be asked to reason abstractly about the nature of an algorithm,or to perform a technique by hand on an small problem. Please read the instructions carefully, note any formatting requirements, and review your answers before hitting submit. Except for the most challenging questions, you will often only have one attempt to answer a question.

**Projects (programming):** These offer an excellent opportunity for you to dive into Python programming and design while solving AI problems and learning its applications. You will often be presented with a general problem and asked to come up with solutions to the problem by implementing algorithms from scratch. As mentioned above, expect to spend at least several hours to complete the programming assignments.

#### **Grading**

*Quizzes (20%):* There will be 11 quizzes worth 2% each for a total of 20%. The lowest score will be dropped. 

*Projects (50%):* There will be **5 projects** in Python worth 10% each for a total of 50%. All projects count. 

*Final Exam (30%)*: There will be a final exam one week after the last lecture. 

#### **Passing Grade**

To pass the course, you must score **60% or above**. 

#### **Honor Code**

**Academic Honesty Policy**

You are required to read, and understand the following agreement regarding **Academic Honesty**. Each student is sole owner of his own code and work and **must NOT**:

- - Submit work that is not original.
  - Publish code or solutions online.
  - Post the course questions on forums including stack overflow.
  - Submit someone else’s work, or a modification of that work, with or without that person’s knowledge.
  - Allow someone else to submit his/her work, or a modification of that work.
  - Solve as a group a quiz or project. **All coursework is to be done by the student working alone.** 
  - Contract course work out to others.
  - Plan or execute with another student a cooperative subterfuge during an exam.
  - Make use of unauthorized material during an exam.

Project assignments will be checked with plagiarism detection software.

Thank you for abiding by these rules. Doing so will ensure the experience is fair to everyone taking this class or the future sessions of this class. 

#### **Suggested Readings**

We recommend but do not require this book, which is the main reference in the field:  

***Artificial Intelligence, A Modern Approach. Stuart Russell and Peter Norvig. Third Edition. Pearson Education.***

Check out the book resources  <http://aima.cs.berkeley.edu/>

Check out the list of readings, useful links we suggest for this course. 







## Course Information and Support

 Bookmark this page

### **COURSE INFORMATION AND SUPPORT**

The following links and resources will help you know where to go, and who to ask about any issues or concerns you may have.

**TECHNICAL SUPPORT**

If you experience any technical challenges connecting to edX or elements of the courseware, please contact edX technical support using the "Help" tab in discussion forums, or through info@edx.org. 

**COURSE TEAM AND WEEKLY DISCUSSION FORUMS**

If you have a question about the content, assignments, or discussions in the course, please use the weekly discussion forums and remember to check carefully to see if your question has been asked. Please use this option to ask course-wide questions that you believe will benefit everyone. The course team will respond to questions within 48 hours. You are encouraged to answer questions if you know the answer and can be a community support. The course team will pin frequently asked questions to the top of the forum.

#### Policies

**Academic Honesty Policy**

You are required to read, and understand the following agreement regarding Academic Honesty. Each student is sole owner of his own code and work and must NOT: 

- Submit work that is not original. 
- Publish code or solutions online. 
- Post the course questions on forums including stack overflow. 
- Submit someone else’s work, or a modification of that work, with or without that person’s knowledge. 
- Allow someone else to submit his/her work, or a modification of that work. 
- Solve as a group a quiz or project. 
- All coursework is to be done by the student working alone. 
- Contract course work out to others. 
- Plan or execute with another student a cooperative subterfuge during an exam. 
- Make use of unauthorized material during an exam. 
- Project assignments will be checked with plagiarism detection software. 


Thank you for abiding by these rules. Doing so will ensure the experience is fair to everyone taking this class or the future sessions of this class.

**For students opting to take classes in the ID Verified track, live exam and assignment proctoring may be required. Proctoring is critical to the accreditation process of identity verification.**

All content provided for the course is for use in association with this course. Please do not reuse or redistribute outside of the course. By downloading any content you agree to these terms.

**Discussion Forum Etiquette**

Conversations are an integral part of your learning for this course. We’ll use the edX Discussion Forum for these purposes:

**Weekly discussion forums** – Each week there will be a discussion board. You can use the discussion forum to ask questions, and you can discuss weekly topics with others in the class.

Posting and reading in the discussion forums can feel overwhelming at first. But if you’ve used Facebook you already know how online conversations can work, so don’t worry too much about this! Please watch the course discussion forum guide video for our best advice on how to manage your experience.

**Discussion Forum Etiquette**

We have two fundamental community rules: Be civil, and don’t post offensive or illegal content.

*Your instruction team will moderate the forums.* If you spot something you think violates our community rules, you can flag the post for our attention.

If you want to raise an issue that needs an answer, select "Question" for the post type.

For more information on the discussion forum, please refer to the [EdX Learner's Guide](http://edx.readthedocs.io/projects/edx-guide-for-students/en/latest/sfd_discussions/add_post.html).

**Important Information about Columbia MicroMasters**

 In order to be eligible for credit for this course you must successfully complete all of the following steps:

1. Register as an [ID Verified Student](https://ecommerce.edx.org/basket/add/?sku=C9E0080) by the course ID Verified Deadline. There is a $199 USD fee to be an ID Verified student for this course. 
2. Complete the course with a grade of 60% or higher. 
3. Complete all 4 MicroMasters courses.
4. Meet admission requirements. 
5. ID Verify before the deadline on **August 26th, 2018 at 23:30 UTC.**
6. Learners in the verified track are required to take the proctored final exam as one of the requirements for credit eligibility. 

**If you fail or do not attempt any of the steps above, you will not be eligible for academic credit, although you still may receive an edX ID Verified certificate if you pass this course.**

Once you fulfill all of these requirements, at the end of the course series you will be eligible to apply for academic credit for all four MicroMasters courses from the Fu Foundation School of Engineering and Applied Science at Columbia University. Completion with a passing grade for all 4 MicroMaster courses qualifies for the edX MicroMasters certificate. A MicroMasters certificate is not transferable to another academic institution.  Academic credits obtained or converted after admission to Columbia University may not be transferable to other academic institution.  

**Accessibility Statement**

Qualified students with disabilities may be eligible to receive academic support services and accommodations.  Every effort is made to provide reasonable accommodations for qualified students with disabilities. Qualified students who wish to request an accommodation for a disability should contact [info@edx.org](mailto:info@edx.org). 







 



## Proctored Exam Guidelines

 Bookmark this page

#### **Proctored Exam Overview and Guidelines**

Proctored exams are exams with time limits that learners complete while online proctoring software monitors their computers and behavior. 

**Learners in the verified track are required to take the proctored final exam as one of the requirements for credit eligibility.** 

**To satisfy the proctored exam requirement for credit eligibility, learners must take the exam as a proctored exam and receive a Satisfactory result for their proctoring session review as well as a passing grade on the exam itself.**

**Please note: Learners cannot complete proctored exams using the edX mobile app.**

Learners who agree to take an exam with online proctoring must install proctoring software, which checks that the person taking the exam is the same person who is taking the course for credit, and also detects any attempts to cheat on the exam.

Learners perform a series of checks on their computer and test environment and must also provide photo identification before being allowed to proceed. The proctoring software then runs in the background, monitoring the test environment and screen activity as the learner takes the exam.

Learners complete a proctored exam, either by submitting their answers or when the time expires for the exam.

Before proctoring session results are available, learners see a Pending result. After their proctoring sessions are available, learners can receive either a Satisfactory or Unsatisfactory result.

Learners can check their proctoring session results by returning to the proctored exam in the course. Their proctoring session review results are updated there when results are received. In addition, learners can go to their Progress pages, where they can check their proctored exam results as a part of their overall credit eligibility status.

**\*Practice proctoring exam will be available in week 7 of the course.***

**For more information about the technical requirements for taking a proctored exam, and edX’s online proctoring rules, please see theEdX Learner's Guide.** 

 



## Week 1: Suggested Readings

 Bookmark this page

1. **Artificial Intelligence: A Modern Approach**, 3rd Edition Russell & Norvig, Chapter 1.

Please note: This book is strongly suggested but is not mandatory to complete the course.

2. Is Artificial Intelligence Permanently Inscrutable?

<http://nautil.us/issue/40/learning/is-artificial-intelligence-permanently-inscrutable>.

This article talks about model correctness versus interpretability, which is a very important "higher-level" concept.   



### Is AI permanently Inscrutable?

Dmitry Malioutov can’t say much about what he built.

As a research scientist at IBM, Malioutov spends part of his time building machine learning systems that solve difficult problems faced by IBM’s corporate clients. One such program was meant for a large insurance corporation. It was a challenging assignment, requiring a sophisticated algorithm. When it came time to describe the results to his client, though, there was a wrinkle. “We couldn’t explain the model to them because they didn’t have the training in machine learning.”

In fact, it may not have helped even if they were machine learning experts. That’s because the model was an artificial neural network, a program that takes in a given type of data—in this case, the insurance company’s customer records—and finds patterns in them. These networks have been in practical use for over half a century, but lately they’ve seen a resurgence, powering breakthroughs in everything from speech recognition and language translation to Go-playing robots and self-driving cars.

![Bornstein-BR-1](http://static.nautil.us/10291_ef1890585bae446a0668afed3012daa2.png)**HIDDEN MEANINGS:** In neural networks, data is passed from layer to layer, undergoing simple transformations at each step. Between the input and output layers are hidden layers, groups of nodes and connections that often bear no human-interpretable patterns or obvious connections to either input or output. “Deep” networks are those with many hidden layers.Michael Nielsen / [NeuralNetworksandDeepLearning.com](http://neuralnetworksanddeeplearning.com/chap1.html)

As exciting as their performance gains have been, though, there’s a troubling fact about modern neural networks: Nobody knows quite how they work. And that means no one can predict when they might fail.

Take, for example, an episode recently reported by machine learning researcher Rich Caruana and his colleagues. They described the experiences of a team at the University of Pittsburgh Medical Center who were using machine learning to predict whether pneumonia patients might develop severe complications. The goal was to send patients at low risk for complications to outpatient treatment, preserving hospital beds and the attention of medical staff. The team tried several different methods, including various kinds of neural networks, as well as software-generated decision trees that produced clear, human-readable rules.

The neural networks were right more often than any of the other methods. But when the researchers and doctors took a look at the human-readable rules, they noticed something disturbing: One of the rules instructed doctors to send home pneumonia patients who already had asthma, despite the fact that asthma sufferers are known to be extremely vulnerable to complications.

The model did what it was told to do: Discover a true pattern in the data. The poor advice it produced was the result of a quirk in that data. It was hospital policy to send asthma sufferers with pneumonia to intensive care, and this policy worked so well that asthma sufferers almost never developed severe complications. Without the extra care that had shaped the hospital’s patient records, outcomes could have been dramatically different.

[![Sapolsky_TH-F1](http://static.nautil.us/14208_2e945b99f24f789d68d85ee332131c93.png)](http://nautil.us/issue/57/Communities/waiting-for-the-robot-rembrandt)

[ALSO IN ARTIFICIAL INTELLIGENCE](http://nautil.us/term/f/Artificial%20Intelligence)  

#### [Waiting For the Robot Rembrandt](http://nautil.us/issue/57/Communities/waiting-for-the-robot-rembrandt)

By Hideki Nakazawa

The cellist Jan Vogler famously claimed that art is what makes us human. But what if machines start making art too? Here’s an example of a piece of art made by an artificial intelligence (AI): A bit of art: A...**READ MORE**

The hospital anecdote makes clear the practical value of interpretability. “If the rule-based system had learned that asthma lowers risk, certainly the neural nets had learned it, too,” wrote Caruana and colleagues—but the neural net wasn’t human-interpretable, and its bizarre conclusions about asthma patients might have been difficult to diagnose.1 If there hadn’t been an interpretable model, Malioutov cautions, “you could accidentally kill people.”

This is why so many are reluctant to gamble on the mysteries of neural networks. When Malioutov presented his accurate but inscrutable neural network model to his own corporate client, he also offered them an alternative, rule-based model whose workings he could communicate in simple terms. This second, interpretable, model was less accurate than the first, but the client decided to use it anyway—despite being a mathematically sophisticated insurance company for which every percentage point of accuracy mattered. “They could relate to [it] more,” Malioutov says. “They really value intuition highly.”

Even governments are starting to show concern about the increasing influence of inscrutable neural-network oracles. The European Union recently proposed to establish a “right to explanation,” which allows citizens to demand transparency for algorithmic decisions.2 The legislation may be difficult to implement, however, because the legislators didn’t specify exactly what “transparency” means. It’s unclear whether this omission stemmed from ignorance of the problem, or an appreciation of its complexity.

> Some researchers hope to eliminate the need to choose—to let us have our many-layered cake, and understand it, too.

In fact, some believe that such a definition might be impossible. At the moment, though we can know everything there is to know about what neural networks are doing—they are, after all, just computer programs—we can discern very little about how or why they are doing it. The networks are made up of many, sometimes millions, of individual units, called neurons. Each neuron converts many numerical inputs into a single numerical output, which is then passed on to one or more other neurons. As in brains, these neurons are divided into “layers,” groups of cells that take input from the layer below and send their output to the layer above.

Neural networks are trained by feeding in data, then adjusting the connections between layers until the network’s calculated output matches the known output (which usually consists of categories) as closely as possible. The incredible results of the past few years are thanks to a series of new techniques that make it possible to quickly train deep networks, with many layers between the first input and the final output. One popular deep network called AlexNet is used to categorize photographs—labeling them according to such fine distinctions as whether they contain a Shih Tzu or a Pomeranian. It consists of over 60 million “weights,” each of which tell each neuron how much attention to pay to each of its inputs. “In order to say you have some understanding of the network,” says Jason Yosinski, a computer scientist affiliated with Cornell University and Geometric Intelligence, “you’d have to have some understanding of these 60 million numbers.”

Even if it were possible to impose this kind of interpretability, it may not always be desirable. The requirement for interpretability can be seen as another set of constraints, preventing a model from a “pure” solution that pays attention only to the input and output data it is given, and potentially reducing accuracy. At a DARPA conference early this year, program manager David Gunning summarized the trade-off in a chart that shows deep networks as the least understandable of modern methods. At the other end of the spectrum are decision trees, rule-based systems that tend to prize explanation over efficacy.

![Bornstein-BR-2](http://static.nautil.us/10301_b1f130b49d0fcfa2348098ee4467452f.png)**WHAT VS. WHY:** Modern learning algorithms show a tradeoff between human interpretability, or explainability, and their accuracy. Deep learning is both the most accurate and the least interpretable.Darpa

The result is that modern machine learning offers a choice among oracles: Would we like to know *what* will happen with high accuracy, or *why* something will happen, at the expense of accuracy? The “why” helps us strategize, adapt, and know when our model is about to break. The “what” helps us act appropriately in the immediate future.

It can be a difficult choice to make. But some researchers hope to eliminate the need to choose—to allow us to have our many-layered cake, and understand it, too. Surprisingly, some of the most promising avenues of research treat neural networks as experimental objects—after the fashion of the biological science that inspired them to begin with—rather than analytical, purely mathematical objects. Yosinski, for example, says he is trying to understand deep networks “in the way we understand animals, or maybe even humans.” He and other computer scientists are importing techniques from biological research that peer inside networks after the fashion of neuroscientists peering into brains: probing individual components, cataloguing how their internals respond to small changes in inputs, and even removing pieces to see how others compensate.

Having built a new intelligence from scratch, scientists are now taking it apart, applying to these virtual organisms the digital equivalents of a microscope and scalpel.

Yosinski sits at a computer terminal, talking into a webcam. The data from the webcam is fed into a deep neural net, while the net itself is being analyzed, in real time, using a software toolkit Yosinski and his colleagues developed called the Deep Visualization toolkit. Clicking through several screens, Yosinski zooms in on one neuron in the network. “This neuron seems to respond to faces,” he says in a video record of the interaction.3 Human brains are also known to have such neurons, many of them clustered in a region of the brain called the fusiform face area. This region, discovered over the course of multiple studies beginning in 1992,4, 5 has become one of the most reliable observations in human neuroscience. But where those studies required advanced techniques like positron emission tomography, Yosinski can peer at his artificial neurons through code alone.

![Bornstein-BR-5](http://static.nautil.us/10315_059bbd8df7767d3bc7829e3735c221e2.png)**BRAIN ACTIVITY:** A single neuron in a deep neural net (highlighted by a green box) responds to Yosinski’s face, just as a distinct part of the human brain reliably responds to faces (highlighted in yellow).Left: Jason Yosinski, *et al.* Understanding Neural Networks Through Deep Visualization. Deep Learning Workshop, International Conference on Machine Learning (ICML) (2015). Right: Maximilian Riesenhuber, Georgetown University Medical Center

This approach lets him map certain artificial neurons to human-understandable ideas or objects, like faces, which could help turn neural networks into intuitive tools. His program can also highlight which aspects of a picture are most important to stimulating the face neuron. “We can see that it would respond even more strongly if we had darker eyes, and rosier lips,” he says.

To Cynthia Rudin, professor of computer science and electrical and computer engineering at Duke University, these “post-hoc” interpretations are by nature problematic. Her research focuses on building rule-based machine learning systems applied to domains like prison sentencing and medical diagnosis, where human-readable interpretations are possible—and critically important. But for problems in areas like vision, she says, “Interpretations are completely within the eye of the beholder.” We can simplify a network response by identifying a face neuron, but how can we be certain that’s really what it’s looking for? Rudin’s concerns echo the famous dictum that there may be no simpler model of the visual system than the visual system itself. “You could have many explanations for what a complex model is doing,” she says. “Do you just pick the one you ‘want’ to be correct?”

Yosinski’s toolkit can, in part, counter these concerns by working backward, discovering what the network itself “wants” to be correct—a kind of artificial ideal. The program starts with raw static, then adjusts it, pixel by pixel, tinkering with the image using the reverse of the process that trained the network. Eventually it finds a picture that elicits the maximum possible response of a given neuron. When this method is applied to AlexNet neurons, it produces caricatures that, while ghostly, unquestionably evoke the labeled categories.

![Bornstein-BR-4](http://static.nautil.us/10293_37f2fc94430a30d7dba690d94d7e1223.png)**IDEALIZED CATS:** Examples of synthetic ideal cat faces, generated by the Deep Visualization toolkit. These faces are generated by tweaking a generic starting image pixel by pixel, until a maximum response from AlexNet’s face neuron is achieved.Jason Yosinski, *et al.* Understanding Neural Networks Through Deep Visualization. Deep Learning Workshop, International Conference on Machine Learning (ICML) (2015).

This seems to support his claim that the face neurons are indeed looking for faces, in some very general sense. But there’s a catch. To generate those pictures, Yosinski’s procedure relies on a statistical constraint (called a natural image prior) that confines it to producing images that match the sorts of structure that one finds in pictures of real-world objects. When he removes those rules, the toolkit still settles on an image that it labels with maximum confidence, but that image is pure static. In fact, Yosinski has shown that in many cases, the majority of images that AlexNet neurons prefer appear to humans as static. He readily admits that “it’s pretty easy to figure out how to make the networks say something extreme.”

To avoid some of these pitfalls, Dhruv Batra, an assistant professor of electrical and computer engineering at Virginia Tech, takes a higher-level experimental approach to interpreting deep networks. Rather than trying to find patterns in their internal structure—“people smarter than me have worked on it,” he demurs—he probes how the networks behave using, essentially, a robotic version of eye tracking. His group, in a project led by graduate students Abhishek Das and Harsh Agrawal, asks a deep net questions about an image, like whether there are drapes on a window in a given picture of a room.6 Unlike AlexNet or similar systems, Das’ network is designed to focus on only a small patch of an image at a time. It moves its virtual eyes around the picture until it has decided it has enough information to answer the question. After sufficient training, the deep network performs extremely well, answering about as accurately as the best humans.

> Trained machines are exquisitely well suited to their environment—and ill-adapted to any other.

Das, Batra, and their colleagues then try to get a sense of how the network makes its decisions by investigating where in the pictures it chooses to look. What they have found surprises them: When answering the question about drapes, the network doesn’t even bother looking for a window. Instead, it first looks to the bottom of the image, and stops looking if it finds a bed. It seems that, in the dataset used to train this neural net, windows with drapes may be found in bedrooms.

While this approach does reveal some of the inner workings of the deep net, it also reinforces the challenge presented by interpretability. “What machines are picking up on are not facts about the world,” Batra says. “They’re facts about the dataset.” That the machines are so tightly tuned to the data they are fed makes it difficult to extract general rules about how they work. More importantly, he cautions, if you don’t know how it works, you don’t know how it will fail. And when they do they fail, in Batra’s experience, “they fail spectacularly disgracefully.”

Some of the obstacles faced by researchers like Yosinski and Batra will be familiar to scientists studying the human brain. Questions over the interpretations of neuroimaging, for example, are common to this day, if not commonly heeded. In a 2014 review of the field, cognitive neuroscientist Martha Farah wrote that “the worry ... is that [functional brain] images are more researcher inventions than researcher observations.”7 The appearance of these issues in very different realizations of intelligent systems suggests that they could be obstacles, not to the study of this or that kind of brain, but to the study of intelligence itself.

Is chasing interpretability a fool’s errand? In a 2015 blog post entitled “The Myth of Model Interpretability,” Zachary Lipton, of the University of California, San Diego, offered a critical perspective on both the motivations behind interpreting neural networks, and the value of building interpretable machine learning models for huge datasets in the first place. He submitted a provocative paper on this subject to a workshop (organized by Malioutov and two of his colleagues) on Human Interpretability at this year’s International Conference on Machine Learning (ICML).8

Lipton points out that many scholars disagree over the very concept of interpretability, which suggests to him either that interpretability is poorly understood—or that there are many equally valid meanings. In either case, chasing interpretability may not satisfy our desire for a straightforward, plain-English description of a neural net output. In his blog post, Lipton argued that, when it comes to enormous datasets, researchers have the option to resist the impulse to interpret and could, instead, “place faith in empirical success.” One purpose of the field, he argued, is to “build models which can learn from a greater number of features than any human could consciously account for,” and interpretability could keep such models from reaching their full potential.

But this ability is both feature and failing: If we don’t understand how network output is generated, then we can’t know what aspects of the input were necessary, or even what might be considered input at all. Case in point: In 1996, Adrian Thompson of Sussex University used software to design a circuit by applying techniques similar to those that train deep networks today. The circuit was to perform a straightforward task: discriminate between two audio tones. After thousands of iterations, shuffling and rearranging circuit components, the software found a configuration that performed the task nearly perfectly.

Thompson was surprised, however, to discover that the circuit used fewer components than any human engineer would have used—including several that were not physically connected to the rest, and yet were somehow still necessary for the circuit to work properly.

He took to dissecting the circuit. After several experiments, he learned that its success exploited subtle electromagnetic interference between adjacent components. The disconnected elements influenced the circuit by causing small fluctuations in local electrical fields. Human engineers usually guard against these interactions, because they are unpredictable. Sure enough, when Thompson copied the same circuit layout to another batch of components—or even changed the ambient temperature—it failed completely.

The circuit exhibited a hallmark feature of trained machines: They are as compact and simplified as they can be, exquisitely well suited to their environment—and ill-adapted to any other. They pick up on patterns invisible to their engineers; but can’t know which of those patterns exist nowhere else. Machine learning researchers go to great lengths to avoid this phenomenon, called “overfitting,” but as these algorithms are used in more and more dynamic situations, their brittleness will inevitably be exposed.

 

mitry Malioutov can’t say much about what he built.

As a research scientist at IBM, Malioutov spends part of his time building machine learning systems that solve difficult problems faced by IBM’s corporate clients. One such program was meant for a large insurance corporation. It was a challenging assignment, requiring a sophisticated algorithm. When it came time to describe the results to his client, though, there was a wrinkle. “We couldn’t explain the model to them because they didn’t have the training in machine learning.”

In fact, it may not have helped even if they were machine learning experts. That’s because the model was an artificial neural network, a program that takes in a given type of data—in this case, the insurance company’s customer records—and finds patterns in them. These networks have been in practical use for over half a century, but lately they’ve seen a resurgence, powering breakthroughs in everything from speech recognition and language translation to Go-playing robots and self-driving cars.

![Bornstein-BR-1](http://static.nautil.us/10291_ef1890585bae446a0668afed3012daa2.png)**HIDDEN MEANINGS:** In neural networks, data is passed from layer to layer, undergoing simple transformations at each step. Between the input and output layers are hidden layers, groups of nodes and connections that often bear no human-interpretable patterns or obvious connections to either input or output. “Deep” networks are those with many hidden layers.Michael Nielsen / [NeuralNetworksandDeepLearning.com](http://neuralnetworksanddeeplearning.com/chap1.html)

As exciting as their performance gains have been, though, there’s a troubling fact about modern neural networks: Nobody knows quite how they work. And that means no one can predict when they might fail.

Take, for example, an episode recently reported by machine learning researcher Rich Caruana and his colleagues. They described the experiences of a team at the University of Pittsburgh Medical Center who were using machine learning to predict whether pneumonia patients might develop severe complications. The goal was to send patients at low risk for complications to outpatient treatment, preserving hospital beds and the attention of medical staff. The team tried several different methods, including various kinds of neural networks, as well as software-generated decision trees that produced clear, human-readable rules.

The neural networks were right more often than any of the other methods. But when the researchers and doctors took a look at the human-readable rules, they noticed something disturbing: One of the rules instructed doctors to send home pneumonia patients who already had asthma, despite the fact that asthma sufferers are known to be extremely vulnerable to complications.

The model did what it was told to do: Discover a true pattern in the data. The poor advice it produced was the result of a quirk in that data. It was hospital policy to send asthma sufferers with pneumonia to intensive care, and this policy worked so well that asthma sufferers almost never developed severe complications. Without the extra care that had shaped the hospital’s patient records, outcomes could have been dramatically different.

[![Sapolsky_TH-F1](http://static.nautil.us/14208_2e945b99f24f789d68d85ee332131c93.png)](http://nautil.us/issue/57/Communities/waiting-for-the-robot-rembrandt)

[ALSO IN ARTIFICIAL INTELLIGENCE](http://nautil.us/term/f/Artificial%20Intelligence)  

#### [Waiting For the Robot Rembrandt](http://nautil.us/issue/57/Communities/waiting-for-the-robot-rembrandt)

By Hideki Nakazawa

The cellist Jan Vogler famously claimed that art is what makes us human. But what if machines start making art too? Here’s an example of a piece of art made by an artificial intelligence (AI): A bit of art: A...**READ MORE**

The hospital anecdote makes clear the practical value of interpretability. “If the rule-based system had learned that asthma lowers risk, certainly the neural nets had learned it, too,” wrote Caruana and colleagues—but the neural net wasn’t human-interpretable, and its bizarre conclusions about asthma patients might have been difficult to diagnose.1 If there hadn’t been an interpretable model, Malioutov cautions, “you could accidentally kill people.”

This is why so many are reluctant to gamble on the mysteries of neural networks. When Malioutov presented his accurate but inscrutable neural network model to his own corporate client, he also offered them an alternative, rule-based model whose workings he could communicate in simple terms. This second, interpretable, model was less accurate than the first, but the client decided to use it anyway—despite being a mathematically sophisticated insurance company for which every percentage point of accuracy mattered. “They could relate to [it] more,” Malioutov says. “They really value intuition highly.”

Even governments are starting to show concern about the increasing influence of inscrutable neural-network oracles. The European Union recently proposed to establish a “right to explanation,” which allows citizens to demand transparency for algorithmic decisions.2 The legislation may be difficult to implement, however, because the legislators didn’t specify exactly what “transparency” means. It’s unclear whether this omission stemmed from ignorance of the problem, or an appreciation of its complexity.

> Some researchers hope to eliminate the need to choose—to let us have our many-layered cake, and understand it, too.

In fact, some believe that such a definition might be impossible. At the moment, though we can know everything there is to know about what neural networks are doing—they are, after all, just computer programs—we can discern very little about how or why they are doing it. The networks are made up of many, sometimes millions, of individual units, called neurons. Each neuron converts many numerical inputs into a single numerical output, which is then passed on to one or more other neurons. As in brains, these neurons are divided into “layers,” groups of cells that take input from the layer below and send their output to the layer above.

Neural networks are trained by feeding in data, then adjusting the connections between layers until the network’s calculated output matches the known output (which usually consists of categories) as closely as possible. The incredible results of the past few years are thanks to a series of new techniques that make it possible to quickly train deep networks, with many layers between the first input and the final output. One popular deep network called AlexNet is used to categorize photographs—labeling them according to such fine distinctions as whether they contain a Shih Tzu or a Pomeranian. It consists of over 60 million “weights,” each of which tell each neuron how much attention to pay to each of its inputs. “In order to say you have some understanding of the network,” says Jason Yosinski, a computer scientist affiliated with Cornell University and Geometric Intelligence, “you’d have to have some understanding of these 60 million numbers.”

Even if it were possible to impose this kind of interpretability, it may not always be desirable. The requirement for interpretability can be seen as another set of constraints, preventing a model from a “pure” solution that pays attention only to the input and output data it is given, and potentially reducing accuracy. At a DARPA conference early this year, program manager David Gunning summarized the trade-off in a chart that shows deep networks as the least understandable of modern methods. At the other end of the spectrum are decision trees, rule-based systems that tend to prize explanation over efficacy.

![Bornstein-BR-2](http://static.nautil.us/10301_b1f130b49d0fcfa2348098ee4467452f.png)**WHAT VS. WHY:** Modern learning algorithms show a tradeoff between human interpretability, or explainability, and their accuracy. Deep learning is both the most accurate and the least interpretable.Darpa

The result is that modern machine learning offers a choice among oracles: Would we like to know *what* will happen with high accuracy, or *why* something will happen, at the expense of accuracy? The “why” helps us strategize, adapt, and know when our model is about to break. The “what” helps us act appropriately in the immediate future.

It can be a difficult choice to make. But some researchers hope to eliminate the need to choose—to allow us to have our many-layered cake, and understand it, too. Surprisingly, some of the most promising avenues of research treat neural networks as experimental objects—after the fashion of the biological science that inspired them to begin with—rather than analytical, purely mathematical objects. Yosinski, for example, says he is trying to understand deep networks “in the way we understand animals, or maybe even humans.” He and other computer scientists are importing techniques from biological research that peer inside networks after the fashion of neuroscientists peering into brains: probing individual components, cataloguing how their internals respond to small changes in inputs, and even removing pieces to see how others compensate.

Having built a new intelligence from scratch, scientists are now taking it apart, applying to these virtual organisms the digital equivalents of a microscope and scalpel.

Yosinski sits at a computer terminal, talking into a webcam. The data from the webcam is fed into a deep neural net, while the net itself is being analyzed, in real time, using a software toolkit Yosinski and his colleagues developed called the Deep Visualization toolkit. Clicking through several screens, Yosinski zooms in on one neuron in the network. “This neuron seems to respond to faces,” he says in a video record of the interaction.3 Human brains are also known to have such neurons, many of them clustered in a region of the brain called the fusiform face area. This region, discovered over the course of multiple studies beginning in 1992,4, 5 has become one of the most reliable observations in human neuroscience. But where those studies required advanced techniques like positron emission tomography, Yosinski can peer at his artificial neurons through code alone.

![Bornstein-BR-5](http://static.nautil.us/10315_059bbd8df7767d3bc7829e3735c221e2.png)**BRAIN ACTIVITY:** A single neuron in a deep neural net (highlighted by a green box) responds to Yosinski’s face, just as a distinct part of the human brain reliably responds to faces (highlighted in yellow).Left: Jason Yosinski, *et al.* Understanding Neural Networks Through Deep Visualization. Deep Learning Workshop, International Conference on Machine Learning (ICML) (2015). Right: Maximilian Riesenhuber, Georgetown University Medical Center

This approach lets him map certain artificial neurons to human-understandable ideas or objects, like faces, which could help turn neural networks into intuitive tools. His program can also highlight which aspects of a picture are most important to stimulating the face neuron. “We can see that it would respond even more strongly if we had darker eyes, and rosier lips,” he says.

To Cynthia Rudin, professor of computer science and electrical and computer engineering at Duke University, these “post-hoc” interpretations are by nature problematic. Her research focuses on building rule-based machine learning systems applied to domains like prison sentencing and medical diagnosis, where human-readable interpretations are possible—and critically important. But for problems in areas like vision, she says, “Interpretations are completely within the eye of the beholder.” We can simplify a network response by identifying a face neuron, but how can we be certain that’s really what it’s looking for? Rudin’s concerns echo the famous dictum that there may be no simpler model of the visual system than the visual system itself. “You could have many explanations for what a complex model is doing,” she says. “Do you just pick the one you ‘want’ to be correct?”

Yosinski’s toolkit can, in part, counter these concerns by working backward, discovering what the network itself “wants” to be correct—a kind of artificial ideal. The program starts with raw static, then adjusts it, pixel by pixel, tinkering with the image using the reverse of the process that trained the network. Eventually it finds a picture that elicits the maximum possible response of a given neuron. When this method is applied to AlexNet neurons, it produces caricatures that, while ghostly, unquestionably evoke the labeled categories.

![Bornstein-BR-4](http://static.nautil.us/10293_37f2fc94430a30d7dba690d94d7e1223.png)**IDEALIZED CATS:** Examples of synthetic ideal cat faces, generated by the Deep Visualization toolkit. These faces are generated by tweaking a generic starting image pixel by pixel, until a maximum response from AlexNet’s face neuron is achieved.Jason Yosinski, *et al.* Understanding Neural Networks Through Deep Visualization. Deep Learning Workshop, International Conference on Machine Learning (ICML) (2015).

This seems to support his claim that the face neurons are indeed looking for faces, in some very general sense. But there’s a catch. To generate those pictures, Yosinski’s procedure relies on a statistical constraint (called a natural image prior) that confines it to producing images that match the sorts of structure that one finds in pictures of real-world objects. When he removes those rules, the toolkit still settles on an image that it labels with maximum confidence, but that image is pure static. In fact, Yosinski has shown that in many cases, the majority of images that AlexNet neurons prefer appear to humans as static. He readily admits that “it’s pretty easy to figure out how to make the networks say something extreme.”

To avoid some of these pitfalls, Dhruv Batra, an assistant professor of electrical and computer engineering at Virginia Tech, takes a higher-level experimental approach to interpreting deep networks. Rather than trying to find patterns in their internal structure—“people smarter than me have worked on it,” he demurs—he probes how the networks behave using, essentially, a robotic version of eye tracking. His group, in a project led by graduate students Abhishek Das and Harsh Agrawal, asks a deep net questions about an image, like whether there are drapes on a window in a given picture of a room.6 Unlike AlexNet or similar systems, Das’ network is designed to focus on only a small patch of an image at a time. It moves its virtual eyes around the picture until it has decided it has enough information to answer the question. After sufficient training, the deep network performs extremely well, answering about as accurately as the best humans.

> Trained machines are exquisitely well suited to their environment—and ill-adapted to any other.

Das, Batra, and their colleagues then try to get a sense of how the network makes its decisions by investigating where in the pictures it chooses to look. What they have found surprises them: When answering the question about drapes, the network doesn’t even bother looking for a window. Instead, it first looks to the bottom of the image, and stops looking if it finds a bed. It seems that, in the dataset used to train this neural net, windows with drapes may be found in bedrooms.

While this approach does reveal some of the inner workings of the deep net, it also reinforces the challenge presented by interpretability. “What machines are picking up on are not facts about the world,” Batra says. “They’re facts about the dataset.” That the machines are so tightly tuned to the data they are fed makes it difficult to extract general rules about how they work. More importantly, he cautions, if you don’t know how it works, you don’t know how it will fail. And when they do they fail, in Batra’s experience, “they fail spectacularly disgracefully.”

Some of the obstacles faced by researchers like Yosinski and Batra will be familiar to scientists studying the human brain. Questions over the interpretations of neuroimaging, for example, are common to this day, if not commonly heeded. In a 2014 review of the field, cognitive neuroscientist Martha Farah wrote that “the worry ... is that [functional brain] images are more researcher inventions than researcher observations.”7 The appearance of these issues in very different realizations of intelligent systems suggests that they could be obstacles, not to the study of this or that kind of brain, but to the study of intelligence itself.

Is chasing interpretability a fool’s errand? In a 2015 blog post entitled “The Myth of Model Interpretability,” Zachary Lipton, of the University of California, San Diego, offered a critical perspective on both the motivations behind interpreting neural networks, and the value of building interpretable machine learning models for huge datasets in the first place. He submitted a provocative paper on this subject to a workshop (organized by Malioutov and two of his colleagues) on Human Interpretability at this year’s International Conference on Machine Learning (ICML).8

Lipton points out that many scholars disagree over the very concept of interpretability, which suggests to him either that interpretability is poorly understood—or that there are many equally valid meanings. In either case, chasing interpretability may not satisfy our desire for a straightforward, plain-English description of a neural net output. In his blog post, Lipton argued that, when it comes to enormous datasets, researchers have the option to resist the impulse to interpret and could, instead, “place faith in empirical success.” One purpose of the field, he argued, is to “build models which can learn from a greater number of features than any human could consciously account for,” and interpretability could keep such models from reaching their full potential.

But this ability is both feature and failing: If we don’t understand how network output is generated, then we can’t know what aspects of the input were necessary, or even what might be considered input at all. Case in point: In 1996, Adrian Thompson of Sussex University used software to design a circuit by applying techniques similar to those that train deep networks today. The circuit was to perform a straightforward task: discriminate between two audio tones. After thousands of iterations, shuffling and rearranging circuit components, the software found a configuration that performed the task nearly perfectly.

Thompson was surprised, however, to discover that the circuit used fewer components than any human engineer would have used—including several that were not physically connected to the rest, and yet were somehow still necessary for the circuit to work properly.

He took to dissecting the circuit. After several experiments, he learned that its success exploited subtle electromagnetic interference between adjacent components. The disconnected elements influenced the circuit by causing small fluctuations in local electrical fields. Human engineers usually guard against these interactions, because they are unpredictable. Sure enough, when Thompson copied the same circuit layout to another batch of components—or even changed the ambient temperature—it failed completely.

The circuit exhibited a hallmark feature of trained machines: They are as compact and simplified as they can be, exquisitely well suited to their environment—and ill-adapted to any other. They pick up on patterns invisible to their engineers; but can’t know which of those patterns exist nowhere else. Machine learning researchers go to great lengths to avoid this phenomenon, called “overfitting,” but as these algorithms are used in more and more dynamic situations, their brittleness will inevitably be exposed.