# 7.1 神经网络
## MLP

今天的课程关于神经网络．　神经网络是试图模仿大脑的功能的一系列的算法．
As we have seen before, our brain is a network of neurons,
and each neuron is connected to between tens
and tens of thousands of other neurons that
communicate between them.
So some information is being transferred from one node
to another one.
This family of algorithms works extremely well
on several applications.
And this includes handwritten character cognitions,
such as digits on zip code on an envelope, spoken words,
faces, and so on and so forth, just to cite a few.
It has been a topic of research that was extensively
studied in the '80s and '90s but had some moderate success
at the time.
Now it's back with lots of successes
in using deep learning, thanks to algorithmic and computational
progress that we have been witnessing since the '90s.
As you recall, the first algorithm, or the simplest
neural network, is called the perceptron,
invented by Rosenblatt in 1959.
And we have seen this in the previous lecture.
As you recall, the perceptron is a gross simplification
of the function of real neuron.
So the idea is that, given some inputs,
the neuron should produce some output.
The input is taken from all the neurons,
and the output is dispatched to the other neurons.
So similarly, a perceptron would take values that are actually
the feature values in a table, that
are a table of the training examples, for example.
So suppose it says one of the examples,
it says the example I.
This example, I, we have feature values xi1,
xi2, xid, assuming there are d dimensions.
We are going to introduce xi0, which is actually the bias.
Remember, when we do the weighted sum, where
we're going to do w0 plus w1 x2 plus w2 x2, et cetera.
So this first value here has an x of 1.
So I'm going to put an xi0 of 1.
Then, these values are put into a weighted sum of the values.
That would be the sum of over all the dimensions of wjxij,
so all the dimensions j.
For the feature, i, we are going to do this weighted sum
of the values.
So this weight has to be learned.
We don't know them a priori.
This weighted sum is then plugged into some squashing
function that actually will tell us
whether the output, or the label of the example, i,
is either plus 1 or minus 1.
So we're going to use here, in the kinds
of squashing functions we could use, one of them
is called the step function, shows us this
one step function.
Or threshold function, it actually takes the weighted sum
and then compares it to 0.
If the weighted sum is bigger than 0,
then it's declared that the function is 1 and minus 1
otherwise. So which means we are going to try to find
The learning here is to try to find the ways that we actually
match the input to the output.
So going to learn.
So as you balance, you have for the example
of the input, the right output.
So the weight has to be learned by training this perceptron.
So the function, again, is a function
that takes the sign of the weighted sum of the feature
values by the vector w, which is actually the weights of the--
that are the arrows between the neurons as input,
and the neuron that tells the sum of the feature values.
One point that emerged from our lecture on perceptron
is how expressive are perceptrons.
So if we go to the, for example, the perceptrons with the step
function, just like with the example we saw now,
the idea of finding the perceptron
is to adopt an iterative method that
will start with a random hyperplane
and tweak it, tweak it using the training data,
until we find the separating hyperplane.
So this is good enough to represent lots of functions.
And this includes Boolean functions like the AND, the OR.
Now, here there's a typo.
It should be the NOT OR, the NOT, NOT AND, et cetera.
But it is not able to actually separate the data that
looks like an XOR.
It just produces a linear separator of the input space.
So let's qualify this point.
So suppose we have two feature values, x1,
x2, that could take either 0, 0; 0, 1; 1, 0; and 1, 1.
We're going to express x1 and x2 as evaluated equal to 1 only
when both values are equal to 1, 0 otherwise.
x1 or x2 would be equal to 1
everywhere, except when they're both equal to 0.
And x1, let's do the XOR, which would be actually equal to 1
only when they are differing by their difference,
so the rest would be equal to 0.
So if you want to find the hyperplane that separates
the data for these four points, so we will have to draw x1,
x2 on this plane.
And then, we could represent the points that are 0 as for 0, 0,
it's 0.
For 0, 1, it's also 0 and 0 here.
And we'd have another positive point right here.
So as we said, it's easy to find this separating hyperplane that
actually separate the data into two.
Same thing will happen with the x, with the
OR, in which we'd have data that looks like this, x1, x2.
In this case, everything will be actually 1, except for 0, 0.
This point here would be a negative point, or 0.
And the hyperplane in this case would
be this line that separates the positive side
from the negative side.
So the positive side would be on this side.
And finally, in the XOR example,
we're going to see that it's not possible to separate
this with the hyperplane.
So we'll have x1 equal to plus--
equal 1 or.
In both cases where we have one of them
is equal to one of the features equal to 1,
the 0's would be on both sides.
And it's not possible to find one line that
separates this data.
How much we can stay separated with two lines
if we draw, for example, a line here and the line here?
All this area in the middle will be plus.
This area will be-- so the plus will be down.
And everything in here will be negative,
and everything in here will be negative.
So we need, in this case, two hyperplanes
instead of one hyperplane.
So the perceptron can be extended
to what we call a multilayer perceptron, in which
the multilayer perceptron will explore
the ability of the perceptrons to represent simple function.
But then you're going to combine them together
to make a more complicated network of different layers
of those elementary questions.
So however, if we take a cascade of linear function,
the result is still linear.
And you want to be able to actually represent
high non-linear functions.
For example, if we have data that looks like this.
So remember with points like here as negative,
and the points around as positive and the points
around as negative, it's very hard to find really several hyperplanes
and that would separate this data
the positive from the negative points.
So we need to find something that
would allow us to really represent those highly
non-linear functions.
Also, the perceptron is using a threshold function,
which is undifferentiable and not
suitable for gradient descent, in case data is not
linearly separable.
We want a function whose output is
a linear function of the input.
One possibility is to use what we call the sigmoid function,
that is g of z, ez divided by 1 plus ez,
or equivalently 1 divided by 1 plus e minus c.
This function has this shape here,
in which we have actually-- the interesting property is also
called the S function, the S function.
It's shaped like an S. And this function
has the interesting property that it
will be converging to 1 whenever z goes to plus infinity,
and converging to 0 whenever z goes to minus infinity.
So this value will continue toward--
when z goes to minus infinity it's
0, when 0 goes to plus infinity, it converges to 1.
So I'm going to use this function
to show that we can represent actually the XOR function.
So we're giving a new look to our perceptron, in which case
we still have our weighted sum of the values.
But in this case, we're going to use this, what you call
the activation function is no longer the step function.
In this case, it's the sigmoid function.
And this will be g of what of the weighted sum of the values.
So we're not doing this step function anymore.
And f of xi, in this case, will be equal to 1 divided by 1 plus
e minus z.
Our z is xi, which is the weighted sum of the values.
So our f of xi would be simply 1 divided by 1, plus e
of minus the weighted sum of the feature values.
So keep everything the same, but our activation function here
is a new squashing function, which is the sigmoid function.

## XOR
Now let's get back to our example of the XOR
in which we are going to try to create a multiple layer
perception, which are otherwise neural networks, that actually
produces the XOR function using elementary perceptrons.
And these elementary perceptions will be the AN, the OR, the NOT
AND, the NOT, et cetera.
We'll see what we need.
So the next step would be to--
first of all, let's observe that for the function g,
the function g is 1 divided by e minus z.
And we observe that g of 10 is close to 1, and g of minus 10
is close to 1.
So we need to have these two values
you have to choose as whenever the function is actually
reaching 1 and when it is reaching 0.
So let's consider that, actually,
for any z bigger than 1, g of z will tend to 1, and for any z
less than or equal than minus 1, g of z will have a limit of 0.
So first of all, what is the perceptron of the OR?
So remember, we just did the example of the OR,
and we know that the step function allows us to classify
the examples in the OR table.
We can also do it with the new squashing function
g, which is our sigmoid.
So let's see how we can define the weights using
the sigmoid function.
So we have a similar table that's
just like we have seen-- x1, x2.
We have the OR between the two values.
So we're going to build a perceptron in which g
is actually the sigmoid function of the weight at some
of the feature values for all dimension d.
So we want to be able to produce the OR, which
means whenever we introduce 0, 0, we get, as outcome, 0.
If we introduce 0, 1, we get as output 1.
And this will help us train the algorithm to find the weights.
So here the weights are straightforward that we
can do, given that we know that we are going to have,
for any value less than minus 10,
we're going to have g converge to 0.
For any value 10 or more, we're going
to have g converge to plus 2 plus 1.
So what we can do is to actually replace this g of z
as the g of the weighted sum.
So we have w0 plus w1, x1, w2, x2. Here,
both of these two values are 0, which mean that g of w0
should be equal to 0, which would be represented here
by g of minus 10.
We do the same thing in the second case.
And we're going to have--
the value 1 would be g of 10.
And we're going to figure out ways that are actually
giving us this 0, 1, 1, 1 here.
So in other words, if I take this second case,
it will be g of 1, g of w0 plus w1, x1 will be 0, plus w2, x2.
And w2, x2 is 1.
So there is no need to keep this value here.
So plus w2.
g of w0 plus w2 should be equal to 1.
So for this to be true, we have to have w0 plus w1 equal to 10.
And w0, we know it's minus 10 from the previous line, which
means that we have minus 10 plus w1 equals 10, which
will lead us to w1.
Possibility will be w1 equals 20.
We do the same thing at this level and at this level,
and we get the weights for the connections
between the neuron 1, the bias, neuron x1, which
is the first feature value, neuron x2, which
is the second feature value.
So in this case, what we're going to do is to--
this function here would be equal to what?
It's a g of minus 10 plus 20 x1 plus 20 x2,
which would be the sigmoid function, which was what--
1 divided by 1 plus e to the minus of minus 10
plus 20 x1 plus 20 x2.
So if we try to, we'll see that, actually, this
would classify correctly all of the examples
by using the sigmoid functions of the weighted input of the--
using this weight that we just learned.
So we're going to do the same thing to learn other functions.
Similarly, we could get the perceptions of the AND and NOT
AND.
And this perception would look like this.
So the NOT AND has the weights minus 30, 20, 20.
But there are other possibilities, of course.
We just pick that we are going to consider g of minus 10
is equal to 0, and g of 10 equal to 1.
So note that the weights in the NOT AND
are just the inverse of the weights in the AND.
So next, we're going to write our XOR functions
using elementary functions.
So x1 XOR x2 could be written as what?
As x1 OR x2 AND x1 NOT AND of x2.
So if you compare these two functions here,
you will see that they have exactly the same output,
and they are equivalent.
So a proof by truth table will show that, actually, these two
functions are equivalent, which means
that building the perceptron of XOR
is actually building the combinations of the perceptrons
of the OR, the AND, and NOT AND.
So here we are combining with an AND
the perceptron of the OR and the NOT AND.
If we put them together, we're going
to get this XOR combination of the three basic perceptrons--
the AND, the OR and the NOT AND corresponding
to the previous table.
So if you want to write the answer for a given input,
the answer would be something that
actually is a g, is a sigmoid function of what--
of minus 30 times this bias times 1
plus 20, the weight of this connection here, 20 of what--
of actually a g of something, then plus 20 of what--
of a g of the AND.
So this would represent, actually, the OR,
and this would represent the NAND.
So actually, what is the OR?
The OR is simply--
the result of this function here would be of g of what?
g of minus 10 times 1 plus 20 x1 plus 20 x2.
And the output for the function of NAND
will be g of what-- of 30 minus 20 x1 minus 20 x2.
So this is how we would get the answer
for any input, which would be the training example given
here.
We would use these weights to calculate the final outcome
g, which is what you call here the output layer.
And in this case, we are going to call this layer here
the input layer.
This is our examples.
In other words, features-- and in this case,
it's features on the left side--
and the output would be the prediction or the label.
So we use this function to make the prediction,
and this works for the XOR function.
This is how we solve that for the XOR.
So now, how do we learn the weights?
We have seen that for the XOR, with some tricks
about the limits of the sigmoid function,
it's possible to find the weights,
recover good weights for the learning algorithm.
So I need to clarify first the difference between what
we call the feedforward no network as opposed
to recurrent network.

A recurrent network has connections or loops.
However, our feedforward no network that we talk about,
the one with the different layers,
is called the feedforward neural network.
It doesn't have a loop.
We use what we call back propagation--
that stands for backwards propagations of errors--
to learn the weights.
So we're going to learn the weights
for a multilayer network.
Given a network architecture with a fixed architecture,
which means that we have the units of the neurons,
and we have the interconnections,
we're going to use, again, the gradient descent to minimize
the squared error between the network output
on the right side, which is the output layer,
and the input layer, which is the values of the features.
So we suppose that we have multiple output k.
It could be multi-classification.
And the challenge is, then, to search for all possible weight
values for all the units in the network
to find the best weighting of this architecture that
would lead us to minimize this squared error of the training
data.

## 反向传播算法
So far, for the XOR function, we have learned
the rate in ad hoc fashion.
So we were able to, just by tweaking
the value of the sigmoid function,
we could find the rates on the connections
in the neural network.
So you can imagine that, for larger networks with more
complicated functions, it's not possible to tweak the rates
this way with the method.
And one of the methods to do it is
called "backpropagation algorithm"
that actually allows us to learn the rate of the network using
gradient descent.
So let me first mention that we are talking here
about what we call "feedforward neural networks"
as opposed to recurrent networks since we
have no connections that actually loop.
So we're going to go with the networks that actually
are simple, going from left to right
with the inputs on the left and the outputs on the right.
So the learning task here is to learn the rates
for a multilayer network and to use the backpropagation
method that stands actually for "backward
propagation of errors."
So we see in a bit what this means.
So given a network with a fixed architecture--
so we know what other neurons in the connections--
use gradient descent to minimize the squared error
between the network output value, which
we call "o" and the ground truth y,
which means the labels of the example.
So we suppose that we have multiple output k.
This is to make it general.
And the challenge is actually to search for all possible rate
values for all neurons in the network,
and this actually means that we're
going to sift through possible combinations of weights that
actually lead to a good learning of the examples and their class
labels.
To clarify these concepts or feedforward and backward
propagation, let me give you an example
of a fully connected neural network
with one hidden layer, one input layer, and one output layer.
So observe here that this is a fully connected network
in which we have any neuron is connected to all
the neurons in the next layer.
So there are also rates on the different connections.
So each connection will be weighted by the wij between the neuron i
and neuron j, also called "units."
So giving an example, let's call it
e, a pair x and y, where x is-- remember,
this is the future vector.
And y is the label.
What we are going to do is to feed the network your x.
Right?
So we're going to put the x, which is the feature values,
into the neurons as input to the network.
OK.
So these values would be moved forward
to the right from connection to connection using the weights.
So each neuron new member has some activation function
that actually is this weighted sum of the neurons at input
that is cast within some function, sigmoid or other.
And the information at each one is then calculated by taking
the inputs of this neuron rated by the w's .
And then you could push the information forward.
until you get an answer.
Let's call it "o."
So for this example, we're going to get an oe.
This is the predicted outcome or the function
that actually this network is computing for the example.
Observe here that actually this is our f of x actually--
right-- for the example e.
So observe here that actually we can easily
compare the outcome that we are getting to the true label.
The true label is not used for training,
so we're just comparing.
Given the current state of the weights,
so we are going to compare what the network is telling us
as the predicted value for the label
as compared to the true label y.
So we're going to compare these two.
And if there is an error, how we happened to fix the weights
is to do what we called "backpropagations."
I'm going to move back this information
about the network making a mistake backward
in the network.
So as I go back and fix the weights,
until I get the right answer I'm looking for.
OK.
So the backpropagation means that, given the true label
and given the output label, we're
going to compare them and go back, if necessary,
to fix the rates in the network.
So we're going to backpropagate the information
and start over until there is convergence
and the network is classifying this example properly.
Let's formalize the concept of backpropagation
by a set of backpropagation rules.
So first of all, in the general case,
we are going to consider k output which
means that, on the output layer, we have k possible neurons
or units.
This comes in handy if we have a middle class classification,
let's say the digits between 0 and 9.
For an example, e defined by the pair x, y--
x being the future values and y being the label--
the error on training the example e's
is the sum over all the output neurons in the network.
In other words, we're going to consider
the error of the example e given a configuration of rates
is the sum of all the mistakes that the network is making
as compared to the true label squared and we
add half to make calculations easier later on.
So you recognize is this last function.
Remember, when we use the gradient descent,
we iterate through all of the training examples one at a time
by just descending the gradients through actually of the errors
as we update the weights.
So in this case, the data of wij--
so wij is the weight between the neuron i and the neuron j--
would be minus alpha where alpha is actually
some learning rate just as we did in the gradient descent
for linear regression with a derivative of the error
that we have on each example, with respect
to the partial derivative of e with respect to wij, which
is the weight between i and j.
So we're going to descend the gradient another time.
But this time, we're going to consider
the error for each example and try
to find better ways by decreasing
those weights by delta wij.
So let's add up the following notations.
We're going to call xij the ith input to neuron j, which
means that this is what's coming out from the ith neuron
going to the jth neuron and using the weights in between.
So the weight wij would be the weight associated with the ith
input to neuron j.
We're going to call zj actually the rated sum of the xj, wij,
for all i.
OK.
So this represents actually the rated sum
of the outputs for neuron j where
oj represents the output computed by neuron j.
It could be at the end.
It could be in the middle, in the hidden layers.
g is our sigmoid function we have defined earlier to used
to squash the input, the weighted sum--
the outputs are the set of neurons in the output layer.
And we are going to call 6 successors of j or Succ
of j, the set of neurons whose immediate inputs include
the output of neuron j.
To clarify these notations, let's just
put them on the private network that
is fully connected with one hidden layer in the middle.
Suppose our jth units or jth neuron
is this neuron here that actually
gets its input from this neuron here-- this one, this one,
and this one.
So any of these neurons is actually possibly
an i neuron given its output to the neuron j.
OK.
And remember, the connections are
wij which means that what's coming out of i
is being weighted by wij and contributes into the weighted sum
wij xj for all i.
So we're going to take the output of each
of the neurons going into the j rated
by wij to get the function that we call "z of j."
OK.
This is g of z.
So remember, we are going to use a squashing function
or a threshold function or any other kind of functions
to squash just between--
make a smooth function to squash
these values between 0 and 1 and minus 1 and plus 1.
So we're going to apply a g of zi.
And remember, g of zi is the sigmoid function.
OK.
So this will actually constitute what we call
the "output of the neuron j."
So at this level, we are talking here about the hidden layer,
but j would be actually this neuron here.
And in this case, we are going to consider
these two neurons going into j.
And the principle is the same.
We're going to get an o of j, but you're going to wait.
z of j would be the weighted sum of these three neurons
that we put into the logistic function
and we get o of j, which is g of z of j input
the weighted sum of all the inputs.
The only difference between neurons
in this layer and the output neurons
is that there is no more neurons after that.
However, for this one, each one of these neurons
has a set of successors.
So for example, neuron j has only one successor,
which is also the successor of this neuron and this neuron.
If we have, for example, multiple neurons
in the output layer, then we would have--
the successors of j would be these two neurons here.
This would be constituting the successor of j.

## 反向传播规则
Recall we want to descend the gradient of the error
by delta wij equal minus alpha derivative
of the error for the example e with respect to wij.
So we're going to need to calculate this term.
And this term could be written as a partial derivative of e
with respect to zj, which is the weighted sum
of the wxij times the derivative of zj with respect to wij.
So we know that zj is the weighted sum of wij xij for all i's.
So that's going to be the derivative for a specific hwij
would be xij.
So it would replace this value by this value.
So we are left with calculating now this term
here, which is the partial derivative of e for the example
e with respect to zj.
So we're going to be able to write delta wij now
as minus alpha times derivative of e
with respect to zj times xij.
Because of the two cases-- because you
have seen that the j could be either in the hidden layer
or in the output layer.
So in calculating the derivative of e with respect to zj
would depend actually on whether the neuron is
in the middle or the end.
So case 1, neuron j is the output neuron.
And case 2, neuron j is the hidden neuron.
So let's see how we calculate this derivative in the two
cases.
In the first case, in which we have
neuron j is an output neuron, we're
going to write the partial derivative
of the error with respect to zj as the partial derivative of e
with respect to oj, which is the outcome of the neuron
j times the derivative of oj with respect to zj.
So we have now two terms.
Let's calculate each of them separately.
So first of all, the derivative of e with respect to oj.
We start by writing what's the e.
So remember the error of training example e
is summed over all the output neurons in the network,
which means that we're going to take the k output--
see the difference between yk and ok, which
is what actually we are supposed to find minus
what the network is giving us.
So we're going square this to avoid handling the negation.
So this is a derivative of this function
here, with respect to oj.
So this will lead us to what?
To the partial derivative of this function
with respect to oj, in which we're going just
to focus on the j-th neurons because this sum is over all
the k output neurons.
So the partial derivative of this value
here is 2 times this value times the derivative of this value.
So it's going to be 1/2 times 2 times
y minus oj times the partial derivative of this value
here with respect to oj.
Which finally will lead us to the formula minus yj minus oj.
So this first term here is simply this value here.
We're now going to do a similar calculations
for the second term.
Partial derivative of oj with respect to zj.
And this is actually in this case oj is g of zj.
This is the sigmoid function of the weighted sum of the inputs.
So remember we pick this function sigmoid
because we could calculate the partial derivative
of this function.
So the partial derivative of the sigmoid function
for some variable z-- with respect to z-- is g of z minus 1
minus g of z.
So this is the partial derivative
that we could deploy here.
In this case we know that g of z is actually our output, oj.
So I could just replace that by oj.
In which case we would have this term here
being equal to oj times 1 minus oj.
So we have these two values here that we could put and replace
in the formula to calculate what's
the derivative of the error with respect to zj.
So we're going to plug these two terms back into derivative of e
with respect to zj, which would make
this combination here of the difference between yj oj or j
times 1 or minus oj.
We calculate the delta wij, the difference
we need to make to descend the gradient as the alpha times
minus alpha times these terms.
Which will lead us to alpha times this term times xij.
So recall that we were aiming to find this term, because delta
wij is minus alpha times the derivative of e
with respect to zj times xij from our previous slide.
So we're just replacing actually this term here.
Plug it in here to get this final formula that allows
us to update the weights.
This is the update of the weights when
the neuron is in the output.
So we're going to note in the following delta j, which
is the error for the neuron j as minus the derivative of e
with respect to zj.
So in this case we're going to write
it wij in a simpler way as alpha times delta j times xij.
This was for the first case where the neuron j is actually
the output neuron.
Now in the case where the neuron j
is the neuron in the hidden layer, which means that we
have our hidden layer neurons.
One of them is j.
And this neuron is actually fully
connected to the output layer in which we have k output neurons.
We're talking between the connection of j
to one of the neurons k.
These are called successors of j.
Any one of them is one of the successors.
Remember we want to be able to propagate back this error.
So we're going to calculate the error at the level j
as the sum of the errors for the successors of j.
So the special derivative of the error with respect to zj
can be written as the sum of the partial derivative
of the error with respect to the zj for all successors of zj.
This can be broken down into partial derivative of e
with respect to zk times a partial derivative of zk
with respect to zj.
Remember this term here, we actually called it-- this term,
we called it delta k.
And this, actually we know how to calculate it.
We are left with the second term here,
delta zk with respect to zj.
So this can be written as, again, as delta zk with respect
to oj times the partial derivative of oj
with respect to zj.
So we are going to break down the information
as we could take into considerations that actually
for a neuron k, we're going to have
outputs oj from this input.
And the z of k for this value here--
z of k is actually the weighted sum of all the oj
for the j coming into k.
So the partial derivative of zk with respect to oj
is simply wjk, which is actually the weight of this connection
here, this one.
So going to have the z of k would be actually
the weighted sum of wjk times oj for all the inputs.
And if we derive with respect to oj,
we get the weight wjk times the partial directive
of oj with respect to zj.
I told you that.
Lots of indices, lots of notations.
But it's actually simple derivations
at different levels.
So we are going replace, then, this value here.
Remember this value was calculated before.
The partial derivative of oj with respect to zj,
we know that now it is oj times 1 minus oj.
So we can now put everything together.
We're going to call delta j, which
is the error for the neuron j as minus the partial derivative
of e with respect to zj, which is oj times 1 minus oj
because this has nothing to do with the sum of the successor
of a.
So this can come out of the summation.
Sum of delta k wjk.
And this will actually constitute our delta j
for a neuron j in the hidden layer, one
of the neurons in the hidden layer.

## 反向传播实例

So now, let's write together the algorithm,
the backpropagation algorithm.
Our input is a set of training examples just
like in the other methodology.
So we have a pair--
x, y.
Each of it is an example.
We have some learning rate.
This is the learning rate with which we descend the gradient.
For example, we could take alpha equal to 0,1.
And we have actually a number of neurons in the input,
a number of neurons in the hidden layer,
and a number of neurons in the output.
So based on this input, what we would like to achieve
is to actually output a neural network with one
input layer, one hidden layer, and one output layer.
The input, hidden, and output layer
would have a number of neurons--
ni, nh, and no respectively.
And this learning algorithm will actually also give us
the waves that are between every neuron with all
its connections.
So because they're here on the neurons with one hidden
layer, super connected, all connected
neurons where each neuron is connected
to all the rest of the neurons in the next layer.
OK.
So this is our input and our output.
So first of all, let's start by building
this architecture in which we have ni,
nh, no neurons in the layers from input, hidden, and output
layer.
Second, we're going to start with some random numbers
for the weights where all the weights for all the networks
were connections in the network will
be set to some random number.
For example, 0.2 or 0.5, which is some initial small value
for the weight.
So we're going to then feed the network with the examples x,y
and alter these weights until actually we end up with some
stable values for those weights.
So we're going to search the possibility of combinations
of all the weights in the connections in the network.
So we're going to repeat until convergence,
which means that maybe the weight would stop changing.
So for each training example x,y where x are the feature values
and y is the label, we're going to first feed forward which
means that we're going to take the example x and feed it
through the network and compute the output oj for every neuron.
So there's a typo here.
Sorry about that.
So we're going to basically feed the x throughout the network.
And remember, we have weights already
that are set up in the network.
We're going to produce an output for each single neuron
in the hidden layer and each single layer neurons from the k
neurons in the output layer.
OK.
So we are going to know right away
whether we are getting an error as a final output
from the output layer.
Then we're going to propagate the errors backwards, which
means we are going to go back and change all the weights based
on the on the output of the neural net
with respect to this input.
So we have seen two cases.
In the first case, for each of the output neurons k,
we are going to calculate its error.
The error is delta k, which is obtained
by the formula we just derived.
In the second case, we are going to calculate
for each of the neurons in the hidden layer.
Its error based on the errors produced in the output
layers of the other successors of the neuron
h, which are the k output neurons in the output layer.
So this error for the hidden layer
are actually based on the error of each of the output
neurons in the network.
So we updated these two arrows for the output,
for all the neurons in the hidden layer,
and we can calculate the update that we
need to do on the wij to actually descend a gradient.
So wij would receive the old value of wij
plus the learning rate times the error on the neuron j times
the xij, which is the connection between i and j.
OK.
So this is how backpropagation algorithm works.
This is one possibility of doing backpropagation
when you have a fixed architecture of one input
to one hidden and one output layer in which we could use any
of the activation functions.
So here, we are specifically using the sigmoid function.
OK.
So a couple observations here.
First of all, convergence means that the weights are not
changing much anymore.
So you see that, in the algorithm,
we are doing repetition of this task here.
For each training example, we're going
to repeat into convergence which means that we're
going to repeat the process over and over again
until we have at descent matching between x and y, which
means that we are going to stop whenever the weights start
to change by very, very small values.
So I wanted to mention that.
We spoke about the sigmoid function,
but there are other possible activation functions.
For example, the hyperbolic tangent function
is practically known to be better
than the sigmoid function for neural network training.
Its actually output ranges between minus and 1.
So here, we have the functions.
So recall that for k = 1 here for the red curve
represents the sigmoid function that goes between 0 and 1.
The sigmoid function can be changed a little bit
by adding a factor of k If we want to do sigmoid of kx.
So we'd be actually compacting the function a little bit,
so that would be the green function here.
So all of them would be squashing
functions between 0 and 1.
We could also use, as I said, a hyperbolic tangent
of x, which is close to g of x.
It's simply a rescaling of the logistic function.
But in this case, it goes between minus 1 and plus 1.
OK.
So we see the same s-shaped function.
But in this case, it could be interesting to have
the output ranging between minus 1 and plus 1.
So all of these functions actually
are just smoothing the outcome as compared to the--
remember, we used to use this threshold function,
and you want to be able to stretch this output
and have a smooth and squashed but smooth outcome that
could be used even as a probability
as we have seen in the logistic regression lecture.
So in practice, try to use hyperbolic tangent function
because it leads to a better training
of the neural networks.
So I wanted to recall that, when you have multiple classes,
you will have k output neurons.
So for example, in the case of 110 digits recollection,
we're going to feed the network the image of the digit written
by humans.
So suppose you have an 8, so this 8 will
be fed to the network as the input to the network.
And you're going to try to learn the weighting between all
the interconnections between the neurons
in order to obtain the correct answer that
says whether this digit is 0, 1, 2 through 8, and 9.
OK.
So the best answer would be that this digit is an 8,
but you need to find out what would
be the structure of the network in terms of rates that would
actually match the image 8 to the outcome that
says 8 is a label of the image.
I wanted also to mention that nowadays networks
with more than two layers--
otherwise called "deep networks"--
have proven to work very, very effectively in many domains.
So example of deep networks include
what we call "restricted Boltzmann machines,"
"convolutional neural networks," and also "encoders."
So neural networks can do very well on applications,
but they are actually getting better
when you do a little bit more complicated structures
or architectures of the network
And when you have the computational power
to train and learn those weights.
So its very technical.
This is a very technical aspect of neural networks,
and it becomes more of an art to actually train the network
and find the weights between the connections.
I wanted to finish with this very interesting application
that's actually one of the state-of-the-art applications
of neural network which is called a "handwritten digital
combination."
There is a famous database called the "**MNIST database**"
that actually collect a set of 60,000 examples-- training set--
and 10,000 example in the test set.
The examples are images that are images of digits.
When you write in digits on an envelope,
this is actually captured as an image.
So if the digit is discretized to 28 by 28 images--
that is like you have 28 pixels by 28 pixels--
you create vectors of R and R784 if you do a grayscale images.
So labels are actually the digits they represent.
So we have-- our examples in this case
would be an example of actually 784 grayscale values
and the labels.
So for example, for the number 8,
it's going to be all the other pixels over this area
and the true label 8.
So our examples in this case are fed into the network,
and you want to be able to recognize every single digit.
There is a wide body of literature
on learning from this database through discretized digits.
So there have been many papers.
Linear model has actually performed very well
with an error rate between 7 and 12%,
but then people have tried K nearest neighbors
to protective machines, neural networks, and also
conventional neural networks with errors
varying depending on the publications
between in these ranges.
Now, the record is to a convolutional neural network
that actually decrease the error on the set
to 0.23%, which is actually very, very good.
So actually, this is a problem that is pretty much solved.
Your letters, your address in which
you write the ZIP code on the envelope
usually reach the destination.
This is because the reason smart neural network behind that
can recognize your digits and recognize your handwriting.
So this is a very successful line of research.
It's very technical.
It requires a lot of tuning of the weights
and the tuning of the number of neurons in the architecture
and number of layers, but it has been very, very successful
in several applications such as face recognition, in language,
and many other applications.
So I hope you enjoyed neural networks.




