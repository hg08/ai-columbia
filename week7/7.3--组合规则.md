# 组合规则

## 深度优先搜索和广度优先搜索

You could imagine different ways of traversing--
going through this lattice--
to search for these frequent patterns, which
means that finding this border, this border
between the frequent on the top and the frequent on the bottom,
by doing a search over the search space.
The search could be done with methods we have seen so far,
such as just breadth first search, or depth first search,
or even a mix of both.
So the breadth for search, we're going to as you know,
go through each level one by one.
And we won't go to the next level
unless we explore all the item sets at one level.
So we're going to start by the item sets of size one,
and then we combine only those frequents
to test in the next stage, and so on, and so forth.
In contrast, depth first search won't do the same way,
but we go actually extend an item set by size until it
reaches the maximum possible.
So we're going to explore the first item a.
If a is frequent and we're going to explore item ab.
If the item set ab is frequent, we're
going to extend it, and so on and so forth.
If it's not frequent, however it's not,
we're going to backtrack and check the combination
with another item.
So it's possible to explore the search space in different ways
in any search method we have seen so far.
The Apriori algorithm is what we call a level-wise algorithm.
It goes level by level as it adopts
a breadth first search on the lattice of possible item sets.
So start at level one, which would be k=1 here.
The idea is to generate candidates, check
the frequency, generate the next generation of candidates,
and so on, and so forth.
So the idea is to generate the candidates of size k.
Suppose you are starting with the size one,
so we start with finding only the singleton items.
So the candidates of size one will be a set of pairs,
item set of size k, and the support of size Ck.
For which for all elements x, including Ck, that's not empty.
The support of x is bigger or equal to minimum of support.
So remember, the downward closure says that actually,
if you're trying to make a larger item sets,
any subsets of this should be frequent.
So any subsets of Ck, any x subsets of Ck,
has a support necessarily bigger than minimum support.
Which means that we're going to create the candidates based
on the smaller frequent item sets.
once we get our candidate, what we do is to scan the data set.
Go through all the examples in the data
sets to check the frequency of each candidate.
So, we're going to scan the data set to compute the support
and we're going to keep only those pairs Lk, support of Lk.
L stands for large.
Large in apriori framework means frequent.
So we're going to keep all large k item
sets that have the support that is
bigger or equal than the minimum support
threshold we have set up.
And then after that, those frequent--
these frequent item sets will serve to generate the next.
Size of candidates are going to go back,
but this time with k+1.
So we have k+1 going to generate,
use the frequent patterns of size k to generate
the candidates of size k+1, and so on, and so forth.
So it's an iterative method that actually
goes through the search space of the lattice line by line.
Each time it generates candidates
it checks the frequency, how?
By scanning the data set to calculate
the frequency of each item set.
So let's take our toy example again with data set d.
So recall that we had five transactions, ab, ac, cd, bcd,
and abcd.
Suppose we set a minimum support threshold of 40%,
so which means that we're going to be interested in all
the current patterns that appears in at least 40%
of your data set.
All right?
So, at the first level of the-- so remember
we started with a lattice of empty.
The empty set.
So we are going first to focus on candidates of size one.
So we're going to check a, b, c, and d.
This will constitute the candidates of size one.
These are the item sets of size one,
c1. We're going to scan data set to see
how frequent each of these items sets are.
So for example, transaction 1 has ab, so for transaction one
we're going to increase any item set's frequency
that contain a or b
So the a will increase to one.
Initially set it at zero.
And the b would start increase to one.
Transaction two has ac, so I'm going to increase
the support of a to two.
And the support of c to one, right?
And so on, and so forth.
So at each transaction I'm going to increase all the item
sets that actually appear in this transaction.
All right?
So based on this, so the support is
the ratio of the frequency divided
by the size of the data set.
So this would lead us to the frequency
of the item set ab3 over five, b3 over five, etc.
Given the minimum support of 2.5,
I'm going to keep all of them.
They are all frequent item sets of size one.
So the set of frequent item sets of size one
will be each item a, c, and d with this supports.
OK?
So this is the first level, so I ensured
that I went through all possible candidates here
and I calculated the frequency of each of them.
And it turned out that they are all frequent.
So now I'm going to combine this with frequent item
sets of size one to make candidates of size two.
So the candidates of size two would combine a and b, a and c,
a and d, b and c, b and d, etc.
So for now I don't have any frequency of support,
so the support for now is just zero everywhere.
I'm going to do another scan of the database
to calculate the support of each of these items sets of size
two.
So ab contains the item set ab so I'm
going to increase ab to one.
For example the fifth transaction, abcd,
we are going to increase ab, ac, bc, bd, etc.
by one.
So we end up with the support of the candidates of size two
being two over five, two one, two two, two and 3, all right?
So again, we look at the minimum supports
we have and we are going to prove
that all the candidates of size two that don't meet the support
threshold.
So in this case I'm going to get rid ad
because it's not appearing frequently in the data set.
And with the downward closure, if an item set is not
appearing frequently, there's no chance
that a larger one will appear frequently.
So we're going to end up with a set of size two,
of frequent item sets of size two.
And the process goes on.
We're going to do again c3. Here we're
going to compare all of those that have some head together,
similar head for example a, b, and c are combined to make abc.
bc and bd are combined to make bdc, and this was to ensure--
this would ensure actually, to have to create
only candidates of size three.
So we don't want to combine for example, ab and acd,
because it's going to be a larger subset.
All Right
So candidate of size three.
I scan the data set, and I complete the support again.
I get rid of abc.
It's not frequent.
And I end up with a set of frequent sets of size--
of item sets of size three.
So the process is quite easy when
we talk about the two data sets we had with five transactions.
But real live data sets are really
not like this for example.
The characteristics of real life data sets
is that there are billions of transactions
and tens of thousands, and even millions of items.
So think about, for example, the Amazon surveys that
have actually lots of people using Amazon every day
to purchase the items.
And also there are lots of items to choose from.
So right is that we are dealing with billions of transactions,
maybe millions of items, and which makes
tera, and terabytes of data.
So this leads us to, actually when
we do the apriori algorithm, to have really a very, very, very
deep lattice of possible items sets on a different level.
So also, which will lead us to do a lot of scans
to calculate the frequency of all the candidates.
Which will actually also make them--
the process-- very time consuming
given that we need to access the disk
to make this calculation of frequency.
And the other problem is that there is also
a huge number of candidates that remember,
if you have a set of items i, if the item set is millions
of items, you're going to have a lattice of the size
of the lattice has all the possible item
sets is two to the size or cardinality of i.
So it's going to be really a computationally expensive
process to really uncover all the frequent patterns
or frequent item sets in your data set
that is very, very, very large.
There are different kinds of representation
of the transaction data set idea that
is proposed into the chercher to make this frequency
calculations, and access, and scan of the data set easier.
So that method that's actually adopt a row-side
of presentations, in which we have just
like in other machine learning algorithms,
we have the rows representing the examples.
And each row has all the items in which that actually
belong to the transaction.
Other positions are presented as a column-wise representation,
which we have for each item, we have the set of transactions
in which it appears.
Active A appears in one, two, and five,
the fifth transaction.
And the last representation is a Boolean one
in which we have the items appearing in them as features,
and the transaction identifiers appearing in the row.
So for example, the item A appears in transaction one,
two, and five, right?
So given any of this representation,
the algorithm that adapts to this representation
in order to calculate the frequency of the item set.
Recall the general framework in which
we have a set of transaction d, from which
we generate the set the frequent pattern f,
and we want to generate now the walls from set of f.
It is mentioned start extracting this set of association
rules are is way less expensive than mining frequent patterns
f.
And the reason is that we don't need anymore
to access the data set to calculate any frequency.
All we need to use is the set of frequent item sets
and generate rules from that.
So however, the process still remains
exponential in the size of the frequent item sets.
So if you recall the framework, so
given the set of frequent item sets f,
and we have now another threshold
called the minimum confidence threshold.
MinConf are going to generate all rules of the form.
Left-hand side implies right-hand side, right,
that are solid.
For example, if we find this rule with a 20% confidence,
this means that 20% of items that are in the left-hand side
are also in the right-hand side.
OK?
So these kinds of rules have a confidence attached to them
to express how strong they are.
One way to generate the rules and avoid this potentially
another bottleneck in the--
given that the process is exponential in the size
of the item sets, we're going to generate
all the rules in which we have the left-hand side is l minus c
and the right side is d.
So l is a frequent item set.
Remember we know they operate from a call it large
when is frequent.
OK?
So we're going to generate those rules,
and check that confidence.
Confidence is this value set that actually is calculated
based on the support of the item sets l
divided by the support of the item sets l minus C. OK?
So if this exceeds the minimum confidence then
we keep it as a strong rule.
Otherwise, we reject it.
All right?
So for a k item set, or otherwise
and item set of size k, we can generate two to the k minus one
possible.
So this is possible thanks to the another property
that was introduced by Aguilar and his coworkers,
it actually means that if a rule with a consequence c is solid,
then all the rules with consequences that are
subsets of c are also solid.
So we're going to see in that example
how this property could actually help us just generate
the rules that are strong.
Suppose we have a minimum confidence of 60%.
We start with the item sets that are frequent
so these are coming from f, right?
So if we have the frequent item sets ab,
then we can generate two possible rules-- a imply b,
and b imply a.
We calculate the confidence of the support of ab, both ab,
divided by the support of of a.
So that's how many transactions have ab among the a.
OK?
So that's going to be two over three.
Is this rule strong?
Yes, given that exceeds the support of 60%.
So I'm going to do the same thing for all frequent
sets of size two.
The process goes on for all--
every level of item set.
So for item set bcd, we could generate cd
implies b, bc implies c, etc.
And also items on the left side
implies bc, and likewise are going
to calculate the frequency, actually
the confidence of the rule.
cdb is the support of cdb divided by the support of cd.
OK?
So in this case, we're going to keep all of them
except this last rule here.
I don't have enough confidence to be kept as strong rule.
All right, this is how the process works.
So you can see hear that you're going
to go through all the items and generate all possible rules,
and check just the confidence without access
to the data set at all.

## 组合规则的几率解释

So this is how association rules are generated based
on a set of frequent item sets.
There have been also some efforts
in formalizing the task of association rule
with a probabilistic interpretation and framework.
So this task was actually proposed
by Brin and his coworkers in 1997,
in which an association rule R, A implies
C, in which A is called antecedent,
and C the consequent of the rule,
is expressed as a distribution of A and C
in the finite space of the data set D.
So we're going to see estimate the probabilities in a space
limited by the data set D.
So the sets A and C are seen as two probabilistic events
to which you could calculate the probability P of A and P
of C that stand for the probabilities
that event A and event C happen respectively.
So the probability in this case, as I said,
is estimated by the frequency of A and C
respectively in a dataset D.
So recall the support of a rule A
implies C is the support of observing the items in A
and the items in C. So this is the support of A union C.
In the probabilistic framework, we
talk about the probability of observing the event A
and observing the event C happening.
Likewise, the confidence of A implies C
is written as a probability of the events.
In this case, the probability of observing the event C given
that the event A happened, which would be just
simply the conditional probability of C given A.
So it's going to be the probability of A intersect C
or A and C divided by the probability of A.
There has been also some efforts in devising
other kinds of evaluation measures
beyond the support and confidence.
Why beyond the support and confidence?
Because this pair has some limitations,
as we can see in this example inspired from a paper
from Brin et al in 1997.
So suppose we have a data set that we summarize
in this confusion metrics.
This summarizes the frequency we observe for the customers
buying tea and coffee.
So for example, this number here, 20 of the customers
bought both tea and coffee.
50 of the customers bought tea but didn't buy coffee.
20 of the customers didn't buy tea but bought coffee.
And 5 customers didn't buy coffee nor tea.
We're going to express the marginals
in terms of the sum of the rows and the sum of the columns.
The sum of the rows express the number
of people who bought tea, the number of people
who didn't buy tea.
The columns will be the number of people
who bought coffee and the number of people
who did not buy coffee.
The total number of instances or examples
would be 100 in this case, which is the sum of the marginals.
We are interested in the rule tea implies coffee.
This association rule by the support and confidence
framework is interesting, because the support
of the world would be equal to 20%.
So we could observe that we have 20%, 20 over 100,
which represent 20% of customers who
bought tea also bought coffee.
This is a support of the appearance of both tea
and coffee in the data set D.
The confidence would express the conditional probability
of buying coffee given that the customer bought tea.
And the confidence if you apply the measure,
the formula of confidence which would
be the support of tea and coffee of 20%,
divided by the support of tea, will represent 80%.
If you put the minimum support of, say,
20% and a minimum confidence of, say, 60%,
then this road here would be deemed as a strong association
rule.
It is a strong one.
However, it's probably not an interesting one.
And why is that?
Simply because it's a misleading rule.
Observe that the support of coffee by itself is 90%.
This actually introduces a big bias in that the confidence
cannot detect.
90% of the data is actually about customers
who bought coffee.
This would provide lots of weight to coffee,
and this cannot be detected by the confidence measure alone.
Actually, the confidence of a rule in this case
would be the confidence of tea implies
coffee, which is equal to the support of tea and coffee
divided by the support of tea, would simply actually ignore
the support of coffee, which is actually
a very high support of 90%.
So that's where the limitation of the pair
of support-confidence comes from.
Therefore there was some efforts in devising other evaluation
measures, such as interest, also known as the lift.
The interest of a rule antecedent
implies consequent is the probability observing
both the antecedent and the consequent,
divided this time by the probability of A times
the probability of C. So you could see that in this case,
we are factoring in the probability
of the consequent, which would be the coffee in our case.
This probability is estimated by the support
that we observe in the dataset D, which
would be the support of having the items in A union C
divided by the support of A times the support of C.
Depending on the value of interest, if the interest is 1,
we deem the relationship between A and C
as two independent events, and then
there is no relationship between them.
If the interest is bigger than 1,
then A and C are positively correlated or dependent.
And if the interest is less than 1,
which means that A and C are negatively
correlated or dependent.
So we could write other forms of writing the interest of A
and C. If we have the confidence and the support,
we could simply turn that into using
the ratio between the confidence of the rule divided
by the support of C, which has been calculated
in the previous step of combining frequent patterns.
So let's take again our example of tea and coffee.
Besides the confidence and the support of the rule,
we can calculate the interest of the rule tea
implies coffee by plugging in the numbers in the formula
that we just defined.
The interest of tea implies coffee
will be in this case equal to 0.89, which is less than 1.
So by this interest, we are going
to deem this rule as interesting,
but the tea and coffee items are actually negatively dependent.
So we could also think of generating
all possible rules from this confusing matrix, which
would be tea implies coffee, tea implies not coffee,
coffee implies not tea, and not coffee implies not tea.
We could calculate the interest measure,
and we would see that actually there
is a strong relationship between tea and not coffee,
and between coffee and not tea.
So it makes sense that maybe coffee drinkers are not always
tea drinkers, and vice versa.
So this gives a better picture about the strength
of the rule than the pair support and confidence alone.

## 多维规则

Another line of research in association rule
就是：　to consider multi-dimensional rules.
So if we talk about just simple items,
we could express their rule as bread implies butter.
But if you want to put that into another formalism,
so we have bread implies butter, that actually you
could write also similarly with a different syntax
as buy x bread implies buy x butter,
so where x is the customer.
A customer who tends to buy bread tends also to buy butter.
So if we move toward multidimensional rules in which
we talk about maybe the age of the customers,
then we could include what we call more predicates.
In this case, you could express a rule
like if x buys pizza and age of x is young,
which is young customers, then x is likely to buy also a Coke.
So we are going to just extend the framework by using
what we called predicates.
We will include variables.
And this will allow more expressiveness
in the formation of those.
So in this case, instead of talking about items,
we talk about predicates.
Instead of talking about k item sets,
we talk about k predicate sets.
So another question is how about handling,
how do we handle numerical features or numerical variables
in the case of association rules?
So we could simply think about what
we called discretizing, or splitting the domain
into different intervals.
If you want to be specific about how young are the customers,
you could say that most of the people who
buy pizza who are between the age of 18 and 22
tend also to buy Coke.
So we're going to discretize the feature of age
in different ranges.
But you will see that this is not necessarily the best
approach to handle numerical features in the case
of association rules.
Another fact about association rules
is that they may lead to a very large number of rules, which
makes it difficult to sift through the rule
to find the nuggets, or the interesting relationships.
So how can one reduce the number of rules?
This can been done in different ways.
We could use many evaluation measures.
So we have seen that we have the pair, support confidence.
You could make it as a triplet, support confidence interest.
We could use also the other iteration
measures that were proposed in the literature.
And what we can do also is to increase the minimum support.
The more you increase the minimum support,
the less frequent patterns you generate
and the less rule you will generate.
We could also increase the minimum confidence,
which means that we are really interested in very
strong associations that are let's
say confident by 80% or 90%.
Finally, we could impose some rule templates, in which case
you are not going to generate all
possible combinations of items on the left-hand side
and the right-hand side of the rule,
but you are going to just maybe focus
on one item on the left-hand side
one item on the right-hand side, and even
be specific about what features to use, et cetera.
So it means that defining constraints
on the length of the rule, on the items to put in,
and to include some specific items or exclude some of them.
OK
I wanted to talk a little bit now about the implementation.
There have been very successful implementations
of mining association rules.
There is actually a repository of algorithms and data sets
that you could explore.
It's called the F-I-M-I, FIMI, Frequent Itemsets and Mining
Implementations Repository, that actually is maintained since
2003.
There have been after two workshops called FIMI
'03 and FIMI '04.
So this website here would give you
a wealth of information about data sets and algorithms
that are implemented that you could use and try.
There is also a famous implementation
by Borgelt of the Apriori algorithm
and some of its variant that you could find in this link here.
You probably know the system Weka,
which implements many data mining algorithms and machine
learning algorithms, and includes actually
a strong module on mining association rules.
And finally, there are many others.
And if you are interested in using Matlab for mining
association rules, there is the system ARMADA
that you could explore.
And there are many, many others.
But these are the ones that I think are the most well known.
There have been also several extensions
of the Apriori algorithm since the seminal work from Agrawal.
Actually, the different algorithms
are based on different search strategies,
which we know well now.
Breadth First Search has been the first approach,
in which the algorithm goes level wise in the lattice,
level by level.
And this is actually very time consuming
when the size of the frequent itemset is long,
which means that we have to go very far in the lattice depth.
So methods of doing Breadth First Search methods,
doing Breadth First Search include the Apriori algorithm
itself, some extensions of it to use transaction identifier
intersections, Partition, DIC, et cetera.
Methods that use Depth First Search
include Eclat, Clique, and Depth Project.
And finally, there was a method that
combined the benefits of Depth First Search and Breadth First
Search, such as AprioriHybrid, Hybrid, Viper, and Kdci.
Other methods are actually using a graph representation
of the data and finding patterns on those.
And these methods are called pattern growth.
That means that there is no candidate generations,
but the data is represented in graph.
Methods like this include Fpgrowth, HMine, Cofi,
et cetera.
This has been for some time really
an active research area to find the best
way to address the computational complexity of the problem.
So you have seen so far that we are talking
about a transaction, like I said,
in which we have the examples in lines or sets of items
that the framework has been extended
to the problem in which we have a simple machine learning
table, in which we have the examples in the rows
and the we have the features in the column.
So Apriori has been initially designed
for what we call Boolean tables, or transaction databases.
So in this case, propositional logic
has been sufficient to express the item,
the item sets, and the rules, such as milk implies cereals.
In the case where we have a regular table
like in the machine learning original framework,
within which we have rows and columns of features,
we are going to talk about relational tables.
And we can extend the framework and the notions of items
to literals.
So in this case, an item is no longer an item in the table,
like an item in a supermarket.
But it's a pair of attribute values, such as age, 20.
And this value is seen now as an item
to generate frequent item sets.
In this case, the attribute could
be seen in three categories.
The first one is categorical.
This is simply cereals, color blue, et cetera.
It could be quantitative with actually
a few numerical values.
For example, the number of cars could be either 1 or 2.
It's really that the family has three or more cars.
So it's more like a numerical value with a few numericals.
For example, the number of cars, 2,
could be a pair attribute value.
It could be seen as an item.
And finally, if the quantitative or numerical value
has large domains, then we have to discretize.
And in this case, the item attributes
value would be something like age in the interval 20, 40.
So I built items depending on the category of attribute
that we have, either categorical, quantitative
with few numerical values, or with large numerical values.
So here's an example, inspired from the literature.
Suppose we have a data set of people, D. This data set has
actually a set of examples, examples just
like in a regular machine learning framework.
We have the transaction identifier,
or the line identifier.
This is the first example, and that
has an age of 23 who is not married
and who has one car in the family.
So we are going to have this table, which
is a table without a label.
So there is no label.
It's completely unsupervised.
And we are going to--
example of extracting infrequent item sets.
In this case, a pretty good set would be the age range 20, 29.
The age and the range, 3, 29.
Whether the person is married, yes or no.
And the number of cars, 1, number of cars, 2.
And combine these kind of items sets, such as age is in 30, 39.
And married?
Married is yes.
So we're going to calculate the support in this table.
For example, this last item set is actually
frequent in two of the examples.
There are two examples with age between 30 and 39,
and who are married.
Sorry, there's two lines here.
We're going to apply the framework exactly similarly,
but this case, the item has a different formulation.
And from this frequent item sets,
you are going to generate the words, such as this one that
expresses that basically 66.6% of people aged between 20
and 29 has only one car.
And this rule, which means that number of people in this age
range and with this number of cars
actually represents 60% of the table here.
So we're just reformulating the problem
in this case, where we have a more complex kind of item
in the data.

### 定量组合规则


So we have seen so far that for numerical attributes, if they
have a few number of values, we could see them as categorical.
If they have a large number of values,
we have to split the data into intervals.
So this is not simple.
And the question is, can we consider
mining quantitative association rules, which
are the rules that contain numerical values in intervals,
as a simple extension of mining categorical rules?
And the answer is no.
And the reason is that first of all,
there is an infinite search space.
We don't have any more discrete number of values.
In Boolean association rules, the apriori property
loves to prune the search space efficiently, which
means that if an item is not frequent,
none of the more complex items on your items
when you add items to it will be frequent.
But we do explore the whole search space
of hypotheses, a whole lattice of item sets, which
is impossible for quantitative association rules,
because there are an infinite way to represent
these intervals in the data.
The second problem is that the support-confidence trade off
is actually choosing an interval that is quite
sensitive to those measures.
If the intervals are too small, then they
won't have enough support.
If the intervals are too large, then we
are going to lack of confidence.
There is in this case a trade off
between finding rule in which there
is a balance between support and confidence.
And of course, here in this case,
we don't have any label to rely on
to split the data into intervals anyway, right?
Just dealing with a completely unsupervised discretization
of the numerical values.
There have been different attempts
to mine quantitative association rules.
And these include discretization based approaches, distribution
based approaches, and finally, optimization based approaches.
So for the discretization based approach,
we're going to perform a pre-processing step in which we
just split the interval, the domain
of the numerical features into intervals.
So we could use different strategies,
use domain knowledge to have this cutoff points
of the numerical features.
And there have been several approaches to do that.
We could also do some discretization-based
clustering or interval merging.
For example, if you have two intervals that actually don't
have the support, you could merge them
so as the final support of the final interval would be larger.
So the problem with this approach
is that it is univariate-- you don't just
look at how the features interact between each other.
It can be sensitive to outliers.
So if you have an outlier, it would
be making the interval, maybe the last intervals
on this on the extreme sides of the domain being very large.
And we lose a lot of information because
of this new variability of the discretization,
and also the presence of outliers.
The second approach to find association rues that
are having numerical attributes is
based on the distribution of those attributes.
So we are going to just include statistics of those features
in the rule itself.
So for example, this rule expresses
that females are lucky to have a height with the mean of 168,
and the weight with a mean of 68.
So we're going to plug in this standard deviation
and mean of the features in the rule itself.
There have been some attempts to do that.
However, the form of the rule is restricted
to some pattern on the left-hand side and some
on the right-hand side, to generate those kinds of rules.
Optimization-based approach are actually
the last line of research in mining quantitative association
rules, in which numerical attributes or features are
optimized rather than discretized.
There have been different approaches to do that.
One of them is inspired by image segmentation, in which we
are going to find intervals that optimize the gain of the word
rather than just concreting the support and confidence.
So we're going to restrict the rules into one or two
numerical attributes at a time to perform that
in this first line of research.
The second line uses genetic algorithms
to optimize the interval of the features and the attributes,
and uses some fitness function that takes in consideration
the number of attributes, along with the amplitude
and other parameters in the intervals.
So it adopts actually and apriori-like algorithm
to mine the set of association rules.
The most recent approach includes using half-spaces
to mine such rules like a rule that says x sub 1 is bigger
than 1, then 0.5x sub 3 plus 2.6x sub 6 is bigger also than
100.
But this approach unfortunately does not
handle categorical attributes, because of its attempt
to find half-spaces to mine such rules.
A last approach is called QuantMiner
that optimizes the gain of the rules, templates using
another genetic algorithm.
So suppose you are combining in a rule
two attributes, attributes one and two,
in which we have two intervals, so we're going
to instantiate these intervals.
And we're going to apply genetic algorithms and their operators
to generate other rules.
And the idea is to improve the rules along generations.
So suppose we start with this these tools,
with some initial instantiations of the limits of the interval.
If you do a crossover, you're going
to generate new kinds of combinations
of these intervals.
You can also do a mutation, in which
you are going to reduce the size of the interval
from the left hand side or the right hand side.
So by doing so, we're going to generate
generations and generations of rules
based on the two operators of cross over and rotation.
And we are going to keep those that have actually the best
gain, which is a combination of the support and confidence that
makes them balanced in the assessment of whether a rule is
interesting or not.
So let's take an example, with the family that
is set from the machine repository, UCI Iris dataset.
So we have in Iris three species of flowers described
by their petal length, petal width, sepal length,
and sepal width.
And you want to find some rule that
has some template, such as have the special value
on the left-hand side implies any combinations of the feature
of the flower on the right-hand side.
And you want to generate the support and the confidence
of them all, OK?
So applying something like QuantMiner
to generate quantitative association rules will lead
to the rules that describe, let's say,
species equals setosa, such as the iris flowers.
23 of the flowers are actually having
a petal width in 1/6 interval, a sepal width of 31,
39 are in millimeters, have a petal length of 10 to 19,
and a sepal length in 46, 54.
And rule is actually have a confidence of 70%.
70% of the flower setosa has this combination
of feature values, of feature intervals in the data.
The species setosa with these features
represent 23% of the dataset.
So we could do the same thing with having
this versicolor on the left hand side,
or virginica on the left hand side.
So these rules are genetic, basically,
by combining those intervals in several ways
throughout many generations of rules.
But we do start with the rule template
that actually we want to fill by finding actually
the best values of the left and the right of the intervals.
If you want to play with a tool to mine association rules
and quantitative association rules,
you could play with this tool that's
available on GitHub, in which we have a simple process.
So in the case of the iris dataset,
we start with a table in which we have the examples
and the features to associate.
And so this is the UCI iris dataset.
The goal of the algorithm is to mine all interesting rules
with respect to some pattern.
So it's a five-step wizard, in which you select your data,
you select the features you are interested in,
and you ask the system to generate the rules for you.
The system actually allows you to open
a dataset of feature values.
So you have the features on the column,
you have the examples on the rows.
So this is the example of the iris dataset.
It is one of the benchmarks in the tool.
The tool is a set of five steps, in which you choose your data
attributes, choose your template of rule
that you're interested in, and then generate the rule.
So as an example of a rule in which we
have an iris versicolor is it has a petal length
in the interval 3.3, 4.7, petal width of 1 to 1.5,
and this is generated by the genetic algorithm, in that you
could observe the interval in the whole domain of each
of the numerical features.
And this rule is deemed interesting,
because it has a confidence of 82%
and a support of 27.33% in the data set of 150 flowers.
So these were association rules.
I think it's an interesting line of research
in the unsupervised setting of machine learning.
So you could use it as an expression tool
when you have to understand better your data,
see some cross-training of the different items.
Even if you interact with a domain expert,
it's good to sit together and explore what data you have.
It's an interesting tool to do as a preprocessing
as an explanation of your data, before you even
start to try other kinds of supervised and unsupervised
machine learning methods.
So I hope you found this useful.
I will see you in the next lecture.


