# 6.1 分类和感知算法

## 分类

在分类问题中，我们有一个训练数据(例子的集合／记录的集合／样本的集合)．每一个记录由一系列的特征X以及一个标签y构成．
这里的标签是取的**分立值**，或是定性的值．比如，
$$
{\bf y} = \left(
\begin{matrix}
 
0 \\1

\end{matrix}
\right),
$$
或者
$$
{\bf y} = \left(
\begin{matrix}
 
\text{Banana} \\ 
\text{Orange} \\
\text{Grape }

\end{matrix}
\right).
$$


可见，标签属于一个集合，在分类问题中，我们一般用
$$
+1, -1 \nonumber
$$
或者
$$
1, 0 \nonumber
$$
等数字来标记这些分类.



**分类的目标是**：

学习一个分类函数f
$$
f:  f({\bf X}) = {\bf y}
$$

当一个分类模型是线性的时，我们就在做线性分类．就是说:

$f$由一个线性函数(或者一个线性超平面)表示．



**分类的例子**： 

建立分类，意味着找到决策边界来区分＋和－.



## 感知算法

感知算法(perception algorithm).
感知算法属于神经网络,　而神经网络是一类试图模拟大脑功能的算法.

1959年，**弗兰克－罗森布拉特**(Frank Rosenblatt)发明了感知机(Perceptron)，这是神经网络算法中的第一个．[1]

![罗森布拉特](/home/huang/git/ai-columbia/week6/罗森布拉特.png)



通常,神经网络可用来识别手写的字符, 说出的话,　以及人脸，且效果极好.

该方法在上世纪90年代很流行,但接下来就不那么流行了. 随着深度学习的兴起，神经网络再次流行.
一般地，当我们**想要将可分离的数据完全分离时**，感知算法就可以用．
在这里, 我们有多个**正样本**well-behaved on one side,
在另一侧，我们也有多个负样本．此情形下，我们可以找出一个**线性函数**将这两类样本分离开．
是故感知算法是一种线性分类方法．

实际上，它是多种线性分类方法中最简单的一种．
它表示最简单的神经网络.
仅当对于完全可分的数据，它才有效.
由于受到生物学的启发，感知机模仿神经元在单神经元层次的最简单的功能.
每个神经元将实值之集合作为输入，并输出一个值.
这个输出值又可以作为其他神经元的输入．
具体地，感知机将数据中的**特征之集合**作为输入,并完成这些值的**线性组合**.





然后，该输出将经过一个阶跃函数．该函数的输出是1 如果线性组合的值超过

某个阈值, 否则该函数就输出-1.　例如，双曲正切函数
$$
f(x) = \text{tanh}(x)
$$
![tanh_x](/home/huang/git/ai-columbia/week6/tanh_x.png)





So we could write typically that the function that we want
to learn or the model is the sign of, again, a weighted sum
of the feature values.
Again,感知机算法 works perfectly
如果该数据是 **linearly separable**.(该算法收敛的条件)
这是它收敛的一个条件.
如果不是线性可分的,那么这个算法就不会收敛. 

Perceptron的基本思想是： 从一个随机超平面开始，
并利用训练数据调整其权重.

我们将把这些数据喂给这个Perceptron.
每当你产生了偏差，你将修正这些权重.
这是一个迭代方法，该方法将检查每一个例子，并且试着修正,
和匹配输入值到输出值，通过相应地改变权重.

算法如下.
我们的输入是特征值的集合, 连同一个标签,　其输出是这个超平面.
因为我们在讨论线性分类, 我们将得到该超平面的权重.　The weights would be called typically in Perceptron w's.

在线性回归中， we are talking about the betas.
So the weights will be one weight per dimension
or per feature.
We would have, 
$$
w_1, ..., w_d; \   w_0
$$
其中，
$$
w_0
$$
代表截距(offset, intercept).

算法开始时，将权重都初始化为0.
And it will repeat，直到收敛.
Repeat what?
对每一个例子，检查当前这些，定义出这个超平面的权重，在每一个例子上做得怎么样．
我们将仔细检查所有的例子,例如，我们将检查是否当前模型（当前权重）能将例子正确分类.
为了达到这个目的,我们能做的就是
将正确的标签
$$
y_i
$$
与这个例子的函数相乘．　这意味着我们应用当前权重（当前模型）,到这个例子,
并检查是否我们出了差错．

如果恰巧有
$$
y_i f(x_i) < 0
$$
这意味着我们有具有不同的极性(polarity)的两个标签, 也就是说我们出错了.
如果当前权重（当前模型）不对, 那么我们将调整权重.

当前超平面不能对我们将要测试的例子进行分类.
如果我们做了错误的分类, 我们将修正该超平面.
更新权重的方法就是： 通过增加 
$$
y_i x_i.
$$
其中，
$$
y_i
$$
是真标签.
$$
x_i
$$
是特征值本身.
这个过程将重复进行，直到收敛．这意味着我们将发现能分离这些数据的一个超平面或者或一条线，也就是把标签为正的例子与标签为负的例子分开.

关于感知机算法，这里有一些observations．
首先, 权重
$$
w_1, ..., w_d
$$
决定决策边界的斜率．也就是决定了我们将得到什么样的超平面.
w0决定了补偿值(offset). 在回归情形，我们也称之为Percept.
它决定了决策边界的补偿值.
也将它记为
$$
b.
$$
Let's clarify what line 6 represents.
Line 6 表示更新权重.
如何更新这些权重呢?
如果我们对一个标签为正的例子做了错误分类, 也就是说
$$
y_i = 1,
$$

$$
f(x_i) = -1.
$$

那么修改权重的方法就是把x增加到权重矢量．因此我们将使超平面相应地倾斜．
如果我们对标签为负的例子做了错误分类，
那么我们将减去x, so as we cover properly
the example that we have.
对负例子判断失误意味着例子的标签是-1,

但利用当前的权重，我们将其标签预测成了1.
因此我们必须对权重做相应地调整：我们从权重矢量减去x.
There's some other variance of adding or subtracting
x from the weight vector.
And one of the variants is to either subtract
1, which just provides different speeds of convergence,
but it's the same principle.
何时收敛呢?
当权重不再改变时，就收敛了.
此时我们就得到了将正负例子完全分开的超平面.　此时就没有必要更新权重了,
which means that in our condition here, yi f of xi
never becomes negative anymore for all the data points.

Here's a toy example illustrating
the principle of the Perceptron on a few data points.
So we have two classes.
The red points represent the positive class,
and the blue points, let's say, represent the negative class.
we start initially with a random hyperplane that actually
misclassifies most of the time.
如果我们think that everything on the top is red,
everything on the bottom is blue,
it is misclassifying these four blue points here.
By changing the hyperplane, we are just moving the line
back and forth until this line classifies the data properly.
So this is the progression of the line.
At some point there will be some convergence,
and the line will actually be able to classify
the data perfectly, just like in this configuration.　最终收敛.
每一个数据点都被适当地分类了.
在超平面上面的点将被分到红色类.
Everything on the bottom will be classified as blue.

So we could plug in this data some examples, some test
examples and see what we are doing well.
So if we have additional data points,
we would see that we'd classify them as positive if they
are on the top of the hyperplane,
or negative if they are on the bottom of the hyperplane.
Note that the w's determine the contribution of each
of the features to the label.
So we do linear combinations of those w's times
the features x to see how this actually affects the label.
Note also that minus w0 is the quantity
that the weighted sum of the features
needs to exceed in order for the Perceptron to output 1.

最终, 感知机能表示任意**Boolean**
**functions**, 例如 **the AND, the OR, the NOT AND, the NOT**
**OR, and the NOT** negation, but cannot represent others.
For example, consider the **OR function**
for which we have typically positive examples on this side,
and the negative example on the other side, in which case
if we have 0,0 the output of the OR function is 0.
But anything else in which we have at least one 1
would have an output of 1.
However for the XOR, we would have data points
that lay so that we have the positive points
on this side, the negative points on this side.
And no matter how we try to draw this separating hyperplane,
it's not possible to find anything that would
separate perfectly our data.
So in this case, the **XOR is not a Boolean function** for which
we could build the Perceptron.

So the idea of neural networks is
to use many Perceptrons to build a neural network.
So the network uses the ability of a single Perceptron
to represent elementary functions, and combine them
in a network of layers of elementary questions.
However, a cascade of linear functions is still linear.
And we want them to represent higher nonlinear functions.
So we see this in the next lecture.

A final observation here is that actually the Perceptron
picks one hyperplane or one line separating that data.
There are actually many other lines
that could be good candidates.
那么，哪一个最好呢?
There are lots of possible solutions.
And the idea of methods like SVMs is to find 最优解 rather than just picking
one of the possible solutions.
下一部分，我们讲神经网络, 也就是将多个Perceptrons结合起来，用以解决更复杂的问题.

# 扩展

[1] 弗兰克－罗森布拉特出生在纽约，1956年在Cornell大学拿到博士学位，留校任教，研究心理学和认知心理学。1957年，罗森布拉特提出了Perceptron的理论。1960年他用硬件结构搭建了一个神经网络．![img](https://common.cnblogs.com/images/loading.gif)

　　　　　　　　　　 ![img](https://images2015.cnblogs.com/blog/1038216/201612/1038216-20161201114836131-705673445.png)

 

　　但是和所有先驱一样，罗森布拉特开创性的工作并没有在当时得到认可．　当时两位科学家Minksy 和 Papert对罗森布拉特的工作表示质疑，写了一本书来批判感知机，书名就叫《Perceptron》．从此，Perceptron沉寂了将近20年. 直到80年代, 辛顿(Hinton)发明BP算法，让感知机成为当今AI最热门的领域.