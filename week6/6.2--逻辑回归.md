# 机器学习(二)

# 6.2 逻辑回归

I will talk now about 逻辑回归：统计和机器学习里的一种方法.
逻辑回归 is actually not a regression method.
其实是一种分类方法．

学习内容：

逻辑回归的思想，以及如何用它来做分类.

So we dispose again of training data
with feature values and labels.
标签可以是 二进制标签--
1, －1,或者 0, 1.
We want to learn a 分类函数 and the 分类模型 here is 假设为线性
if it is represented by 一个线性函数-- 一个线性超平面.
So far 我们已经见过几个类似的分类的例子,
分离出垃圾邮件,
良性肿瘤和恶性肿瘤的分类.
And also, let's take the example in this lecture about credit
default--
or not default-- so we have customers in a bank
and we want to be able to decide for a new customer
whether he would be defaulting or not.
数据为两个特征--存款和月收入，标签是默认是或不是.
So if you put this data-- it's not perfect data--
there is some overlap between the classes.
So we have, on the x-axis the balance--
on the y-axis, the income.
And we have positive and negative examples
that say whether the customer has defaulted or not.
In fact, we can't really predict credit card default
with any certainty.
Suppose we want to predict how likely is a customer
to default. This is an output of probability between 0 and 1
rather than a binary classification 0 or 1.
So it makes sense and would actually
be more suitable and practical for the bank
to know how likely is the customer to default
with the value between 0 and 1.
So in this case, we're talking about an 实输出,
so we are talking about 在区间[0,1]上的一个值.
But that is bounded by 0 and 1, which makes it a classification
problem.
What we are trying to model in this case--
a function that maps the input to the output.
And what we're trying to model specifically
is actually the probability of default
given the data description.
So probability of default is equal to yes
given that the balance value is such and such.　
You must be wondering whether we should just use linear regression.
After all, we have a set of feature values
and we have an output that is 0 or 1.
And linear regression allows us to find a numerical value
as a prediction.
Yes.
However, first of all, this would work only
for binary classification when we have two classes,
and won't work for multiclass classification, when
we have three or more classes.
Also if you use linear regression,
we could expect that the prediction would
be outside of the domain 0 and 1,
so we have to make sure that this has to be really tightened
into 0, 1.
And finally, the model can actually be poor.
Here's an example.
So in the credit card default example,
we're going to put all the positive examples on the 0--
property of 0 default. And all the points
that have a probability of-- this is our input data.
Our input data is the feature values
with the label that is either 0 or 1 or minus 1 and plus 1.
It's not a numerical value as label, right?
So we have our data point laying actually on the 0 and 1.
If we do a linear regression, we are
going to get this line that actually
has a very poor performance in predicting
whether the probability that the customer will default or not--
just like for linear regression, we're
going to model the problem with a linear function.
So we're going to write y as a function of x-- function of the input or the feature values--
as beta 0 plus beta 1 x-- this is the equation of a line, one more time.
Just like for linear regression.
So in the specific case of the example,
we are going to write the default
value of the label as beta 0 plus beta 1 times the feature
balance.

我们的目标是find the function f that actually is bounded

between 0 and 1, and the function
f is the probability of the y.
The default value is equal to 1 given the data we have, x.
OK.
在这里我们用sigmoid 函数,
that actually is g(z)--
is e to the z divided by 1 plus ez, or also
it could be with an as 1 divided by 1 plus e minus z.
OK.
So this actually is a beautiful as function
that actually **is bounded between 0 and 1,**
**that actually goes to 1 whenever z goes to infinity,**
**and goes to 0 whenever z goes to minus infinity**. 
So this is a function that actually
smashed data between 0 and 1 and has a nice s shape that
would actually give us a good distribution of the probability
of y given x.
So now we have a new f(x), and a new f(x) is a sigmoid
function-- the g function-- of the linear combinations
of our features-- beta 0 plus beta 1 x.
In general, f of x is the sigmoid function
of the sum of beta jxj if we have d dimensions.
In other words, we are going to cast the output
to bring the linear function quantity between 0 and 1.
One can use any other-- by the way--
any other kind of s-shaped function
as a g to cast the results between 0 and 1.
So here's a comparison between what an s function would
look like on the data of credits default as
compared to the line that we're obtaining actually
using linear regression.
再次注意：逻辑回归不是一个回归方法,而是一种分类方法.
So here is an example of using 逻辑回归
来做预测.
So suppose we have learned the betas
and the betas are beta 0 is minus 10.65.
Beta 1 is 0.0055.
What is the probability of default for a customer
with $1,000 balance?
So I'm going to plug these numbers into the function--
the logistic function.
The probability of default in this case
equal yes given that the balance is $1,000.
We are going to plug the numbers that we have for the betas
and for the $1,000 into the logistic function.
We have 1 divided by 1 plus e minus
z, which would make beta 0 plus beta 1 x1,
which would bring us to a probability of default given
equally as given the balance of $1,000 of 0.00576,
which means that the customer is unlikely to default.
If you want to predict the class,
we can make a threshold whenever the logistic function g of z
is bigger than 0.5--
we predict the class to be one.
Otherwise, we predict the class to be zero.
But if you want to probability a prediction,
then we could use just simply the logistic function
without the threshold.
So now, how can we find the beta?
So we're going to do something similar to find the beta as we
did for linear regression.
So recall we calculated some loss function
that is the difference between the predicted
and the actual label.
So we're going to define the loss
as half of the described difference
between the predicted and the true label.
So we're going to put that into the risk function that actually
is the sum over all the data points of the last function.
All right?
So remember that f(x) is now the logistic function.
It's not the linear function, which
means that f(x) minus y squared
is no longer the quadratic function we
had when f was linear in the case of linear regression.
So now the cost is a little bit more
complicated to calculate because it's
a non-linear function, which means that if it's not
a quadratic function, then we would have many local optima.
Hence the gradient descent-like methods
will not find the best global optimum.
So we have to find a different function that is rather convex.
Convex means that there is-- if we
descend the gradient we are going
to find only one local optimum.
So we are going to transform this function here.
The logistic function is interesting
because it has the s-shaped function.
It can get us the probability between 0 and 1,
but it's not convex.
So we're going to convexify it.
So here is a new convex function.
If the cost of f of xy is either equal to minus
log f of x if y is equal to 1, or 2 minus log 1
minus f of x if y is equal to 0.
So we could observe these two configurations, these two
cases, that are actually also shown in these pictures.
If y is equal to 1-- if the projection f of x is equal to 1
means that we didn't make a mistake-- the cost is 0.
If y is equal to 1 and the prediction
is f of x is equal to 0, then the cost
is infinity, which means that this function here makes sense.
It has nice features for predicting
whenever y is equal to 1.
If y is equal to 0 and we don't make a mistake,
the cost is infinity.
If y is equal to 0 and we make a mistake, the cost is 0.
So we're going to define a function that's
actually combining these two functions here.
And the function is actually-- because we
have y is equal either to 0 or 1,
we can write the loss f of xy as minus y log of f of x minus 1
minus y log of 1 minus f of x.
So you could see that in the function.
For y equal to 0, we we're going to have this term--
we go to 0.
We have minus 1 minus 0 log of 1 minus f
of x, which is minus log of 1 minus f of x, which
corresponds to this case here.
And if we have y is equal to 1, then minus y
log f of x will be minus log f of x and this whole term
will be going to 0.
So we're going to have only minus y
log of f of x if y is equal to 1 just like this case.
So the risk function in this case would be minus 1
divided by m sum of this different loss of f of xy
which will bring us to this function here.
All right?
So we're going to do something similar afterwards
with gradient descent, and just like we
did for linear regression, I'm not
going to go into the details here and leave these details
to your machine learning class that
is coming after the AI class.