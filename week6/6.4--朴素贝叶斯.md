# 朴素贝叶斯

## 贝叶斯规则

### 学习目标
1. 生成模型
2.　什么是贝叶斯分类器？
3.　基于数据，如何估算概率？
4.　回忆条件概率的含义？
5.　

### 贝叶斯规则

今天我们开始研究另一种用于分类的机器学习方法----朴素贝叶斯分类器.
贝叶斯分类器是一类机器学习方法(generative　models)中的一个典型方法. 

深入讨论之前，先回顾条件概率这一概念.

条件概率是已知事件B发生时，事件A发生的概率:
$$
P(A|B) .
$$
例如，假设你有一个事件A----你迟到.　有一个事件B----你的闹钟在早上没有响.

从你以往的经验看，你迟到的概率,P(A),也许很小．
你的闹钟坏了的几率P(B)也很小.
然而, 如果你想得到闹钟坏了时你迟到的概率(条件概率)，它是不是还很小呢？

这可以写成 the probability of you missing
school and the alarm clock failed,
divide by the probability of the alarm clock failing.
$$
P(A|B) = P(A)P(B)/P(B)　？？？
$$
你能预计P(A)会很小. P(B)也会很小.　然而, P(A|B)则可能比它们都大得多，因为在你闹钟坏了时你很可能迟到.

表达条件概率的另一种方法is by just doing the multiplication of these two
terms here.
And you have probability of the conjunction
A and B is equal to P of A given B times P of B.
So you see how this could be useful in many ways.
So it's possible to write P of A intersect B
just as we have seen before using.
It's possible to write P of A intersect B this way.
It's also possible to write P of A given B in terms of P
of B given A times P of A by just applying the Bayes
Rule one more time.
But this time instead of writing P(A|B),
we would write P(B|A) is P(B and A),
which is also P of A and B similarly,
and divided by P of A. So we could also
do the product of these two and have the term return this way.
So if we have P of A intersect-- we are writing P of A and B
in two ways. We could put these two terms in an equality.

And in this case, we are going to derive the rule that
says probability of A given B is probability
of B given A times the probability of A divided
by the probability of B.
$$
P(A|B) = \frac{P(A)P(B|A)}{P(B)}
$$
 这就称为贝叶斯规则.　它用在了很多不同的的领域．

我们称P(A)为先验概率.
It's what we observe in the population, for example,
for the event A to happen.
P(A|B) 称为后验概率(posterior),
即B发生时A的后验分布.
P(B) 称为事件.
最后, P（B|A) is called the likelihood.
这是概率论中最重要的公式之一，在各个应用场景都有广泛应用.

今天我们用它来推导朴素贝叶斯分类器.　应用贝叶斯规则推导朴素贝叶斯分类器之前,
让我们看看朴素贝叶斯分类器在实际中是多么有用.
Let's first of all write a table in which
we give all possibilities of events A and B happening.
Suppose we are just focusing on two events, A and B.
Whether A happened or not, we're going
to call it A. If it didn't happen we'd call it not A.
The event B happening or not happening.
So we're going to see how the intersections all possibilities
of A and B happening or not happening.
So for the intersection of A and B,
we are going to talk about the probability of A and B
happening.
At the intersection of not A and B
we're going to talk about probability of not A and B
happening, right.
The lines-- at the end of the line,
we have what we call-- in the columns
we have what we call the marginals.
Probability of B happening is the probability of A and B
happening, plus the probability of not A and B happening,
which means that B happening no matter
whether A happened or not.
We're writing P of B as the sum--
which is the marginal-- as the sum of the line
would be very useful, because we are going
to plug it into Bayes Rule.
So we have P of B and P of not A and B.
So recall, Bayes Rule is of P of A given
B:  P(A|B) is written as P(B|A) times P(A) the prior divided
by P(B). So you can just plug in this value here, right.
And then you could rewrite P of A intersect B using
the conditional probability.
Not A intersect B, using again the conditional probability,
which lead us to another formulation of Bayes Rule,
which is the formula here.
So you see you organize this is what we had before in Bayes
Rule, but this one now is a little bit more detailed--
a lot more detailed--
using the table that we just saw.
So here's an example of using the new formulation of Bayes
Rule in order to predict whether a patient has cancer given
that some lab test is positive.
So we have some prior data about how effective was the lab test.
And we are going to use the new formulation
to estimate the probability of the patient
having cancer given that the patient had a positive lab
test.
This is something that is hard to estimate.
But using different kind of probabilities as observed
from prior observations, we could come up
with an estimation of probability
of having cancer given that the lab test is positive.
All right.
So suppose that the incidence of cancer in the population
is of 0.8%, which means that the probability
of someone having cancer, which is the prior--
in this case, we call it the prior-- is 0.008, OK.
The probability of not having cancer in this case
is the complementary.
Recall one of the actions of probability,
probability of the event A is simply
1 minus the probability of the event
not A. Right, not A. All right, in this case
we could calculate the probability of not having
cancer as 0.992.
Suppose we have another piece of information that tell us
how effective is this lab test to predict
someone having cancer.
In other words, the probability of the patient
having a positive lab test given that the patient truly
has cancer.
So the test is in this case very good.
It can predict the probability of patient
having a positive test given that the patient truly
had cancer is of 98%.
OK.
Likewise we calculate the probability
of the lab test being negative given
that the patient has cancer, which
would be the complementary of this value here.
Suppose we have another piece of information that tells us
about the false positive, the probability of the lab
tests not being positive given that the patient did not
have cancer.
These are the false positive.
What actually the test is predicting
is different from reality, right.
So probability of having a positive lab
test given that the patient didn't have cancer is of 0.03.
Plug in all these pieces of information
together using the new formation of Bayes Rule.
We get that the probability of that patient having
cancer given that his test was positive is of 0.21.
So you see how the Bayes Rule allowed
us to flip the probabilities and use the information that we
had.
We could observe rather than to estimate
a probability of an event that we couldn't observe.
So now you must be wondering, why
are we talking about probability,
and conditional probability, and Bayes Rule?
So let's recall first our setting for classification.
So we are given some data.
数据有标签.
We have the features.
我们想要训练(学习)一个将特征映射到标签的分类函数.
我们在学习这个映射.
But in this case, what we are trying
to accomplish is to learn this function, this mapping,
not through a decision boundary like we did before,
but through using probabilities, the conditional probability
of the label Y given what was observed in the data which
are the features.
Before we show how we could find this mapping or this function
based on the probability of Y given X,
I would like to clarify an important point
in machine learning which is the existence of two
classes of algorithms.
We talk about discriminative algorithms
when we want to model the probability of Y
given X, which is the conditional distribution
of the labels given the data, right.
In discriminative algorithms, we aim
to find a decision boundary that separates positives
from negative examples.
Recall that, so we did that in the previous lectures.
To predict a new example, what we
check is, in which side does this data point when
you part in the dimensional space,
where is this data point located?
On which side of the decision boundary?
This helps us decide the label of the point, right.
So what we are doing here implicitly
is to calculate the probability of Y
given X. This is what we are modeling by finding these
at such a decision boundary.
In contrast, generative algorithms
adopt a different approach.
Instead of building one single model,
which is a decision boundary, and use it
to classify a point as positive or negative,
what you are building in generative algorithms
is two models--
one model to what positive example looks
like, and one model, a different model, to what
a negative example looks like.
To predict the class of a new example,
we are going to match this example to the existing model,
to the two models, and see which matches the best, right.
So we are modeling in this case the probability of the data
given the label and the probability of the label,
and we are going to use Bayes Rule in order
to flip the probability, right.
So in discriminative algorithms
we are modeling P of Y given X. In generative algorithms
we are going to model P of X given Y, model P of Y,
and then use Bayes Rule in order to flip the probability
and have an estimation of the probability of the label given
the data observed.
OK.

我们如何做预测呢?
To make a prediction we are going to use Bayes Rule
and see which Y maximizes the Bayes Rule.
In other words, so we are going to use--
so here's a function.
Argmax means that what are the Y. What
is the Y that actually maximizes the probability of Y given X?
So probability of Y given X we are going to use Bayes Rule
and replace it in here, right.
So we are going to check what would
be the Y or the label, the positive or negative,
that actually maximizes this quantity.
Probability of Y given X, OK.
This could be just estimated as what?
As the probability of just the P of X given Y times P of Y. OK.
So we are going to estimate that by using this formulation here.
This actually defines what's a naive Bayes classifier, which
is a probabilistic model which is a highly practical method
in practice, especially in natural language text
documents.
How to classify, for example, spams from non-spams, etc.
It's naive because of the strong independence assumption
that it makes, which is not realistic
but in practice this happens to work.
It's a very simple model, very simple to implement.
And however it's a strong method that
can be comparable to decision trees on neural networks
or other kinds of algorithms.
OK.
So here's the setting.
So we dispose of data again so we have data and labels.
X is the feature vector, and Y is a discrete label.
We have d features and n examples. Example--
consider document classification.
Each example is a document.
Each feature represents the presence or absence
of a word, a particular word in the document.
OK.
So we have our training sets.
A new example with feature values
would be called-- for example, XU would have the feature
values a1, a2, ad for feature 1, feature d.
And you want to predict the class, or the label Y new
for the example X new using the formulation of a naive Bayes
classifier.
OK.
So what we want to achieve is 找到新记录的标签, by looking at--
so we have Y belongs to the set of possible outcomes or labels.
It could be plus 1 or minus 1, or Z or Y,
or bananas and oranges, right.
So what would be the Y that maximizes the quantity
P of Y given a1, a2, ad, which is the values observed
in the data?
So what would be the probability of--
what would be the Y that maximizes the probability of Y
given the data observed?
So this is what we want to achieve.
我们想对新记录 X,找到其标签Y.
This label would be estimated by looking
into the Y that maximizes this quantity, probability
of the label Y given the data, right.
So we are going to use of course Y in big Y,
which is the possible labels plus 1,
minus 1, 0, 1, or even bananas and oranges, right.
So what would be the Y that maximizes this quantity here?
OK.
Applying Bayes Rule means that we're
going to just replace this formula
by its corresponding Bayes Rule.
And as we said before, this will be
estimated by just looking at the probability of data
given the label times probability of the label.
So the question is, can we use the training data
to estimate these two terms?
The answer is for P of Y, yes.
We could easily estimate it by--
if we have a very large set, we could estimate probability
based on the frequency of Y happening.
So we just count the frequency with which each label
Y occurs in the training data.
So how many "yes" you have, how many "no" you have,
just like in the example we have seen before
about hiring applicants, OK.
P of Y can be easily estimated from the data
by counting the frequency of positive,
let's say, and frequency of negative examples in the data.
All right.
However, the probability of a1, ad given Y
is harder to estimate.
Why?
Unless we have really a very, very, very huge, large sample,
it's very hard to find exactly the combinations
of value a1, value a2, value ad happening, given the label Y.
So it's really we need to see every example many times.
Every appearance of this combination
of feature values in the data, see it enough times
to be able to estimate, to have an accurate
estimate of its probability.
So this is hard.
And that's where the term "naive" is
happening, which is that 我们需要做一些假设.
And if the assumption happens to be naive
because we are going to assume that a1, a2, ad are independent
given Y. So how are we going to estimate
this probability of data given the label Y,
is by making some simplifying assumption.



## 朴素贝叶斯分类器
It turned out that to estimate the possibility of a1, a2, ad
given y, we have to make some simplifying assumption.
What's this assumption is that this is what
makes the classifier naive.
We're going to make the naive assumption that the feature
values are conditionally independent
given the label, which means that given
the label of the example, the probability of observing
the conjunction a1 and a2 and ad, which
is a row in the table, so the probability of observing
this commission of possible values for each of the features
is 特征的几率的乘积 product of the probability of the individual features.
So where is this coming from?
So this is coming from the definition of independence
in probability.
如果你有两个事件 A和 B,　并知道P(A),P(B), 且假设A,B是独立的.
那么the probability of the conjunction
is simply the product of the probabilities.
So if we have p of A and B given some condition y,
this would be probability of A given y times the probability
of B given y.
Applying this concept into this formula here,
we have the probability of observing the value
a1 and the value a2 and the value ad
given y is the product--
this sign stands for product--
is the probability of all possible feature
values of each individual feature value given the y.
So this one make things much easier.
Instead of calculating p of a1, a2,
I'm just going to do p of a1 given y times of p of a2
give y times et cetera, p of ad given y.
And each of these values is actually easier
to estimate from the data without having a huge data set.
Because it's likely that a1 happens,
but it's less likely that a1 and a2 and ad happens.
So we don't have to really focus on having
a large sample in which a1, ad appears enough
in order to estimate the probability.
Got it?
So given this, the Naive Bayes classifier
is simply written as, if you want
to calculate the label of the new example,
we're going to see which y in the possible labels that
maximize what?
So we have this term, which is probability of a1 ad given y.
We multiply this by p of y based on this formula here.
So now can we estimate these two terms from the train data?
Yes.
Just as I said now, p of y is just
count the frequency of p of y for each of the possible values
of labels.
p of j given y we could simply calculate
if we have aj appearing enough, we
could estimate the probability of the feature value
is equal to aj given that we observe the label y.
The algorithm is pretty simple.
What you need to do is given a data set.
Estimate all p of y's for all possible labels.
Estimate all conditional probability
for each single feature value given the y.
So for all y for ai, so we have, ai, all possible values
for all feature values, and the classification
is done by using the formula that we just
saw to calculate the label, y new, as the label that
maximizes the probability of y times product
of the probability of each individual value given the y.
So we did it.
This is how we derive the Naive Bayes classifier
and how we make a classification of a new instance.
So here you can observe that there is no model, per se.
There is no hyperplane.
There is no decision boundary.
We're just given a large sample of data or a data set,
which is a snapshot of reality, and we
hope that this snapshot is big enough
to estimate the probabilities.
Just count the frequencies of various data combinations
within the training examples to estimate these probabilities, p
of y, p of each value given y, and plug that into this formula
to make a prediction.
So here is an example.
So we call this example of hiring different applicants.
Suppose we have, again, these four features--
highest degree, work experience, favorite programming language,
needs work visa, and we have some historical data
that tells whether the company has hired this applicant
or not.
So we have this data set, and this data set
is supposed to be very large.
We want to be able to predict the class of new--
this would be our x new of new instance x new.
So given that this applicant has a master's degree, has
this kind of work experience, is familiar with Java,
and it's his favorite language, and does need a work visa,
are we going to hire this applicant or not?
Can we predict this class?
So let's use 朴素贝叶斯分类器 to do so.
So we're going to use the formula we just
derived in order to calculate what
would be the label, y, for this new instance, x new.
So recall we had in an original formula
is y new is equal to argmax of what?
Of values of y that actually maximizes
the value the estimation probability
of y times the product over all the features of probability
of the value aj given the y.
So in our case, we have the y belonging
to either yes or no, the applicant is hired or not.
p of y would be the probability of, yes,
which would be we just count the frequency of the number
of applicants that were hired versus the number of applicants
that were not hired times probability
of the different values given the label, y.
So in this case, the different values would be for highest
degree is highest degree is masters given the y times
probability of ux deisgn given to y times probability
of favorite language is Java given the y--
so I didn't write the whole feature equal value--
times probability of needing a visa is true given y.
So as I said, these two values can be estimated from the table.
We have 8 cases over 14 of applicants that were hired,
and we have 6 applicants over 14 that were not hired.
So we need to calculate conditional probabilities
for each possible values.
So in this case, we are going to calculate
the probability of a master's given the yes,
the probability of ux's on given a yes, et cetera.
So for each value, aj, we're going
to calculate its probability given the label, yes,
or the label, no.
So likewise, we're going to do it
for the label no p of master's given no, so by counting
the probabilities in the data.
So for example, the probability of master's
given a yes, if you go back to the data, is 4 over 8.
This is because we have--
if you focus on only hires is equal to yes,
so we have 8 hires.
And among these eight hires, we have 1, 2, and 3,
and 4 master's among those.
So this is the conditional probability
of having master's given that the applicant was hired.
So given these values, we're going
to plug those numbers for the case
where we have the hire is yes and the hire is no.
So we are going to just change the y here in this formula
by yes or no and plug in the conditional probabilities
to finally get these two numbers that actually suggest that we
should hire this applicant.
So just to summarize, this is how
Naive Bayes classifier works.
Calculate all probabilities for each possible feature value
given the labels that you have, yes or no
in this case, and plug in these numbers
into the formula for the two classes.
So we have this first formula models the probability of yes.
This second formula models the probability of no.
And get the y actually that maximizes this probability.
It happened that for the label equal yes, we
have the highest probability when you use this formulation.
So a final observation here is that it
could happen that the value has a very low probability, close
to 0.
For example, if you have a very large sample,
the probability of observing the specific value
could be 0, which will have the desired
effect of putting all this formulation here possibly to 0.
So if any of them is 0, the whole product
would end up being 0.
So there was a trick to avoid this kind of situation,
and this trick is to just change the estimation
of the conditional probability of aj given y.
So this trick is called the m-estimate, or the probability,
that estimates the probability of the conditional probability
of observing the value aj given y
using a different formulation.
So usually what we do is suppose that nc
is the total number of examples for which the class is y
and the feature value xj is aj, and ny is actually
the total number of examples for which the class is y.
So usually what we do is just nc divided by ny,
the number of times we have aj among the y's.
So now we're going to use artificially
some extension of both values by imagining that there
is some extra samples.
So the trick here intuitively means
that augment the sample size by m virtual examples distributed
according to some prior p, prior estimate of each
of the values.
If the prior is unknown, just used the probability
of 1 divided by k, which means that all possible values are
equiprobable.
Thus, we have as effect to avoid that this probability ever
gets 0.
So this is just an artificial way
to change the probability estimation
of the conditional probabilities in order
to avoid getting the 0 in the formulation.
So as I said Naive Bayes classifier is widely
used in text classification.
Why is that?
Because learning which articles are of interest on the web
is done by classification.
How to classify web pages by topic,
how to classify your emails as a spam or no spam,
actually uses a lot of the principle of Naive Bayes
classifier in which, actually, case Naive Bayes classifier
happen to be one of the most effective algorithms
to filter your emails.
A question is, what attributes should we use
to represent text documents?
So the intuition behind it is that we
are going to augment a sample size by n virtual examples
distributed according to some prior p, which
is the prior estimate of each value.
If prior is unknown, we can assume a uniform prior,
which means that all features values has k values,
and we can set the probability to 1 over k.
So the idea is avoid having the probability of observing
the value aj given y is 0 by augmenting by some values
using this m virtual example.
Just giving you an intuition that we could go around
the constraint of sample width is
being 0 by adding some values to the probability,
a conditional probability of aj given y.
So I said before, a Naive Bayes classifier is widely
used in text classification, so either we want
to classify text to begin with.
Yes, we want to classify it because we want to know
which news articles to read.
We want to know how to classify web pages by topic.
We also want to know which of our emails are spam or not spam.
We don't want to see the spam in our inbox,
so we are going to deploy some methods that allow
us to filter those for us.
And as I said, Naive Bayes is one
of the most effective algorithms for spam filtering.
A question is, what kind of attributes
shall we use to represent text documents?
So we have seen in the previous example,
we had the well-behaved data with feature values and labels,
so how about documents?
How can we represent these documents?
What are the features that we can
use in order to use something like Naive Bayes classifier?
So here again, because of the rate
of the probability of a word is given some class is 0,
we're going to use the m-estimate
of the probabilities.
The probability of a word wk given cj
is estimated by this formula here
in which nj represent the total number of word's positions
in all training examples of the class, cj.
And ak is the number of times the word wk is found
among those nj word positions.
So I'm going just, in this case, to use this formulation which
we add 1 to nk, and we simply add
the size of the vocabulary of the number of words
in the documents to nj.
So the following functions learns the probability
pwk given cj, describing the probability
that a randomly drawn word from a document with class cj
is the English word, wk.
It also learns the classifiers p of cj.
This is just to give you a flavor of how Naive Bayes could
be used for text documents.
So just to summarize, Naive Bayes
is a very useful technique to classify data
as we have seen for the example of hiring.
It could also classify text very well.
It's actually a linear classifier,
and it's incredibly simple and easy to implement,
but works wonderfully for text, such as spam filtering.
朴素贝叶斯分类器．

 

