# 系综方法: Boosting

### 学习内容：
1. 投票
2. 三个系综方法: boosting(重点), bagging, random forest.
3. 

### 
一类强大的机器学习算法．这类方法产生于大约十几年前----ensemble方法.

So first of all, imagine you have
a randomly chosen hyperplane or decision function derived
with any of the machine learning methods we have seen so far.
So you could expect this to perform randomly--
just like you toss a coin-- with an error rate of 50% or 0.5.
OK?
If we combine many such random functions,
we expect also, by majority voting,
we also expect that the outcome of these combinations
would also be random.
假设我们有m个弱分类器　performing slightly
better than random, which means that there
is some signal in them and the error rate is now
0.5 minus a very small value.
So there's some signal in them, and you
want to know whether we can combine
them to make a decision based on the majority
vote of all of these decisions.
OK.
So the question is, what if we combine these m slightly better
than random classifiers---- would majority voting
in this case be a good choice?
To answer this question, I will first
evoke the concept of probability of majority decisions
in the famous Condorcet Jury Theorem from 1785.
In its simplest form, suppose a group
wishes to come to a decision by majority voting.
Each voter votes for the correct decision with a probability P,
and we suppose that the votes are independent.
The question is, how many voters do we
need to have to come to the right decision?
This will depend a lot on the probability P.
So if P is bigger than 0.5, the theorem
says that adding more and more voters
will increase the probability of reaching the correct decision
by majority voting.
If P is less than 0.5 then adding more and more voters
will just make things worse.
This is really the fundamental theorem
for majority voting based on the probability of providing
the correct decision or the correct label p.

Based on this theorem, 我们来定义ensemble方法.
Ensemble method combines the predictions
of many individual classifiers by majority voting.
This concept is at the core of machine
learning ensemble methods.
So what are ensemble methods?
An ensemble method combines the predictions
of many individual分类器 by majority voting.
Such individual classifiers are called
weak learners are required to have slightly
better than random performance.
So the question is, how do we produce
independent weak learners using the same training data?
Why independent?
注意, Condorcet定理said that we need to have these voters voting independently
of each other, and each of them having
a slightly better probability-- bigger probability than 0.5.
So how do we produce such independent weak learners based
on the same training data?
The answer is to use a strategy to obtain relatively
independent weak learners.
There are different methods we could use.
One of them is boosting, bagging, 或随机森林.
So today we are going to focus mostly
on boosting to give you a flavor of how this works,
and then give you a little bit of a hint about these two
methods.
Let's start first with boosting.
Boosting is one of the first ensemble methods out there,
and was one of the most powerful methods
in machine learning methods today.
The most popular algorithms for boosting
is called AdaBoost.M1, invented by Freund and Schapire in 1997.
Professor Breiman who invented CARTs algorithms
for decision trees described this algorithm
as the best off-the-shelf classifier in the world.
So the algorithm is famous by its simplicity,
but it's also famous by the fact that it can use any kind
of weak learner such as trees, perceptrons--
we know both of these--
along with what we call decision stumps, which
we are going to see today.
And the idea is to train the weak learners--
or the weak classifiers--
on weighted training examples.
So this is the key word.
The weighted training examples means
that we are creating decision boundaries or decision
functions based on modified versions of the training
examples.
The idea of boosting is to apply a weak classifier to a modified
version of the training data.
So initially, suppose we have-- we dispose of training data.
So using this training data, we are
going to produce a first classifier-- let's
call it G1 of x.
That's a classification function.
Using any of the methods you know so far.
The idea is to move to the next step by creating another--
distorting the data by creating another weighted changed data--
weighted data-- and also produce a second function--
G2 of x-- so create a sequence of decision functions
based on the modified versions of data.
And this is a second decision function.
Then, given the outcome of this second decision function,
weight the data again, somehow, we
will see how, weighted data--
and produce a third decision function--
G3 of x-- et cetera, until some number of classifier m on yet
another modified data, and obtain a final--
and we obtain a Gm--
the mth sequential weak learner based on this modified
data at the m level.
OK?
So then this is the process.
We are going to go through a sequence of those classifiers,
and each time we have some decision function we have,
we know how this is performing on the training data,
and we modify the data accordingly at each stage.
Now, the final classifier will be the function G
of x that's actually exploiting or using
all of this decision function or decision boundaries
that we have found before.
So usually we do the sum-- it's the majority voting,
which is the weighted sum of all the classifiers--
G of m of x-- for m going from 1 until m.
So we are going to go through all the classifiers
and see how the classifiers classify the instance x.
We're going to take the sine of this.
We suppose that we are seeking a function that classifies
into the set minus 1 plus 1.
So we are going to take the sine of the sum of Gm of x,
but you are going to weight each of these functions
by its importance in the classification process.
We'll be providing alpha m, which
is a weight affected to each of the classification
functions obtained.
The idea of boosting is to apply a weak classifier to modified
versions of the data.
So we start with the--
first to a classifier applied on the training data.
Training data.
So if we apply this weak classifier,
we're going to obtain a first decision boundary
or first classification function--
let's call it G1 of x-- so this is a classification function.
So based on the outcome of this classification function,
we are going to use a modified version of the data--
modified data-- and build based on this data a second
function--
G2 of x-- that actually takes into account
the outcome of G1 of x, et cetera, and the process goes on.
And we're going to do that as a sequence of classifiers
until some number m--
so this is the mth modification of the data--
from which we are going to apply again, some decision function--
some weak learner--
and obtain a G of m, a function G of m of x.
So we end up with many decision functions.
And the idea is to use a final classifier using the majority
voting between the different functions.
Se we are going to obtain something
that looks like the classification of an example x
would be the majority voting of all the classifiers
G of m of x, for all m equal 1 to big M.
So suppose we have m classifiers,
and these are weighted by some weight
alpha that is actually the importance
of each of those classifiers-- so let's call it alpha of m.
So we're going to take the sign of this function
because we suppose that our classes are in the set minus 1
plus 1.
So we take the majority vote, and we
weigh each of the classifiers by some weight
alpha that we'll see how we calculate.
To summarize the predictions from all of the Gm's-- all
of the weak classifiers from m going from 1 until big M--
are combined with the weighted majority voting.
Alpha m is the contribution of each weak learner Gm,
and these are calculated by the boosting algorithm,
to give a weighted importance of classifiers
in the sequence.
This decision of a highly performing classifier
in the sequence should weight more than the decision
of a less-performing classifier.
And this is captured in, again, this function, that actually
is the sign of the the weighted sum
of the classifiers times the importance of each
of the classifiers.
So as usual, we calculated the error rate
on the training set as the number of mistakes
we make on the examples.
So this is the true label--
this is the predicted label.
And we are going add 1 whenever we
make a mistake on an example.
We weight the error by n, which is the number of instances.
Now, the error rate of each weak learner
is calculated in the same way.
But in this case, we're going to weight the examples by a weight
Wi.
So what is Wi?
It is a way to modify the data to weight the example
that actually the previous classifier in the sequence
made a lot of mistakes.
So suppose we have-- we start with a training
set of pluses and some minuses.
We generate some decision function--
let's call it G1 of x.
When we apply G1 of x on the data,
we're going to see that some of them have actually--
are actually misclassified by G1 of x, and the way
we modify the data is to weight the examples differently.
So next time that we are going to derive another classifier,
we're going just to give **more importance** to those points
here.
So we are going to make their weight really bigger.
如果我们给它们一个更大的权重，那么 they would be a focus to classify correctly
those examples when we derive the next classifier--
G2 of x.
So that's the idea.
So the error now is now weighted by the importance
of each training example in finding the next decision
function in the sequence.
So doing this successfully allows
us to focus on the most difficult examples to classify.
So the intuition behind it is that we
need to give large weights through hard examples
and small weights to easy examples.
For each weak learner m, we're going to associate the error m,
and associate the alpha, which is the importance
of the classifier.
Alpha m is defined by logarithm of 1 minus the
error m divided by error of m.
So this is the function that is used
to associate the weight alpha to the importance
of the classifier.
如果误差非常小, 那么alpha
将会非常高的值．

## AdaBoost algorithm

AdaBoost algorithm按如下方式工作.
第一步, 初始化例子中所有的权重(wi) i equal 1 to n. n是记录的数目s.
So initially, all examples have the same weight
because we didn't run any machine learning algorithm yet
to decide which examples are hard to classify.
Afterwards, we're going to derive m weak learners or weak
classifiers.
We're going to feed the classifier
g of m of x for the training data using the weights wi.
We calculate the error m as the weighted sum
of the errors divided by the sum of the weights of all
the instances.
Then we compute the importance of the classifier, alpha m,
which is the logarithm of one minus the error divided
by this error.
The individual weights of the examples are then updated.
And the update is made such that each example that
has an error on it, which means that the previous classifier
in the sequence has made an error on this example,
will be scaled by a factor of exponential of alpha of m,
which would increase its influence or its weight.
So this is a way to increase those examples
on which the classifier g of m has made an error.
Then the output of this classification is, as I said,
the sign of the weighted sum of the different classifiers
weighted by the importance of each of these classifiers.
Let me give you a very simple example
of a very weak classifier called decision stump.
A decision stump is a simple two-terminal decision
tree for binary classification.
The function is a function f for an example xi, given a feature
j, and the cutoff point t in the values of j.
The function would be equal to plus 1
if the value of this feature for this individual
or for this example is bigger than t, minus 1 otherwise.
So it's a very simple decision tree
in which we have here that the variable xj.
If xj is bigger than t, we decide that it is plus 1.
If the variable is less or equal than t,
we decide that the class is minus 1.
So this is a very, very simplistic decision tree.
One feature at a time for one cutoff point at a time.
So here we have d dimensions.
Suppose in a data set of 10 features,
2,000 examples for training, and 10,000 examples for testing,
we're going to compare the performance of decision stumps
when they are taken individually as compared
to a random classifier, and as compared to a classifier that
is actually boosting of those weak decision
stump classifiers.
So let's compare the performance of AdaBoost
to a random classifier, to a decision stump,
and to a large classification tree.
So as you can see in this figure,
we have a plot of the test error with respect
to the number of iterations in the boosting.
So a random classifier would have
would have a performance of 50% error rate.
So this would be something that would come in this line here.
So that would be our random classifier.
If we take one stump at a time, we
expect this to be very poorly, but a little bit
better than random.
So in this case, the stumps would have a performance
of 0.45 error rate.
And this will make it come to this second position here.
So this is our stump performance.
The error rate is 0.45.
If we divide a decision tree based on the data and all
the features and we develop the whole tree,
we can find the performance of 24.7% of errors
and this will be our decision tree performance.
And you could observe that as we do more and more iterations
using a boosting of the decision stumps as weak learners,
we achieve a very interesting performance, and a decrease
in the error rate that could go as low as actually
5.8% of errors after only 400 iterations.
So you can see that decision stumps by themselves when taken
individually perform poorly.
A decision tree could have a performance of 24.7%.
But however, it's going to be beaten
by when we combine these very small weak learners together
after several hundred of iterations,
we're going to achieve a much lower error rate on the data.
So one of the main interesting features
of using boosting with the decision stumps
is that it can be seen as a feature selection algorithm.
Why?
Because we're going to-- for each of those classifiers--
remember, we have for each of the classifiers g of m,
we are getting an alpha of m that
tells us how important is this classifier in the final answer
of making the decision.
Likewise, since we are deciding on each feature at the time,
so xxj, we're going to for each of those classifiers
obtain an alpha m that tells us how important is this feature
in the final decision.
So this could be seen in this example of spam filtering,
in which we have the words of the e-mails in the white color,
and we have the relative performance
or the-- this would be the alpha of each of the classifiers
based on each of these features individually.
So AdaBoost with decision stumps can play an interesting role
in defining which are the features that
are most important in making the decision.
Now I will give you a hint of what bagging and bootstrapping
are.
So first of all, bootstrapping is a resampling technique
which means sampling from the empirical distribution.
And the aim is to improve the quality of the estimator
or the classifiers.
Bagging and boosting both based on bootstrapping.
So both use resampling to generate weak learners
for classification, but with the exception that boosting
is going to alter the data set by putting weight
on each of the examples.
The strategy is to randomly distort data by resampling,
and train weak learners or resample training sets.
Bagging is a compact way to say bootstrap aggregation.
So bagging works as follows.
So for b equal to 1 to B, draw a bootstrap sample.
The idea is to draw different samples from the data
and build different classifiers based on these samples.
And the classification then is a majority vote among the B
trees or the B classifiers.
So f average is 1 divided by B, the number of samples,
with the some of the functions of B at x.
Now I will give you a little hint of what
bagging and bootstrapping are.
I'm leaving the details for the machine learning course.
First of all, the concept of bootstrapping
is a resampling technique, which means
sampling from the empirical distribution.
The aim is to improve the quality of estimators.
Bagging and boosting are both based on bootstrapping.
And both use a resampling to regenerate weak learners
for classification.
So the strategy is to randomly distort the data by resampling,
and train weak learners on resampled training sets.
So bagging stands for bootstrap aggregation,
and it works as follows.
So suppose we have a sample or training data set.
We are going to sample from this data several times,
build the learners, and then average the results,
simply average the results on all weak classifiers obtained
from the sample data.
So the training is very simple.
So for b equal 1 to big B, draw a bootstrap sample B
of b of size l from the training data and training classifiers
f of b on the sample that we just drew.
The classification is done again by majority vote
among the B trees or the B weak classifiers.
So such as the average, the function, or the outcome
would be one divided by B of the outcomes obtained
from each of the classifiers.
In general, we have observed by empirical experimentation
that actually bagging works very well for trees.
So if we use different samples and apply decision trees,
we are going to get a set of decision trees.
And having them do a majority vote on those trees,
we perform better than the full tree developed on all the data.
Random Forest is a special kind of bagging
with trees that is intended to reduce the correlations
between the trees.
So in this case, the trees are trained to optimize
each split over all dimensions.
So for Random Forest, we need to choose a different subset
of dimensions at each split.
So remember, when we were building a decision tree,
each time we were to pick a feature at some point,
we were considering all the features, calculating the gain,
and pick the feature that actually does
a better split of the data.
So in the case of Random Forest, we
are going to just pick a feature from a subset of the features,
which means that we're going to reduce dimensions from d to m,
and pick among the best features among this subset
of dimensions.
So the subset is chosen at random out
of the 1 to d dimensions.
And it is recommended by practice
to use m is equal to square root of d at each split in the tree.
So this was just a hint of what Random Forest and bagging can
do.
I would like to mention that these are really
two powerful machine learning methodologies that
are comparable to support vector machines, neural network,
and deep learning.
So think of combining your classifiers.
If you think that the single classifier doesn't perform
well, create an ensemble of those classifiers,
and chances are you're going to get a much better decision out
of those combinations.
Hope this helps.

