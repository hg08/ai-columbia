# 6.3 决策树

## 三分类器
### 1. 学习目标：

1. 理解什么是三分类树
2. 决策三分类树的实战
3. C4.5
4. 处理数值特征
5. 剪枝技术
6. 与C4.5不同的另一种可选的方法: CART
7. 三分类器的优点和不足
8.





### 2. 三分类器是什么？

另一种机器学习算法--三分类器. 这里将会学习不同的概念和不同的机制

三分类器是机器学习方法中一种非常流行的分类方法．

特点：　算法简单，易于实现．There are no assumptions we need to make about
whether we are going to deliver a linear model or not.
So we just take the data as it is.
Historically, there were two classes of tree classifiers
in the literature.
They were both invented in the computer science community
by Quinlan.
There was a whole Quinlan series between
'79 and '83 of algorithms, including ID3 and C4.5 family
of algorithms.
And also in the statistics, such as CART, classification
and regression trees, proposed by Friedman in 1977.
There were many refinements afterwards.
In the mid-90s, for example, including pruning
and handling numerical features in the Quinlan family of tree
classifiers.
Tree classifiers has been applied a lot.
These include applications in botany, in medical research,
and in computational biology, to cite a few.
So how does it work?
Let me first point out that the terminology tree
is purely graphic.
So when we build tree classifiers, what
you are building is a kind of decision tree
that is grown from the root downwards.
In the sense that, first of all, we
start with a root in which we have
all our examples to classify, so as we build the model.
And then we're going to send these examples
down the tree using the concept of information entropy
that allows us to build a classifier that
makes a good decision model for the data.
The general steps of the algorithms to build the treat
are as follows.
First of all, we are going to start
with the root node in which we have all of our examples.
Suppose our bananas and oranges.
We are going to start with the set of all fruits at the root.
And we build the decision tree downwards.
So once we put all the data in the root,
we're going to do a greedy selection
for the next best features that actually
splits the data correctly.
So the splitting criteria here is based on the node purity.
So we're going to go through the features one by one
and select which ones are splitting the data in the best
possible way, which means that by picking this feature,
we're going to achieve node purity, which
means that nodes that are either from one
class or the other one.
That's in step two.
And finally in step three we are going
to assign the class majority to the leaves of the tree.
So these are the main steps.
These are the general steps.
So let's recall first the classification scenario.
We have data, and the data is a set
of feature values with a label, where the values are in Rd,
and we have the label which is a discrete value.
So the label could be plus 1 and minus 1,
or 0 and 1 in a binary classification scenario.
The classification aims to find the classification function
f that maps the input to the output,
or otherwise the features to the label.
In the case of tree classifiers, we're
not building a function f.
What we're building is a model that is a tree.
And there is no need for the features xi
to be in Rd, so there is no need to turn any categorical feature
into a numerical feature.
So we just took that data as it is ,
and we build the tree in lieu of building a function f.
Here's an example about job hiring.
Suppose we have historical data about applicants who applied
in a given company for a job.
So we have information.
So we have the list of applicants
from the past who also have features
describing these applicants.
And these features are highest degree, work experience,
their favorite programming language,
and whether they needed a work visa or not.
We have the label, which is whether the company ended up
hiring them or not.
The goal is to build a decision tree that
would help us decide when we have a new applicant
whether we should hire the applicant or not.
So in other words, suppose we have
a new applicant with a master's degree
who works on user experience design, whose
favorite programming language is Java,
and who does need a work visa.
Should we hire the candidate or not?
By applying a tree classifier building method,
we obtain a tree similar to this one that
helps us decide for this new applicant
whether we should hire him or not.
So the applicant has as favorite language, Java.
So we're going to go through the tree, the first node.
The favorite language is Java, so we're
going to go through this path, the left-hand side.
Given that this value for the feature, favorite language,
the next feature to test is highest degree.
The highest degree is master's so we're
going to go through this path.
And this path leads us to the leaf that says yes, we
should hire the candidate.
So the answer is yes.
So we could see that this hierarchy
was obtained by using the data.
The way this tree classifier was built
is by first considering all the examples.
So the example we have, eight of the applicants in the past
were hired and six of the applicants were not tired.
So we start with this large set of applicants.
We're going to need to pick one criteria and one feature value
to split this data.
It turned out that the feature that were selected
was favorite language.
If favorite language is Java, then the set
would be split into left and right.
On the left side we have six applicants who were hired,
and one applicant who were not hired.
And on the right side, we have two applicants hired
versus five who were not hired.
So this seems like doing a pretty good job
of splitting the data between hired and not hired.
And so on and so forth.
So we're going to go again through the process
and pick another feature value with which we split the data.
So the next one that was picked is highest degree.
And this highest degree would have three possible values,
bachelor, master's, and PhD.
And it turned out if you pick this feature,
you're going to have nodes that are all pure, which
means that they have examples of one specific of the classes.
So it's clear that splitting criteria in this kind
of algorithm, and we are talking here specifically about
the line of algorithm that is C4.5 or ID3,
is the central choice is really the splitting criteria.
How do we pick the next attribute or the next feature
to split on?
So we are looking for some criteria that
measures the homogeneity or impurity
of the examples in the nodes.
So you can see here that we were quite
happy to see that in this node specifically,
we have six hired versus one not hired.
So we're looking for some criteria that
help us split the data so as we put them in buckets of classes.
So we are looking for some criteria
that actually help quantify the mix of classes at each node.
The maximum is reached when there
is an equal number of elements or examples
from each class in the node, which
means that there is a high homogeneity
or the node is impure.
And we want this measure to be minimum if the node is pure.
So we have only one class in the node.
A perfect measure for this kind of situation
is commonly used in information theory,
and it's called the entropy.
So the entropy will be measured for node S.
So given some node S, the entropy
will be measured by taking into consideration
the proportions of positive and negative examples in the node.
So the entropy of S is defined as minus proportion of positive
log 2 of proportion of positive, minus proportion
of negative log 2 of proportion of negative in the nodes.
So this measure will tell me what is the mix in the node.
The best way to understand how the entropy can
measure the purity of a node is to plot
the function of entropy.
So you can see that the entropy is at its maximum
whenever we have a perfect balance
of positive and negative.
And this can be seen at this point here.
So here on the x-axis we have the proportions
of positive examples.
When it's 0.5, it means that we have as much positive examples
as negative examples.
And in this case, the function will be at its maximum.
So this is corresponding to the criteria
that we are looking in this and the kind of function that
could help split the data.
The function is at its minimum whenever we have a pure node.
So this is at this point, where we have only positive examples,
or at this point here where the proportion of positive examples
is 0, which means that we only have negative examples.
So this function is the perfect function
to measure this purity of nodes.
In general, if we have c classes,
we can generalize the entropy function for node S
as the sum of minus p i log 2 of p i, where each of the p i
is the proportion of one of the classes in the data.
We use a measure called information gain that
measures the expected reduction in entropy
caused by partitioning the examples according
to the attributes.
In other words, the gain for some node S given some feature
or attribute A is the entropy at the node A
minus the entropy at each of the node that are children of A
if we split by A going through all of its features.
In other words, it's the entropy of S
minus the entropy weighted by the size of each of the nodes.
So we're going to sum over all possible entropies
for all possible values of the feature A,
but divide that by the size of each of the nodes as
compared to the whole size of the node S.


# 数据之过拟合

现在我们运用信息增益和信息熵的原理到我们的数据集.
从一个含有8个正标签和六个负标签的例子开始.
相应的熵等于0.985, which corresponds roughly
to some value very close to the top of the entropy.
Because why?
Because the proportion of positive
example is actually almost equal to the proportion
of negative examples.
So the entropy for the root level is 0.985.
Then we're going to pick one of the features to work with.
Suppose we pick highest degree that has as possible values
bachelor's, master's, and PhD.
If we split by each of these values,
we get an entropy of 0.97 on the left node, 0.722, and 0.811.
So the node, for example, in the middle
corresponds to the entropy of having four positive examples
and one negative example, which lowers
the entropy at this note.
So overall, we see that we're going from 0.985
into a 0.97 for this node, 0.722 for this node,
and 0.811 for this node.
Now that we have calculated the entropy at each of the nodes,
we're going to see how much gain we obtain by splitting
the root based on this feature.
So the gain at the root node based on the feature highest
degree will be equal to the entropy
at the root minus the entropy at each
of the children, each of which is actually
weighted by the proportion of number
of examples divided by the total number of examples.
So for the first node, for example,
we have five examples among the 14 examples initially.
This gives us a gain of 0.149, which
corresponds to how much we gain if was split by this feature.
So we do a similar process for the feature work experience
that splits based on the three values of web
dev, mobile dev, and UX design.
So basically the entropy of the root is the same.
We always have eight positive versus six negative at the top.
If we split that work experience,
the node that is on the left will have an entropy of 0.81.
The nodes in the middle would have an entropy of 0.65,
and the nodes on the right will have an entropy of 1.
So it's clear that in the node on the right
that actually what we are getting
is really we are at the maximum of the entropy value.
Why?
Because we have two positive and two negative,
which means that our proportion of plus
is equal to our proportions of negative examples.
Likewise, we are going to calculate
the gain of main at the root level
and pick work experience, which would be equal in this case
to 0.189.
We did the same thing for the feature favorite language.
We obtain a gain of 0.258.
And finally, we do a similar splitting by need work visa.
That is either true or false.
In which case we're going to obtain
an entropy of 0.985 or 0.985 if the need for work visa
is equal to false.
In this case, observe that the gain
is equal to 0, which means that we gain nothing
by picking this feature.
So it's good news that there was no decision made
on hiring people based on their need for work visa or not.
So this figure here summarizes all
what we have done so far by picking for each
of the values we obtain a gain.
And we're going to compare these gains in order
to make our decision about which features to pick next.
So we summarize these gains into this table.
So we have the feature and information gain obtained.
So we see the need for work visa was 0,
because we didn't gain anything by splitting by this feature.
And it appears that we are having the maximum of gain
if we pick favorite language.
So at the first split, starting from the root,
we choose the attribute favorite language.
That has the maximum gain possible.
Then we'll start the same process at each of the children
nodes of favorite language.
你可能已经注意到了:到目前为止，我们利用决策树处理的数据都是是分立值(categorical). 这是决策树的一个重要特征.  决策树对分立标签很管用!　
那如果你的数据是数值型的又该怎么办呢？比如，发表论文的数量 the number of papers published,
the year of work, the grade point average, and the need
for work visa or not.
So we have three features that are purely numerical,
and we need to do something with that when we feed that
to the decision tree.
So the way it works is either you first start by discretizing
your values-- for example, you could put some intervals
instead of the values.
So this 3.20 could be replaced by something like between 3
and 3.5.
So you could discretize this way.
Or you could also let the algorithm handle it.
And the way the algorithm handles it in decision trees
is by putting cut points.
These cut points are basically the values in the table.
And it would count the proportions of positive
and proportions of negative after and before the cut
points.
So you could calculate the entropy
based on different cut points, cut-off points
on the scale of this feature.
So in the past lecture, we spoke about overfitting the data.
This is one of the main problems that you
need to take care of when you build the model.
You want to build the model that does not overfit the data.
In other words, it has a good enough generalization
capability.
So in the case of decision trees,
decision trees are not an exception.
And overfitting can happen very easily
simply by developing the most developed,
the biggest possible tree.
Which means that **you're going to fit your data perfectly.**
You're going to learn every single data object or data
instance by heart.
So let's show that with a graph.
So suppose you are putting the size of the tree in the x-axis,
size of the tree.
It could be simply **the number of nodes** in the tree.
And you put on the y-axis the accuracy of your decision tree.
I'm going to plot what would roughly the accuracy
on the training set look like.
Actually the accuracy shouldn't exceed 1,
so if you are starting suppose the accuracy
so we are going to be 100% accurate on classifying
your example if you learn the best possible tree, which
is the biggest.
Suppose we have our data split into 10, 20, 30, 50, 60,
up to 100 nodes.
100 nodes would be the largest possible tree
you could build with this data.
So if you observe the behavior of the accuracy
as you increase the number of nodes,
you would see something that would really
go high this way, which means that you're
going to get 100% accuracy on your training data.
However, if you apply this model as you go on your **test set,**
you're going to observe that the accuracy will probably
be OK at the beginning.
Generally you could expect a 10% drop.
But then it will go down as you actually
are overfitting that data because you are going
to stick with data too much.
So remember this very specific description
of the tree that doesn't let any other tree go through.
So your accuracy on the test set would be going down.
So this is the test.
While you observe that actually you
are doing better and better on the training data
and here you can see that actually this gap in accuracy
is simply due to the fact that your decision tree
is really specific to the data that you have in your training
set.
To avoid this problem from happening,
we are going to deploy what we call剪枝策略.
So to get a suitable tree size and 避免过拟合,
what we can do is, first of all, we
could stop growing the tree earlier
before it reaches the point where it's perfectly classifies
all your training examples.
So it's difficult to know a priori when to stop,
but you could do that and do different tests
to see what's going on.
A second strategy to剪枝 is to do rule post pruning, which
means that you take a decision tree
and你将其转换成规则.
So remember, we have the root level,
then you have one feature.
So if the feature is equal to this value,
and the second feature is equal to this value,
and then the class is minus 1 or plus 1.
So you could turn your decision tree into a set of rules
and do the simplifications with just simply the rules
that you have obtained.



Another method to build decision trees invented
in the statistics by Leo Breiman is the CART method.
The CART method adopts the same **greedy search** top down
algorithm to find a decision tree that matches the data,
except that CART uses binary splits instead
of multi-way splits like in the decision trees we have seen
so far, in which each feature is split by all of its modality.
In CART it's just **binary splits**.
And it also uses another index to measure the mix
of the classes in the nodes.
In this case, instead of entropy,
it uses the Gini index.
Gini指数也是基于标签的正负性, 但只是put them differently.
In this case, the Gini would be 1
minus the proportions of positive example squared
minus the proportions of negative points squared.
So this is pretty close to the entropy measure.
This just doesn't have the log of the proportions.
So you can see in this curve that actually
the Gini index in blue is pretty close to the entropy index.
And the two methods perform similarly.
Here are some practical considerations
when you use decision trees in your applications.
So first of all, consider performing
dimensionality reduction beforehand to keep
only the most discriminative features,
the most important features that are
a relationship with the label.
So we have seen, for example, that the visa status
has nothing to do with the decision of hiring
an applicant.
So it's best to remove it from the beginning
and avoid doing the calculations with this feature.
So you could imagine if you have 10 or 20 or 30
features of this kind, you have lots of gain just removing them
from the beginning.
The second one is combining decision trees.
So this is what we call in machine learning
using ensemble methods.
The decision tree has proven to work
very well in many applications.
However, it's proven that when you combine them,
they work even better.
So you could think of something like using Random Forest, which
is a great approach to obtaining the best performance
of your decision tree.
You could also check this paper that actually provides
a meaningful and comprehensive comparison
between different machine learning algorithms.
The third solution is to balance your data
before training to prevent that the tree will just
learn the biased class or the most dominant class.
假如你有90%的数据都是正标签的例子,  剩余的则是负标签的例子.
It's very easy to find a decision
tree that would classify 90% of your data as positive.
So in this case, you would have great accuracy,
but you would do poorly on the remaining 10%.
In this case, you just need to rebalance the data
so as you could do decision trees that
can be really affected by this kind of scenario.
So if there is a skewed distribution, just balance it.
The way to balance it, you could either
use what we call 下抽样(**undersampling**).
If you have lots of data, just **bring down** the majority class.
And if you're careful about you don't have that much data, what
you could do is **oversample** the minority class instead.
So make sure that you're data is balanced before you go ahead
and run your decision tree.

#### 三分类器的优点
tree classifiers pros and cons?

1. very intuitive. It also has very interpretable behavior,
which means that by looking at the tree,
you know what's going on with other features that
are deemed to be most predictive of the outcome or the label.
It can be turned into rules.
If the applicant knows Java and if the applicant has
a master's, then it is likely to be hired.
So you could turn the tree into intelligible rules.
2. It is also very well suited for categorical data.
3. It's simple to build.
4. And it doesn't need to scale the data. Keep it just as it is.


树分类器的缺点是:

1. 它们不稳定. 其含义就是改变数据集中的一个记录会导致一棵完全不同的树.
2. 不具有不变性. 分割数据is done only one attribute at a time, 也就是说它并没有真正将特征组合起来.每次只考虑一个特征( one vision of one feature at a time).
3. 某一个结点处的选择依赖与所有前面的选择,因为存在一系列的选择there is this sequence of choices of your features.
4. 你需平衡你的数据.Otherwise, this is a problem and you
get a very biased tree that is just
learning one of the classes.

